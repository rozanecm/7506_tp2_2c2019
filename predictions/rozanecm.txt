2019.10.27 - 11:42:06: first approach. Not taking any text atts. Random forest.
2019.10.27 - 11:44:24: first approach. Not taking any text atts. Random forest.
2019.10.28 - 09:12:00: repeat first approach. This time, train with all the data. On previous runs, I trianed only with X_test, which is a part of the whole given train set.
2019.10.28 - 12:03:46: Second approach. Same as previous, but adding categorical features with one hot encoding + svd dim. red., to 11 dimensions in the small cases, and to 100 in the larger one. Algorithm is same as previously.
2019.10.28 - 14:40:22: Third approach. Same as previous, but adding text features with one hashing vectorizer + svd dim. red. Algorithm is same as previously.
2019.10.28 - 16:36:13: 4th approach. Same as previous, but adding spanish stopwords.
2019.10.29 - 11:59:35: 7th approach. LightGBM
2019.10.29 - 13:19:02: 8th approach. LightGBM with grid search.
2019.10.31 - 20:26:47: 1st approach with feat. eng.. LightGBM with grid search.
2019.11.01 - 00:32:25: 2nd approach with feat. eng. catboost with grid search.
2019.11.02 - 12:49:38: 3rd approach with feat. eng. LightGBM with grid search done locally.
2019.11.02 - 13:26:01: 3rd approach with feat. eng. LightGBM with grid search done locally, now done with 2000 samples from train set.
2019.11.02 - 14:42:20: 3rd approach with feat. eng. LightGBM with grid search done locally, now done with all train set.
2019.11.02 - 17:01:06: 4th approach with feat. eng. and feature selection via random forest. LightGBM with grid search done locally.
2019.11.03 - 10:46:24: Avg of the best approaches (the ones with score < 600000)
