{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow you to see all column names & rows when you are doing .head(). None of the column name will be truncated.\n",
    "# source: https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/38f2901c592bdffc40726cb0473318cf\n",
    "# Function which plays a beep of given duration and frequency.\n",
    "# Useful for when executing things that need a while to finish, to get notified.\n",
    "import os\n",
    "def beep(duration = 0.6, freq = 200):\n",
    "    \"\"\" play tone of duration in seconds and freq in Hz. \"\"\"\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_full.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test_full.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a seed, so all algorithms that accept a seed, take the same, for consistency reasons,\n",
    "# so everything can be replicated without problems random state\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sample submission no tiene header. **Ojo con eso al guardar la submission.** Hagamos la funcion para guardar submissions ahora, para evitar problemas a futuro y despreocuparnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save predictions.\n",
    "# There must be a directory ../predictions for this to work as expected.\n",
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "import time\n",
    "def _get_filename(my_name, timestamp):\n",
    "    return \"../predictions/\" + timestamp + \" by \" + my_name + \".csv\"\n",
    "\n",
    "def _save_description(authors_name, timestamp, submission_description):\n",
    "    f = open(\"../predictions/\" + authors_name + \".txt\",\"a\")\n",
    "    f.write(timestamp + \": \" + submission_description + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def save_submission(submission_df, authors_name=\"fcozza\", description = \"no description.\", index=False, header=True):\n",
    "    timestamp = time.strftime(\"%Y.%m.%d - %H:%M:%S\")\n",
    "    submission_df.to_csv(_get_filename(authors_name, timestamp), index=index, header=header)\n",
    "    _save_description(authors_name, timestamp, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.1 - XGBoost con CV basico solo features de matriz de correlacion(Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/feature-selection-for-machine-learning-1-2-1597d9ccb54a \n",
    "- https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['gimnasio','usosmultiples','piscina','cant_palabras_positivas','cant_areas_verdes','tiene_bodega',\\\n",
    "                'tiene_servicio','tiene_seguridad','banos','garages','habitaciones','metroscubiertos','metrostotales','year',\\\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:  2.1min remaining:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:  2.1min remaining:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  2.2min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:  2.2min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:  2.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:  2.4min remaining:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.3min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=3,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=-1,\n",
       "                                    objective='reg:squarederror',\n",
       "                                    random_stat...\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [1], 'gamma': [30],\n",
       "                         'learning_rate': [0.06], 'max_depth': [20, 22, 25],\n",
       "                         'min_child_weight': [11], 'n_estimators': [200],\n",
       "                         'reg_alpha': [10], 'subsample': [0.946934]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [20,22,25],\n",
    "    \"learning_rate\": [0.06], \n",
    "    \"colsample_bytree\": [1],\n",
    "    \"subsample\": [0.946934], \n",
    "    \"gamma\":[30],\n",
    "    'reg_alpha': [10],\n",
    "    \"min_child_weight\": [11]\n",
    "}\n",
    "\n",
    "regXGB = xgb.XGBRegressor(objective ='reg:squarederror',nthread=-1) \n",
    "\n",
    "regXGBwithCV = GridSearchCV(regXGB, params, n_jobs=-1,verbose=10,cv=3) # n_iters es la cant de veces que busca, 10 es lo default\n",
    "\n",
    "regXGBwithCV.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568280850314891\n",
      "{'colsample_bytree': 1, 'gamma': 30, 'learning_rate': 0.06, 'max_depth': 20, 'min_child_weight': 11, 'n_estimators': 200, 'reg_alpha': 10, 'subsample': 0.946934}\n"
     ]
    }
   ],
   "source": [
    "print(regXGBwithCV.best_score_)\n",
    "print(regXGBwithCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No sirve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.2 - XGBoost con CV basico solo features de BoostARoota (Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/chasedehan/BoostARoota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['gimnasio','usosmultiples','centroscomercialescercanos','cant_palabras_positivas','cant_areas_dedicadas',\\\n",
    "                   'cant_areas_verdes','cant_areas_entretenimiento_cerca','planta_alta','planta_baja','tiene_bodega',\\\n",
    "                   'comercial','tiene_servicio','edificio','casa','usa_easybroker','tiene_seguridad','tiene_antiguedad',\\\n",
    "                   'antiguedad','tiene_banos','banos','tiene_garages','garages','tiene_habitaciones','habitaciones',\\\n",
    "                   'tiene_metroscubiertos','metroscubiertos','tiene_metrostotales','metrostotales','cant_amenities','year',\\\n",
    "                   'sin_month','sin_day'\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:  3.8min remaining: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:  3.8min remaining:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  4.1min remaining:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:  4.2min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:  4.2min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:  4.5min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  6.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  6.6min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=3,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=-1,\n",
       "                                    objective='reg:squarederror',\n",
       "                                    random_stat...\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [1], 'gamma': [30],\n",
       "                         'learning_rate': [0.06], 'max_depth': [20, 22, 25],\n",
       "                         'min_child_weight': [11], 'n_estimators': [200],\n",
       "                         'reg_alpha': [10], 'subsample': [0.946934]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [20,22,25],\n",
    "    \"learning_rate\": [0.06], \n",
    "    \"colsample_bytree\": [1],\n",
    "    \"subsample\": [0.946934], \n",
    "    \"gamma\":[30],\n",
    "    'reg_alpha': [10],\n",
    "    \"min_child_weight\": [11]\n",
    "}\n",
    "\n",
    "regXGB = xgb.XGBRegressor(objective ='reg:squarederror',nthread=-1) \n",
    "\n",
    "regXGBwithCV = GridSearchCV(regXGB, params, n_jobs=-1,verbose=10,cv=3) # n_iters es la cant de veces que busca, 10 es lo default\n",
    "\n",
    "regXGBwithCV.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6387103697028009\n",
      "{'colsample_bytree': 1, 'gamma': 30, 'learning_rate': 0.06, 'max_depth': 20, 'min_child_weight': 11, 'n_estimators': 200, 'reg_alpha': 10, 'subsample': 0.946934}\n"
     ]
    }
   ],
   "source": [
    "print(regXGBwithCV.best_score_)\n",
    "print(regXGBwithCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No sirve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.3 - XGBoost con CV y todos los features (categorical encoding 1 - label encoding) (Random Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@songxia.sophia/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_columns = ['ciudad', 'tipodepropiedad', 'provincia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df = X[encode_columns]\n",
    "encode_df = encode_df.astype('str')\n",
    "encode_df = encode_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encode_drop = X.drop(encode_columns, axis = 1)\n",
    "score_encode = pd.concat([score_encode_drop, encode_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(score_encode, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"colsample_bytree\": uniform(0.7, 0.3),\n",
    "          \"gamma\": uniform(0, 0.5),\n",
    "          \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n",
    "          \"max_depth\": randint(2, 6), # default 3\n",
    "          \"n_estimators\": randint(100, 250), # default 100\n",
    "          \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=seed)\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits = 8)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=seed,\\\n",
    "                                n_iter=4, cv=time_split, verbose=1, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  2.1min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.9 s, sys: 324 ms, total: 37.2 s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=8),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1...\n",
       "                                        'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27bfc9f828>,\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27c9150e48>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27c13e7438>,\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27c3a85588>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_search.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770412558172045\n",
      "{'colsample_bytree': 0.7467983561008608, 'gamma': 0.02904180608409973, 'learning_rate': 0.2628528437324805, 'max_depth': 5, 'n_estimators': 203, 'subsample': 0.8832290311184181}\n"
     ]
    }
   ],
   "source": [
    "print(xgb_search.best_score_)\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 977271.2515415936\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_search.predict(X_test)\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print ('RMSE:', rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio'], axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_columns = ['ciudad', 'tipodepropiedad', 'provincia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df = X[encode_columns]\n",
    "encode_df = encode_df.astype('str')\n",
    "encode_df = encode_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encode_drop = X.drop(encode_columns, axis = 1)\n",
    "score_encode = pd.concat([score_encode_drop, encode_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 s, sys: 240 ms, total: 56.2 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Entrenar modelo\n",
    "model = xgb.XGBRegressor(colsample_bytree=0.7467983561008608,gamma=0.02904180608409973,learning_rate=0.2628528437324805,max_depth=5,n_estimators=203,\\\n",
    "                 subsample=0.8832290311184181,nthread=-1,objective ='reg:squarederror')\n",
    "\n",
    "model.fit(score_encode, y,eval_metric=\"rmse\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_encode = test.drop(['id','fecha','titulo', 'descripcion', 'direccion'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df_test = test_to_encode[encode_columns]\n",
    "encode_df_test = encode_df_test.astype('str')\n",
    "encode_df_test = encode_df_test.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encode_drop_test = test_to_encode.drop(encode_columns, axis = 1)\n",
    "score_encode_test = pd.concat([score_encode_drop_test, encode_df_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(score_encode_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"1st approach_full_features. XGBoost\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.4 - XGBoost con CV y todos los features (categorical encoding 2) (Random Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@songxia.sophia/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns = ['ciudad', 'tipodepropiedad', 'provincia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_df = pd.get_dummies(X, columns = onehot_columns)\n",
    "score_onehot_drop = X.drop(onehot_columns, axis = 1)\n",
    "score_onehot = pd.concat([score_onehot_drop, onehot_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(score_onehot, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"colsample_bytree\": uniform(0.7, 0.3),\n",
    "          \"gamma\": uniform(0, 0.5),\n",
    "          \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n",
    "          \"max_depth\": randint(2, 6), # default 3\n",
    "          \"n_estimators\": randint(100, 250), # default 100\n",
    "          \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=seed)\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits = 8)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=seed,\\\n",
    "                                n_iter=4, cv=time_split, verbose=1, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:,~X_train.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_search.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot cuelga la pc con las dimensiones, habra que achicarlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en el approach 2 de Matias en first approaches by rozanecm uso lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "params = {\"colsample_bytree\": uniform(0.7, 0.3),\n",
    "          \"gamma\": uniform(0, 0.5),\n",
    "          \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n",
    "          \"max_depth\": randint(2, 6), # default 3\n",
    "          \"n_estimators\": randint(100, 250), # default 100\n",
    "          \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=seed)\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits = 8)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=seed,\\\n",
    "                                n_iter=4, cv=time_split, verbose=1, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_random_search\", xgb_search))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   7.3s\n",
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_random_search, total= 4.7min\n",
      "610214.3501041636\n"
     ]
    }
   ],
   "source": [
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"1st approach_full_features. XGBoost con one hot\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengamos prediccionse para todas las propiedades en nuestro train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [     1      2      3 ... 239995 239996 239997] TEST: [     0      4      6 ... 239991 239998 239999]\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.4s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [160000, 301176]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-0a004d531182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmy_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0my_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [160000, 301176]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_approach_1_4\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train2, y_test2 = y[train_index], y[test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/on_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.5 - XGBoost parameter tunning (Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning max depth y min child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "                             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27)\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = xgb_model,param_grid = param_test1,n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search1\", gsearch1))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search1, total=17.1min\n",
      "573755.0267781329\n",
      "CPU times: user 1min 44s, sys: 1.07 s, total: 1min 45s\n",
      "Wall time: 17min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 9, 'min_child_weight': 5}, 0.8073939828312875)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning mas preciso sobre este tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[8,9,10],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = xgb_model,param_grid = param_test2, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search1\", gsearch2))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search1, total=19.6min\n",
      "CPU times: user 1min 56s, sys: 889 ms, total: 1min 57s\n",
      "Wall time: 19min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=140, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'max_depth': [8, 9, 10],\n",
       "                                          'min_child_weight': [4, 5, 6]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10, 'min_child_weight': 6}, 0.8090391201801971)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valores mayores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2b = {\n",
    " 'max_depth':[9,10,11,12],\n",
    " 'min_child_weight':[6,8,10,12]\n",
    "}\n",
    "\n",
    "gsearch2b = GridSearchCV(estimator = xgb_model,param_grid = param_test2b, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch2b))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   7.2s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=39.5min\n",
      "CPU times: user 1min 57s, sys: 1.32 s, total: 1min 58s\n",
      "Wall time: 39min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=140, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'max_depth': [9, 10, 11, 12],\n",
       "                                          'min_child_weight': [6, 8, 10, 12]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10, 'min_child_weight': 6}, 0.8090391201801971)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2b.best_params_, gsearch2b.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces 10 y 6 son optimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = xgb_model,param_grid = param_test3, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch3))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=14.0min\n",
      "CPU times: user 1min 57s, sys: 3.08 s, total: 2min\n",
      "Wall time: 15min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     min_child_weight=6,\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=140, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.0}, 0.8090391201801971)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = xgb_model,param_grid = param_test4, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch4))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.8s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=50.1min\n",
      "CPU times: user 2min 21s, sys: 1.54 s, total: 2min 22s\n",
      "Wall time: 50min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'colsample_bytree': [0.6, 0.7, 0.8,\n",
       "                                                               0.9],\n",
       "                                          'subsample': [0.6, 0.7, 0.8, 0.9]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'subsample': 0.9}, 0.8117147682581691)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(65,80,5)]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = xgb_model,param_grid = param_test5, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch5))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  11.2s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=35.2min\n",
      "CPU times: user 2min 17s, sys: 1.44 s, total: 2min 19s\n",
      "Wall time: 35min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'colsample_bytree': [0.65, 0.7, 0.75],\n",
       "                                          'subsample': [0.8, 0.85, 0.9, 0.95]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'subsample': 0.95}, 0.8121743836444513)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch6 = GridSearchCV(estimator = xgb_model,param_grid = param_test6, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch6))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.2s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=18.6min\n",
      "CPU times: user 2min 17s, sys: 772 ms, total: 2min 18s\n",
      "Wall time: 18min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'reg_alpha': [1e-05, 0.01, 0.1, 1,\n",
       "                                                        100]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 1}, 0.8123286654703807)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch7 = GridSearchCV(estimator = xgb_model,param_grid = param_test7, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch7))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  10.0s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=18.2min\n",
      "CPU times: user 2min 17s, sys: 1.67 s, total: 2min 19s\n",
      "Wall time: 18min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'reg_alpha': [0.5, 0.75, 1, 1.25, 1.5,\n",
       "                                                        2]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 1.5}, 0.8123413160129532)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test8 = {\n",
    " 'n_estimators':[100, 150, 200, 250, 300]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch8 = GridSearchCV(estimator = xgb_model,param_grid = param_test8, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch8))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  12.5s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=17.4min\n",
      "CPU times: user 3min 18s, sys: 1.61 s, total: 3min 20s\n",
      "Wall time: 17min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'n_estimators': [100, 150, 200, 250,\n",
       "                                                           300]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 300}, 0.8136330892288279)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch8.best_params_, gsearch8.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test9 = {\n",
    " 'n_estimators':[300, 500, 700, 900, 1200]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch9 = GridSearchCV(estimator = xgb_model,param_grid = param_test9, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch9))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.0s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=57.2min\n",
      "CPU times: user 9min 29s, sys: 1.59 s, total: 9min 31s\n",
      "Wall time: 57min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'n_estimators': [300, 500, 700, 900,\n",
       "                                                           1200]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 900}, 0.8151624288480696)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch9.best_params_, gsearch9.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test10 = {\n",
    " 'learning_rate':[0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch10 = GridSearchCV(estimator = xgb_model,param_grid = param_test10, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch10))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=13.2min\n",
      "CPU times: user 2min 16s, sys: 1.44 s, total: 2min 17s\n",
      "Wall time: 13min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'learning_rate': [0.001, 0.01, 0.1,\n",
       "                                                            1]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1}, 0.8123413160129532)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch10.best_params_, gsearch10.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test11 = {\n",
    " 'learning_rate':[0.075, 0.085, 0.095, 0.1, 0.15]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch11 = GridSearchCV(estimator = xgb_model,param_grid = param_test11, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch11))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   4.1s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=16.9min\n",
      "CPU times: user 2min 16s, sys: 1.17 s, total: 2min 18s\n",
      "Wall time: 17min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'learning_rate': [0.075, 0.085, 0.095,\n",
       "                                                            0.1, 0.15]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1}, 0.8123413160129532)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch11.best_params_, gsearch11.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_best_params\", xgb_model))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.0s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total= 2.3min\n",
      "561393.1192913017\n",
      "CPU times: user 2min 19s, sys: 1.07 s, total: 2min 20s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total= 3.4min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(X, axis=1).replace({True:1,False:0}), y)\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"1st approach_full_features. XGBoost with tunning parameters\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengamos predicciones para todas las propiedades en nuestro train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   5.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.9min\n",
      "613617.4667506837\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   7.9s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 3.1min\n",
      "619893.2199777354\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   6.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.9min\n",
      "611674.0503883368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"rozanecm_approach_2\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/on_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.6 - XGBoost optimum parameters + log precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_best_params\", xgb_model))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total=  32.4s\n",
      "0.22766361886806274\n",
      "CPU times: user 4min 16s, sys: 1.62 s, total: 4min 18s\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562383.1390865688\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: LightGBM optimizado with log price + features desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "                                  importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "                                  min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "                                  n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "                                  random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "                                  subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optimized.fit(X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train, sample_weight=None, init_score=None, eval_set=[(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1),y_test)], eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=10,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "y_scores = gbm_optimized.predict(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optimized.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y, sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"2nd approach. LightGBM previous grid search. Log(precio) y features descripcion\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengamos prediccionse para todas las propiedades en nuestro train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['cant_comodidades_en_desc',\n",
    "       'cant_palabras_positivas', 'cant_areas_dedicadas', 'cant_areas_verdes',\n",
    "       'cant_areas_entretenimiento_cerca', 'cant_lugares_cerca', 'planta_alta',\n",
    "       'planta_baja', 'tiene_bodega', 'oficina', 'cerca_o_en_esquina',\n",
    "       'cerca_o_en_avenida', 'comercial', 'tiene_servicio', 'edificio', 'casa',\n",
    "       'parte_de_lote', 'calle_cerrada', 'indica_frente_y_fondo',\n",
    "       'usa_easybroker', 'tiene_seguridad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columnas,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2.1: LightGBM with log price + features desc (grid search con nuevos features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 64.5min\n",
      "[Parallel(n_jobs=-1)]: Done 648 out of 648 | elapsed: 67.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.05, max_depth=25,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMRegressor(silent=False)\n",
    "\n",
    "param_dist = {\"boosting_type\":['gbdt','dart'],\n",
    "              \"max_depth\": [25,50,75],\n",
    "              \"learning_rate\" : [0.001,0.01,0.05,0.1],\n",
    "              \"num_leaves\": [300,900,1200],\n",
    "              \"n_estimators\": [50,100,200],\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(gbm, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"neg_mean_absolute_error\", verbose=5)\n",
    "grid_search.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21345376777287184\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train, sample_weight=None, init_score=None, eval_set=[(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1),y_test)], eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=10,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "y_scores = gbm_optimized.predict(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525219.9818594557\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y, sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"2.1 approach. LightGBM with new grid search for new features of desc. Log(precio)\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: Test promedio LightGBM + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_xgboost = pd.read_csv('../predictions/2019.11.27 - 01:34:19 by fcozza.csv')\n",
    "predictions_lightgbm = pd.read_csv('../predictions/2019.11.27 - 02:14:16 by fcozza.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promedio las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.merge(predictions_xgboost,predictions_lightgbm,on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['target'] = (pred['target_x'] + pred['target_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred [['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"3rd approach. Promedio entre xgboost y lightgbm\"\n",
    "save_submission(pred,description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No mejoró"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 4: Red neuronal con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    NN_model = Sequential()\n",
    "    \n",
    "    # The Input Layer :\n",
    "    NN_model.add(Dense(128, kernel_initializer='normal',input_dim = 83, activation='relu'))\n",
    "\n",
    "    # The Hidden Layers :\n",
    "    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "    # The Output Layer :\n",
    "    NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "    # Compile the network :\n",
    "    NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    return NN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobcs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"nn\", estimator))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.7s\n",
      "Train on 128640 samples, validate on 32160 samples\n",
      "Epoch 1/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.4812 - mean_absolute_error: 0.4812 - val_loss: 0.2934 - val_mean_absolute_error: 0.2934\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.29345, saving model to Weights-001--0.29345.hdf5\n",
      "Epoch 2/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.3043 - mean_absolute_error: 0.3043 - val_loss: 0.2725 - val_mean_absolute_error: 0.2725\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.29345 to 0.27253, saving model to Weights-002--0.27253.hdf5\n",
      "Epoch 3/500\n",
      "128640/128640 [==============================] - 6s 49us/step - loss: 0.2885 - mean_absolute_error: 0.2885 - val_loss: 0.3007 - val_mean_absolute_error: 0.3007\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27253\n",
      "Epoch 4/500\n",
      "128640/128640 [==============================] - 6s 49us/step - loss: 0.2785 - mean_absolute_error: 0.2785 - val_loss: 0.2644 - val_mean_absolute_error: 0.2644\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.27253 to 0.26442, saving model to Weights-004--0.26442.hdf5\n",
      "Epoch 5/500\n",
      "128640/128640 [==============================] - 6s 49us/step - loss: 0.2726 - mean_absolute_error: 0.2726 - val_loss: 0.2641 - val_mean_absolute_error: 0.2641\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.26442 to 0.26409, saving model to Weights-005--0.26409.hdf5\n",
      "Epoch 6/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2688 - mean_absolute_error: 0.2688 - val_loss: 0.2553 - val_mean_absolute_error: 0.2553\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26409 to 0.25534, saving model to Weights-006--0.25534.hdf5\n",
      "Epoch 7/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2630 - mean_absolute_error: 0.2630 - val_loss: 0.2618 - val_mean_absolute_error: 0.2618\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25534\n",
      "Epoch 8/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2610 - mean_absolute_error: 0.2610 - val_loss: 0.2562 - val_mean_absolute_error: 0.2562\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25534\n",
      "Epoch 9/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2592 - mean_absolute_error: 0.2592 - val_loss: 0.2557 - val_mean_absolute_error: 0.2557\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25534\n",
      "Epoch 10/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2582 - mean_absolute_error: 0.2582 - val_loss: 0.2559 - val_mean_absolute_error: 0.2559\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25534\n",
      "Epoch 11/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2572 - mean_absolute_error: 0.2572 - val_loss: 0.2657 - val_mean_absolute_error: 0.2657\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.25534\n",
      "Epoch 12/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2558 - mean_absolute_error: 0.2558 - val_loss: 0.2536 - val_mean_absolute_error: 0.2536\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.25534 to 0.25359, saving model to Weights-012--0.25359.hdf5\n",
      "Epoch 13/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2554 - mean_absolute_error: 0.2554 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.25359 to 0.25144, saving model to Weights-013--0.25144.hdf5\n",
      "Epoch 14/500\n",
      "128640/128640 [==============================] - 6s 51us/step - loss: 0.2529 - mean_absolute_error: 0.2529 - val_loss: 0.2845 - val_mean_absolute_error: 0.2845\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25144\n",
      "Epoch 15/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2540 - mean_absolute_error: 0.2540 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.25144 to 0.25142, saving model to Weights-015--0.25142.hdf5\n",
      "Epoch 16/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2513 - mean_absolute_error: 0.2513 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.25142 to 0.24849, saving model to Weights-016--0.24849.hdf5\n",
      "Epoch 17/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2523 - mean_absolute_error: 0.2523 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.24849\n",
      "Epoch 18/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2504 - mean_absolute_error: 0.2504 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.24849\n",
      "Epoch 19/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2510 - mean_absolute_error: 0.2510 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.24849\n",
      "Epoch 20/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2493 - mean_absolute_error: 0.2493 - val_loss: 0.2483 - val_mean_absolute_error: 0.2483\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.24849 to 0.24829, saving model to Weights-020--0.24829.hdf5\n",
      "Epoch 21/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2492 - mean_absolute_error: 0.2492 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.24829\n",
      "Epoch 22/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2470 - mean_absolute_error: 0.2470 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.24829\n",
      "Epoch 23/500\n",
      "128640/128640 [==============================] - 6s 51us/step - loss: 0.2479 - mean_absolute_error: 0.2479 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.24829\n",
      "Epoch 24/500\n",
      "128640/128640 [==============================] - 6s 51us/step - loss: 0.2470 - mean_absolute_error: 0.2470 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.24829 to 0.24697, saving model to Weights-024--0.24697.hdf5\n",
      "Epoch 25/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2474 - mean_absolute_error: 0.2474 - val_loss: 0.2707 - val_mean_absolute_error: 0.2707\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.24697\n",
      "Epoch 26/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2461 - mean_absolute_error: 0.2461 - val_loss: 0.2550 - val_mean_absolute_error: 0.2550\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.24697\n",
      "Epoch 27/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2454 - mean_absolute_error: 0.2454 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.24697\n",
      "Epoch 28/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2449 - mean_absolute_error: 0.2449 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.24697\n",
      "Epoch 29/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2487 - val_mean_absolute_error: 0.2487\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.24697\n",
      "Epoch 30/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2446 - mean_absolute_error: 0.2446 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24697\n",
      "Epoch 31/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2451 - mean_absolute_error: 0.2451 - val_loss: 0.2493 - val_mean_absolute_error: 0.2493\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24697\n",
      "Epoch 32/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2439 - mean_absolute_error: 0.2439 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24697\n",
      "Epoch 33/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2428 - mean_absolute_error: 0.2428 - val_loss: 0.2622 - val_mean_absolute_error: 0.2622\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24697\n",
      "Epoch 34/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2433 - mean_absolute_error: 0.2433 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24697\n",
      "Epoch 35/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2427 - mean_absolute_error: 0.2427 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24697\n",
      "Epoch 36/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2421 - mean_absolute_error: 0.2421 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24697\n",
      "Epoch 37/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2424 - mean_absolute_error: 0.2424 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.24697\n",
      "Epoch 38/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2431 - mean_absolute_error: 0.2431 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.24697\n",
      "Epoch 39/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2419 - mean_absolute_error: 0.2419 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.24697\n",
      "Epoch 40/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2416 - mean_absolute_error: 0.2416 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.24697\n",
      "Epoch 41/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2413 - mean_absolute_error: 0.2413 - val_loss: 0.2627 - val_mean_absolute_error: 0.2627\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.24697\n",
      "Epoch 42/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2408 - mean_absolute_error: 0.2408 - val_loss: 0.2721 - val_mean_absolute_error: 0.2721\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.24697\n",
      "Epoch 43/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2419 - mean_absolute_error: 0.2419 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.24697\n",
      "Epoch 44/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2403 - mean_absolute_error: 0.2403 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.24697\n",
      "Epoch 45/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2402 - mean_absolute_error: 0.2402 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.24697\n",
      "Epoch 46/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2397 - mean_absolute_error: 0.2397 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.24697\n",
      "Epoch 47/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2403 - mean_absolute_error: 0.2403 - val_loss: 0.2739 - val_mean_absolute_error: 0.2739\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.24697\n",
      "Epoch 48/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2395 - mean_absolute_error: 0.2395 - val_loss: 0.2672 - val_mean_absolute_error: 0.2672\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.24697\n",
      "Epoch 49/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2400 - mean_absolute_error: 0.2400 - val_loss: 0.2605 - val_mean_absolute_error: 0.2605\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.24697\n",
      "Epoch 50/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2393 - mean_absolute_error: 0.2393 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.24697\n",
      "Epoch 51/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2392 - mean_absolute_error: 0.2392 - val_loss: 0.2544 - val_mean_absolute_error: 0.2544\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.24697\n",
      "Epoch 52/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2392 - mean_absolute_error: 0.2392 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.24697\n",
      "Epoch 53/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2393 - mean_absolute_error: 0.2393 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.24697\n",
      "Epoch 54/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2384 - mean_absolute_error: 0.2384 - val_loss: 0.2576 - val_mean_absolute_error: 0.2576\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.24697\n",
      "Epoch 55/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2378 - mean_absolute_error: 0.2378 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.24697\n",
      "Epoch 56/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2379 - mean_absolute_error: 0.2379 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.24697\n",
      "Epoch 57/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2382 - mean_absolute_error: 0.2382 - val_loss: 0.2799 - val_mean_absolute_error: 0.2799\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.24697\n",
      "Epoch 58/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2374 - mean_absolute_error: 0.2374 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.24697\n",
      "Epoch 59/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2379 - mean_absolute_error: 0.2379 - val_loss: 0.2537 - val_mean_absolute_error: 0.2537\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.24697\n",
      "Epoch 60/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2372 - mean_absolute_error: 0.2372 - val_loss: 0.2608 - val_mean_absolute_error: 0.2608\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.24697\n",
      "Epoch 61/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2372 - mean_absolute_error: 0.2372 - val_loss: 0.2567 - val_mean_absolute_error: 0.2567\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.24697\n",
      "Epoch 62/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2364 - mean_absolute_error: 0.2364 - val_loss: 0.2575 - val_mean_absolute_error: 0.2575\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.24697\n",
      "Epoch 63/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2366 - mean_absolute_error: 0.2366 - val_loss: 0.2546 - val_mean_absolute_error: 0.2546\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.24697\n",
      "Epoch 64/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2366 - mean_absolute_error: 0.2366 - val_loss: 0.2555 - val_mean_absolute_error: 0.2555\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.24697\n",
      "Epoch 65/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2364 - mean_absolute_error: 0.2364 - val_loss: 0.2490 - val_mean_absolute_error: 0.2490\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.24697\n",
      "Epoch 66/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2372 - mean_absolute_error: 0.2372 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.24697\n",
      "Epoch 67/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2604 - val_mean_absolute_error: 0.2604\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.24697\n",
      "Epoch 68/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2363 - mean_absolute_error: 0.2363 - val_loss: 0.2787 - val_mean_absolute_error: 0.2787\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.24697\n",
      "Epoch 69/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2363 - mean_absolute_error: 0.2363 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.24697\n",
      "Epoch 70/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2359 - mean_absolute_error: 0.2359 - val_loss: 0.2579 - val_mean_absolute_error: 0.2579\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.24697\n",
      "Epoch 71/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2357 - mean_absolute_error: 0.2357 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24697\n",
      "Epoch 72/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2355 - mean_absolute_error: 0.2355 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24697\n",
      "Epoch 73/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2356 - mean_absolute_error: 0.2356 - val_loss: 0.2491 - val_mean_absolute_error: 0.2491\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24697\n",
      "Epoch 74/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24697\n",
      "Epoch 75/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2353 - mean_absolute_error: 0.2353 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24697\n",
      "Epoch 76/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2349 - mean_absolute_error: 0.2349 - val_loss: 0.2490 - val_mean_absolute_error: 0.2490\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.24697\n",
      "Epoch 77/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.24697\n",
      "Epoch 78/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2349 - mean_absolute_error: 0.2349 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24697\n",
      "Epoch 79/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2347 - mean_absolute_error: 0.2347 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24697\n",
      "Epoch 80/500\n",
      "128640/128640 [==============================] - 6s 51us/step - loss: 0.2345 - mean_absolute_error: 0.2345 - val_loss: 0.2551 - val_mean_absolute_error: 0.2551\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.24697\n",
      "Epoch 81/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2349 - mean_absolute_error: 0.2349 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.24697\n",
      "Epoch 82/500\n",
      "128640/128640 [==============================] - 6s 51us/step - loss: 0.2344 - mean_absolute_error: 0.2344 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24697\n",
      "Epoch 83/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2344 - mean_absolute_error: 0.2344 - val_loss: 0.2622 - val_mean_absolute_error: 0.2622\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.24697\n",
      "Epoch 84/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24697\n",
      "Epoch 85/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2339 - mean_absolute_error: 0.2339 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.24697\n",
      "Epoch 86/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2339 - mean_absolute_error: 0.2339 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24697\n",
      "Epoch 87/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24697\n",
      "Epoch 88/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24697\n",
      "Epoch 89/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2337 - mean_absolute_error: 0.2337 - val_loss: 0.2530 - val_mean_absolute_error: 0.2530\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24697\n",
      "Epoch 90/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2336 - mean_absolute_error: 0.2336 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24697\n",
      "Epoch 91/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2329 - mean_absolute_error: 0.2329 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24697\n",
      "Epoch 92/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2598 - val_mean_absolute_error: 0.2598\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.24697\n",
      "Epoch 93/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.24697\n",
      "Epoch 94/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.24697\n",
      "Epoch 95/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2492 - val_mean_absolute_error: 0.2492\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.24697\n",
      "Epoch 96/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2553 - val_mean_absolute_error: 0.2553\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.24697\n",
      "Epoch 97/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.24697\n",
      "Epoch 98/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2327 - mean_absolute_error: 0.2327 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.24697\n",
      "Epoch 99/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.24697\n",
      "Epoch 100/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.24697\n",
      "Epoch 101/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2329 - mean_absolute_error: 0.2329 - val_loss: 0.2629 - val_mean_absolute_error: 0.2629\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.24697\n",
      "Epoch 102/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.24697\n",
      "Epoch 103/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2492 - val_mean_absolute_error: 0.2492\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.24697\n",
      "Epoch 104/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2575 - val_mean_absolute_error: 0.2575\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.24697\n",
      "Epoch 105/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.24697\n",
      "Epoch 106/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.24697 to 0.24687, saving model to Weights-106--0.24687.hdf5\n",
      "Epoch 107/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2578 - val_mean_absolute_error: 0.2578\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.24687\n",
      "Epoch 108/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24687\n",
      "Epoch 109/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24687\n",
      "Epoch 110/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2548 - val_mean_absolute_error: 0.2548\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.24687\n",
      "Epoch 111/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24687\n",
      "Epoch 112/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2491 - val_mean_absolute_error: 0.2491\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24687\n",
      "Epoch 113/500\n",
      "128640/128640 [==============================] - 6s 50us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24687\n",
      "Epoch 114/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.24687\n",
      "Epoch 115/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.24687\n",
      "Epoch 116/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.24687\n",
      "Epoch 117/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.24687\n",
      "Epoch 118/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2596 - val_mean_absolute_error: 0.2596\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.24687\n",
      "Epoch 119/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.24687\n",
      "Epoch 120/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.24687\n",
      "Epoch 121/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.24687\n",
      "Epoch 122/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.24687\n",
      "Epoch 123/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2302 - mean_absolute_error: 0.2302 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.24687\n",
      "Epoch 124/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2693 - val_mean_absolute_error: 0.2693\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.24687\n",
      "Epoch 125/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2551 - val_mean_absolute_error: 0.2551\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.24687\n",
      "Epoch 126/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2304 - mean_absolute_error: 0.2304 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.24687\n",
      "Epoch 127/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2301 - mean_absolute_error: 0.2301 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.24687\n",
      "Epoch 128/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2303 - mean_absolute_error: 0.2303 - val_loss: 0.2629 - val_mean_absolute_error: 0.2629\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.24687\n",
      "Epoch 129/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2300 - mean_absolute_error: 0.2300 - val_loss: 0.2492 - val_mean_absolute_error: 0.2492\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.24687\n",
      "Epoch 130/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2298 - mean_absolute_error: 0.2298 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.24687\n",
      "Epoch 131/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2297 - mean_absolute_error: 0.2297 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.24687\n",
      "Epoch 132/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2301 - mean_absolute_error: 0.2301 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.24687\n",
      "Epoch 133/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2298 - mean_absolute_error: 0.2298 - val_loss: 0.2547 - val_mean_absolute_error: 0.2547\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.24687\n",
      "Epoch 134/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2296 - mean_absolute_error: 0.2296 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.24687\n",
      "Epoch 135/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2300 - mean_absolute_error: 0.2300 - val_loss: 0.2581 - val_mean_absolute_error: 0.2581\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.24687\n",
      "Epoch 136/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2296 - mean_absolute_error: 0.2296 - val_loss: 0.2490 - val_mean_absolute_error: 0.2490\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.24687\n",
      "Epoch 137/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2295 - mean_absolute_error: 0.2295 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.24687\n",
      "Epoch 138/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2298 - mean_absolute_error: 0.2298 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.24687\n",
      "Epoch 139/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2293 - mean_absolute_error: 0.2293 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.24687\n",
      "Epoch 140/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2292 - mean_absolute_error: 0.2292 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.24687\n",
      "Epoch 141/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2286 - mean_absolute_error: 0.2286 - val_loss: 0.2490 - val_mean_absolute_error: 0.2490\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.24687\n",
      "Epoch 142/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2294 - mean_absolute_error: 0.2294 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.24687\n",
      "Epoch 143/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2292 - mean_absolute_error: 0.2292 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.24687\n",
      "Epoch 144/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2289 - mean_absolute_error: 0.2289 - val_loss: 0.2560 - val_mean_absolute_error: 0.2560\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.24687\n",
      "Epoch 145/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2291 - mean_absolute_error: 0.2291 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.24687\n",
      "Epoch 146/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2284 - mean_absolute_error: 0.2284 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.24687\n",
      "Epoch 147/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2287 - mean_absolute_error: 0.2287 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.24687\n",
      "Epoch 148/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2285 - mean_absolute_error: 0.2285 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.24687\n",
      "Epoch 149/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2284 - mean_absolute_error: 0.2284 - val_loss: 0.2570 - val_mean_absolute_error: 0.2570\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.24687\n",
      "Epoch 150/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2287 - mean_absolute_error: 0.2287 - val_loss: 0.2552 - val_mean_absolute_error: 0.2552\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.24687\n",
      "Epoch 151/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2284 - mean_absolute_error: 0.2284 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.24687\n",
      "Epoch 152/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2284 - mean_absolute_error: 0.2284 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.24687\n",
      "Epoch 153/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2284 - mean_absolute_error: 0.2284 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.24687\n",
      "Epoch 154/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2282 - mean_absolute_error: 0.2282 - val_loss: 0.2490 - val_mean_absolute_error: 0.2490\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.24687\n",
      "Epoch 155/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2278 - mean_absolute_error: 0.2278 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.24687\n",
      "Epoch 156/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2284 - mean_absolute_error: 0.2284 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.24687\n",
      "Epoch 157/500\n",
      "128640/128640 [==============================] - 7s 55us/step - loss: 0.2285 - mean_absolute_error: 0.2285 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.24687\n",
      "Epoch 158/500\n",
      "128640/128640 [==============================] - 7s 54us/step - loss: 0.2281 - mean_absolute_error: 0.2281 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.24687\n",
      "Epoch 159/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2280 - mean_absolute_error: 0.2280 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.24687\n",
      "Epoch 160/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2279 - mean_absolute_error: 0.2279 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.24687\n",
      "Epoch 161/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2276 - mean_absolute_error: 0.2276 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.24687\n",
      "Epoch 162/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2276 - mean_absolute_error: 0.2276 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.24687\n",
      "Epoch 163/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2274 - mean_absolute_error: 0.2274 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.24687\n",
      "Epoch 164/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2272 - mean_absolute_error: 0.2272 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.24687\n",
      "Epoch 165/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2275 - mean_absolute_error: 0.2275 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.24687\n",
      "Epoch 166/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2272 - mean_absolute_error: 0.2272 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.24687\n",
      "Epoch 167/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2276 - mean_absolute_error: 0.2276 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.24687\n",
      "Epoch 168/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2272 - mean_absolute_error: 0.2272 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.24687\n",
      "Epoch 169/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2271 - mean_absolute_error: 0.2271 - val_loss: 0.2493 - val_mean_absolute_error: 0.2493\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.24687\n",
      "Epoch 170/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2273 - mean_absolute_error: 0.2273 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.24687\n",
      "Epoch 171/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2274 - mean_absolute_error: 0.2274 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.24687\n",
      "Epoch 172/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2274 - mean_absolute_error: 0.2274 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.24687\n",
      "Epoch 173/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2274 - mean_absolute_error: 0.2274 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.24687\n",
      "Epoch 174/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2267 - mean_absolute_error: 0.2267 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.24687\n",
      "Epoch 175/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2270 - mean_absolute_error: 0.2270 - val_loss: 0.2545 - val_mean_absolute_error: 0.2545\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.24687\n",
      "Epoch 176/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2273 - mean_absolute_error: 0.2273 - val_loss: 0.2530 - val_mean_absolute_error: 0.2530\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.24687\n",
      "Epoch 177/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2271 - mean_absolute_error: 0.2271 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.24687\n",
      "Epoch 178/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2269 - mean_absolute_error: 0.2269 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.24687\n",
      "Epoch 179/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2268 - mean_absolute_error: 0.2268 - val_loss: 0.2587 - val_mean_absolute_error: 0.2587\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.24687\n",
      "Epoch 180/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2268 - mean_absolute_error: 0.2268 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.24687\n",
      "Epoch 181/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2267 - mean_absolute_error: 0.2267 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.24687\n",
      "Epoch 182/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2270 - mean_absolute_error: 0.2270 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.24687\n",
      "Epoch 183/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2270 - mean_absolute_error: 0.2270 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.24687\n",
      "Epoch 184/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2265 - mean_absolute_error: 0.2265 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.24687\n",
      "Epoch 185/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2264 - mean_absolute_error: 0.2264 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.24687\n",
      "Epoch 186/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2264 - mean_absolute_error: 0.2264 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.24687\n",
      "Epoch 187/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2264 - mean_absolute_error: 0.2264 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.24687\n",
      "Epoch 188/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2267 - mean_absolute_error: 0.2267 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.24687\n",
      "Epoch 189/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2262 - mean_absolute_error: 0.2262 - val_loss: 0.2572 - val_mean_absolute_error: 0.2572\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.24687\n",
      "Epoch 190/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2262 - mean_absolute_error: 0.2262 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.24687\n",
      "Epoch 191/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2268 - mean_absolute_error: 0.2268 - val_loss: 0.2550 - val_mean_absolute_error: 0.2550\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.24687\n",
      "Epoch 192/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2258 - mean_absolute_error: 0.2258 - val_loss: 0.2564 - val_mean_absolute_error: 0.2564\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.24687\n",
      "Epoch 193/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2265 - mean_absolute_error: 0.2265 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.24687\n",
      "Epoch 194/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2259 - mean_absolute_error: 0.2259 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.24687\n",
      "Epoch 195/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2263 - mean_absolute_error: 0.2263 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.24687\n",
      "Epoch 196/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2258 - mean_absolute_error: 0.2258 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.24687\n",
      "Epoch 197/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2262 - mean_absolute_error: 0.2262 - val_loss: 0.2492 - val_mean_absolute_error: 0.2492\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.24687\n",
      "Epoch 198/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2261 - mean_absolute_error: 0.2261 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.24687\n",
      "Epoch 199/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2258 - mean_absolute_error: 0.2258 - val_loss: 0.2666 - val_mean_absolute_error: 0.2666\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.24687\n",
      "Epoch 200/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2260 - mean_absolute_error: 0.2260 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.24687\n",
      "Epoch 201/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2257 - mean_absolute_error: 0.2257 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.24687\n",
      "Epoch 202/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2256 - mean_absolute_error: 0.2256 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.24687\n",
      "Epoch 203/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2260 - mean_absolute_error: 0.2260 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.24687\n",
      "Epoch 204/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2254 - mean_absolute_error: 0.2254 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.24687\n",
      "Epoch 205/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2252 - mean_absolute_error: 0.2252 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.24687\n",
      "Epoch 206/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2255 - mean_absolute_error: 0.2255 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.24687\n",
      "Epoch 207/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2252 - mean_absolute_error: 0.2252 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.24687\n",
      "Epoch 208/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2254 - mean_absolute_error: 0.2254 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.24687\n",
      "Epoch 209/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2254 - mean_absolute_error: 0.2254 - val_loss: 0.2553 - val_mean_absolute_error: 0.2553\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.24687\n",
      "Epoch 210/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2254 - mean_absolute_error: 0.2254 - val_loss: 0.2540 - val_mean_absolute_error: 0.2540\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.24687\n",
      "Epoch 211/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2251 - mean_absolute_error: 0.2251 - val_loss: 0.2554 - val_mean_absolute_error: 0.2554\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.24687\n",
      "Epoch 212/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2253 - mean_absolute_error: 0.2253 - val_loss: 0.2578 - val_mean_absolute_error: 0.2578\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.24687\n",
      "Epoch 213/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2253 - mean_absolute_error: 0.2253 - val_loss: 0.2607 - val_mean_absolute_error: 0.2607\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.24687\n",
      "Epoch 214/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2254 - mean_absolute_error: 0.2254 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.24687\n",
      "Epoch 215/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2252 - mean_absolute_error: 0.2252 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.24687\n",
      "Epoch 216/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2252 - mean_absolute_error: 0.2252 - val_loss: 0.2554 - val_mean_absolute_error: 0.2554\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.24687\n",
      "Epoch 217/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2248 - mean_absolute_error: 0.2248 - val_loss: 0.2491 - val_mean_absolute_error: 0.2491\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.24687\n",
      "Epoch 218/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2248 - mean_absolute_error: 0.2248 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.24687\n",
      "Epoch 219/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2244 - mean_absolute_error: 0.2244 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.24687\n",
      "Epoch 220/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2247 - mean_absolute_error: 0.2247 - val_loss: 0.2537 - val_mean_absolute_error: 0.2537\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.24687\n",
      "Epoch 221/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2243 - mean_absolute_error: 0.2243 - val_loss: 0.2493 - val_mean_absolute_error: 0.2493\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.24687\n",
      "Epoch 222/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2246 - mean_absolute_error: 0.2246 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.24687\n",
      "Epoch 223/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2243 - mean_absolute_error: 0.2243 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.24687\n",
      "Epoch 224/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2246 - mean_absolute_error: 0.2246 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.24687\n",
      "Epoch 225/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2242 - mean_absolute_error: 0.2242 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.24687\n",
      "Epoch 226/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2244 - mean_absolute_error: 0.2244 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.24687\n",
      "Epoch 227/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2241 - mean_absolute_error: 0.2241 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.24687\n",
      "Epoch 228/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2240 - mean_absolute_error: 0.2240 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.24687\n",
      "Epoch 229/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2240 - mean_absolute_error: 0.2240 - val_loss: 0.2565 - val_mean_absolute_error: 0.2565\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.24687\n",
      "Epoch 230/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2245 - mean_absolute_error: 0.2245 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.24687\n",
      "Epoch 231/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2242 - mean_absolute_error: 0.2242 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.24687\n",
      "Epoch 232/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2237 - mean_absolute_error: 0.2237 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.24687\n",
      "Epoch 233/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2237 - mean_absolute_error: 0.2237 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.24687\n",
      "Epoch 234/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2239 - mean_absolute_error: 0.2239 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.24687\n",
      "Epoch 235/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2239 - mean_absolute_error: 0.2239 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.24687\n",
      "Epoch 236/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2238 - mean_absolute_error: 0.2238 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.24687\n",
      "Epoch 237/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2237 - mean_absolute_error: 0.2237 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.24687\n",
      "Epoch 238/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2235 - mean_absolute_error: 0.2235 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.24687\n",
      "Epoch 239/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2236 - mean_absolute_error: 0.2236 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.24687\n",
      "Epoch 240/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2237 - mean_absolute_error: 0.2237 - val_loss: 0.2501 - val_mean_absolute_error: 0.2501\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.24687\n",
      "Epoch 241/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2233 - mean_absolute_error: 0.2233 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.24687\n",
      "Epoch 242/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2234 - mean_absolute_error: 0.2234 - val_loss: 0.2493 - val_mean_absolute_error: 0.2493\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.24687\n",
      "Epoch 243/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2233 - mean_absolute_error: 0.2233 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.24687\n",
      "Epoch 244/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2234 - mean_absolute_error: 0.2234 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.24687\n",
      "Epoch 245/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2232 - mean_absolute_error: 0.2232 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.24687\n",
      "Epoch 246/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2230 - mean_absolute_error: 0.2230 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.24687\n",
      "Epoch 247/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2231 - mean_absolute_error: 0.2231 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.24687\n",
      "Epoch 248/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2231 - mean_absolute_error: 0.2231 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.24687\n",
      "Epoch 249/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2226 - mean_absolute_error: 0.2226 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.24687\n",
      "Epoch 250/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2225 - mean_absolute_error: 0.2225 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.24687\n",
      "Epoch 251/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2229 - mean_absolute_error: 0.2229 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.24687\n",
      "Epoch 252/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2228 - mean_absolute_error: 0.2228 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.24687\n",
      "Epoch 253/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2228 - mean_absolute_error: 0.2228 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.24687\n",
      "Epoch 254/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2227 - mean_absolute_error: 0.2227 - val_loss: 0.2566 - val_mean_absolute_error: 0.2566\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.24687\n",
      "Epoch 255/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2227 - mean_absolute_error: 0.2227 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.24687\n",
      "Epoch 256/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2229 - mean_absolute_error: 0.2229 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.24687\n",
      "Epoch 257/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2227 - mean_absolute_error: 0.2227 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.24687\n",
      "Epoch 258/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2225 - mean_absolute_error: 0.2225 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.24687\n",
      "Epoch 259/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2226 - mean_absolute_error: 0.2226 - val_loss: 0.2501 - val_mean_absolute_error: 0.2501\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.24687\n",
      "Epoch 260/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2226 - mean_absolute_error: 0.2226 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.24687\n",
      "Epoch 261/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2223 - mean_absolute_error: 0.2223 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.24687\n",
      "Epoch 262/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2221 - mean_absolute_error: 0.2221 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.24687\n",
      "Epoch 263/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2221 - mean_absolute_error: 0.2221 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.24687\n",
      "Epoch 264/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2221 - mean_absolute_error: 0.2221 - val_loss: 0.2493 - val_mean_absolute_error: 0.2493\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.24687\n",
      "Epoch 265/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2219 - mean_absolute_error: 0.2219 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.24687\n",
      "Epoch 266/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2220 - mean_absolute_error: 0.2220 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.24687\n",
      "Epoch 267/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2222 - mean_absolute_error: 0.2222 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.24687\n",
      "Epoch 268/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2223 - mean_absolute_error: 0.2223 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.24687\n",
      "Epoch 269/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2216 - mean_absolute_error: 0.2216 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.24687\n",
      "Epoch 270/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2217 - mean_absolute_error: 0.2217 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.24687\n",
      "Epoch 271/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2222 - mean_absolute_error: 0.2222 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.24687\n",
      "Epoch 272/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2213 - mean_absolute_error: 0.2213 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.24687\n",
      "Epoch 273/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.24687\n",
      "Epoch 274/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.24687\n",
      "Epoch 275/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2217 - mean_absolute_error: 0.2217 - val_loss: 0.2554 - val_mean_absolute_error: 0.2554\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.24687\n",
      "Epoch 276/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2214 - mean_absolute_error: 0.2214 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.24687\n",
      "Epoch 277/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2214 - mean_absolute_error: 0.2214 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.24687\n",
      "Epoch 278/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2218 - mean_absolute_error: 0.2218 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.24687\n",
      "Epoch 279/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.24687\n",
      "Epoch 280/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2216 - mean_absolute_error: 0.2216 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.24687\n",
      "Epoch 281/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2214 - mean_absolute_error: 0.2214 - val_loss: 0.2600 - val_mean_absolute_error: 0.2600\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.24687\n",
      "Epoch 282/500\n",
      "128640/128640 [==============================] - 7s 51us/step - loss: 0.2215 - mean_absolute_error: 0.2215 - val_loss: 0.2547 - val_mean_absolute_error: 0.2547\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.24687\n",
      "Epoch 283/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2211 - mean_absolute_error: 0.2211 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.24687\n",
      "Epoch 284/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2211 - mean_absolute_error: 0.2211 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.24687\n",
      "Epoch 285/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2213 - mean_absolute_error: 0.2213 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.24687\n",
      "Epoch 286/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2212 - mean_absolute_error: 0.2212 - val_loss: 0.2535 - val_mean_absolute_error: 0.2535\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.24687\n",
      "Epoch 287/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2212 - mean_absolute_error: 0.2212 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.24687\n",
      "Epoch 288/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2210 - mean_absolute_error: 0.2210 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.24687\n",
      "Epoch 289/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2210 - mean_absolute_error: 0.2210 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.24687\n",
      "Epoch 290/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2208 - mean_absolute_error: 0.2208 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.24687\n",
      "Epoch 291/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2210 - mean_absolute_error: 0.2210 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.24687\n",
      "Epoch 292/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2212 - mean_absolute_error: 0.2212 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.24687\n",
      "Epoch 293/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2209 - mean_absolute_error: 0.2209 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.24687\n",
      "Epoch 294/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2208 - mean_absolute_error: 0.2208 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.24687\n",
      "Epoch 295/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2207 - mean_absolute_error: 0.2207 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.24687\n",
      "Epoch 296/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2208 - mean_absolute_error: 0.2208 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.24687\n",
      "Epoch 297/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2206 - mean_absolute_error: 0.2206 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.24687\n",
      "Epoch 298/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2206 - mean_absolute_error: 0.2206 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.24687\n",
      "Epoch 299/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2204 - mean_absolute_error: 0.2204 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.24687\n",
      "Epoch 300/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.24687\n",
      "Epoch 301/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2206 - mean_absolute_error: 0.2206 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.24687\n",
      "Epoch 302/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.24687\n",
      "Epoch 303/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2206 - mean_absolute_error: 0.2206 - val_loss: 0.2580 - val_mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.24687\n",
      "Epoch 304/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2206 - mean_absolute_error: 0.2206 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.24687\n",
      "Epoch 305/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2525 - val_mean_absolute_error: 0.2525\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.24687\n",
      "Epoch 306/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2202 - mean_absolute_error: 0.2202 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.24687\n",
      "Epoch 307/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2205 - mean_absolute_error: 0.2205 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.24687\n",
      "Epoch 308/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2203 - mean_absolute_error: 0.2203 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.24687\n",
      "Epoch 309/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2201 - mean_absolute_error: 0.2201 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.24687\n",
      "Epoch 310/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2203 - mean_absolute_error: 0.2203 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.24687\n",
      "Epoch 311/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2204 - mean_absolute_error: 0.2204 - val_loss: 0.2543 - val_mean_absolute_error: 0.2543\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.24687\n",
      "Epoch 312/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2201 - mean_absolute_error: 0.2201 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.24687\n",
      "Epoch 313/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2200 - mean_absolute_error: 0.2200 - val_loss: 0.2495 - val_mean_absolute_error: 0.2495\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.24687\n",
      "Epoch 314/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2200 - mean_absolute_error: 0.2200 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.24687\n",
      "Epoch 315/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2200 - mean_absolute_error: 0.2200 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.24687\n",
      "Epoch 316/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2201 - mean_absolute_error: 0.2201 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.24687\n",
      "Epoch 317/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2201 - mean_absolute_error: 0.2201 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.24687\n",
      "Epoch 318/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2199 - mean_absolute_error: 0.2199 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.24687\n",
      "Epoch 319/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2198 - mean_absolute_error: 0.2198 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.24687\n",
      "Epoch 320/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2200 - mean_absolute_error: 0.2200 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.24687\n",
      "Epoch 321/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2197 - mean_absolute_error: 0.2197 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.24687\n",
      "Epoch 322/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2197 - mean_absolute_error: 0.2197 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.24687\n",
      "Epoch 323/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2199 - mean_absolute_error: 0.2199 - val_loss: 0.2501 - val_mean_absolute_error: 0.2501\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.24687\n",
      "Epoch 324/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2197 - mean_absolute_error: 0.2197 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.24687\n",
      "Epoch 325/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.2496 - val_mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.24687\n",
      "Epoch 326/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2199 - mean_absolute_error: 0.2199 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.24687\n",
      "Epoch 327/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2197 - mean_absolute_error: 0.2197 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.24687\n",
      "Epoch 328/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.24687\n",
      "Epoch 329/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2197 - mean_absolute_error: 0.2197 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.24687\n",
      "Epoch 330/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2198 - mean_absolute_error: 0.2198 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.24687\n",
      "Epoch 331/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.24687\n",
      "Epoch 332/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2195 - mean_absolute_error: 0.2195 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.24687\n",
      "Epoch 333/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2196 - mean_absolute_error: 0.2196 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.24687\n",
      "Epoch 334/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2193 - mean_absolute_error: 0.2193 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.24687\n",
      "Epoch 335/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2195 - mean_absolute_error: 0.2195 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.24687\n",
      "Epoch 336/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2194 - mean_absolute_error: 0.2194 - val_loss: 0.2536 - val_mean_absolute_error: 0.2536\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.24687\n",
      "Epoch 337/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.24687\n",
      "Epoch 338/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.24687\n",
      "Epoch 339/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2542 - val_mean_absolute_error: 0.2542\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.24687\n",
      "Epoch 340/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2195 - mean_absolute_error: 0.2195 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.24687\n",
      "Epoch 341/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2194 - mean_absolute_error: 0.2194 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.24687\n",
      "Epoch 342/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2194 - mean_absolute_error: 0.2194 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.24687\n",
      "Epoch 343/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2193 - mean_absolute_error: 0.2193 - val_loss: 0.2498 - val_mean_absolute_error: 0.2498\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.24687\n",
      "Epoch 344/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2193 - mean_absolute_error: 0.2193 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.24687\n",
      "Epoch 345/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2193 - mean_absolute_error: 0.2193 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.24687\n",
      "Epoch 346/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.24687\n",
      "Epoch 347/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.24687\n",
      "Epoch 348/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.24687\n",
      "Epoch 349/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2192 - mean_absolute_error: 0.2192 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.24687\n",
      "Epoch 350/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.24687\n",
      "Epoch 351/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.24687\n",
      "Epoch 352/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.24687\n",
      "Epoch 353/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.24687\n",
      "Epoch 354/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2191 - mean_absolute_error: 0.2191 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.24687\n",
      "Epoch 355/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.24687\n",
      "Epoch 356/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2502 - val_mean_absolute_error: 0.2502\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.24687\n",
      "Epoch 357/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.24687\n",
      "Epoch 358/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2553 - val_mean_absolute_error: 0.2553\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.24687\n",
      "Epoch 359/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.24687\n",
      "Epoch 360/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.24687\n",
      "Epoch 361/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.24687\n",
      "Epoch 362/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2535 - val_mean_absolute_error: 0.2535\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.24687\n",
      "Epoch 363/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2187 - mean_absolute_error: 0.2187 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.24687\n",
      "Epoch 364/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2190 - mean_absolute_error: 0.2190 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.24687\n",
      "Epoch 365/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.24687\n",
      "Epoch 366/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.24687\n",
      "Epoch 367/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2546 - val_mean_absolute_error: 0.2546\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.24687\n",
      "Epoch 368/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.24687\n",
      "Epoch 369/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.24687\n",
      "Epoch 370/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2503 - val_mean_absolute_error: 0.2503\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.24687\n",
      "Epoch 371/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2188 - mean_absolute_error: 0.2188 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.24687\n",
      "Epoch 372/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2189 - mean_absolute_error: 0.2189 - val_loss: 0.2558 - val_mean_absolute_error: 0.2558\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.24687\n",
      "Epoch 373/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2187 - mean_absolute_error: 0.2187 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.24687\n",
      "Epoch 374/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2186 - mean_absolute_error: 0.2186 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.24687\n",
      "Epoch 375/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2185 - mean_absolute_error: 0.2185 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.24687\n",
      "Epoch 376/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.24687\n",
      "Epoch 377/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2185 - mean_absolute_error: 0.2185 - val_loss: 0.2558 - val_mean_absolute_error: 0.2558\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.24687\n",
      "Epoch 378/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2187 - mean_absolute_error: 0.2187 - val_loss: 0.2550 - val_mean_absolute_error: 0.2550\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.24687\n",
      "Epoch 379/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.24687\n",
      "Epoch 380/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.24687\n",
      "Epoch 381/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.24687\n",
      "Epoch 382/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.24687\n",
      "Epoch 383/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.24687\n",
      "Epoch 384/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2185 - mean_absolute_error: 0.2185 - val_loss: 0.2555 - val_mean_absolute_error: 0.2555\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.24687\n",
      "Epoch 385/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.24687\n",
      "Epoch 386/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.24687\n",
      "Epoch 387/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2186 - mean_absolute_error: 0.2186 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.24687\n",
      "Epoch 388/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2184 - mean_absolute_error: 0.2184 - val_loss: 0.2512 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.24687\n",
      "Epoch 389/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2181 - mean_absolute_error: 0.2181 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.24687\n",
      "Epoch 390/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2182 - mean_absolute_error: 0.2182 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.24687\n",
      "Epoch 391/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.24687\n",
      "Epoch 392/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2181 - mean_absolute_error: 0.2181 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.24687\n",
      "Epoch 393/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2182 - mean_absolute_error: 0.2182 - val_loss: 0.2555 - val_mean_absolute_error: 0.2555\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.24687\n",
      "Epoch 394/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2537 - val_mean_absolute_error: 0.2537\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.24687\n",
      "Epoch 395/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.24687\n",
      "Epoch 396/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2182 - mean_absolute_error: 0.2182 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.24687\n",
      "Epoch 397/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2182 - mean_absolute_error: 0.2182 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.24687\n",
      "Epoch 398/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2181 - mean_absolute_error: 0.2181 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.24687\n",
      "Epoch 399/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2579 - val_mean_absolute_error: 0.2579\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.24687\n",
      "Epoch 400/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2530 - val_mean_absolute_error: 0.2530\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.24687\n",
      "Epoch 401/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2179 - mean_absolute_error: 0.2179 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.24687\n",
      "Epoch 402/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2183 - mean_absolute_error: 0.2183 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.24687\n",
      "Epoch 403/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2179 - mean_absolute_error: 0.2179 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.24687\n",
      "Epoch 404/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2178 - mean_absolute_error: 0.2178 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.24687\n",
      "Epoch 405/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.24687\n",
      "Epoch 406/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.24687\n",
      "Epoch 407/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2557 - val_mean_absolute_error: 0.2557\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.24687\n",
      "Epoch 408/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2179 - mean_absolute_error: 0.2179 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.24687\n",
      "Epoch 409/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2509 - val_mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.24687\n",
      "Epoch 410/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2178 - mean_absolute_error: 0.2178 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.24687\n",
      "Epoch 411/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2177 - mean_absolute_error: 0.2177 - val_loss: 0.2576 - val_mean_absolute_error: 0.2576\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.24687\n",
      "Epoch 412/500\n",
      "128640/128640 [==============================] - 7s 52us/step - loss: 0.2179 - mean_absolute_error: 0.2179 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.24687\n",
      "Epoch 413/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2177 - mean_absolute_error: 0.2177 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.24687\n",
      "Epoch 414/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2180 - mean_absolute_error: 0.2180 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.24687\n",
      "Epoch 415/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2178 - mean_absolute_error: 0.2178 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.24687\n",
      "Epoch 416/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2178 - mean_absolute_error: 0.2178 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.24687\n",
      "Epoch 417/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.24687\n",
      "Epoch 418/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.24687\n",
      "Epoch 419/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.24687\n",
      "Epoch 420/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.24687\n",
      "Epoch 421/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.24687\n",
      "Epoch 422/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2173 - mean_absolute_error: 0.2173 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.24687\n",
      "Epoch 423/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.24687\n",
      "Epoch 424/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2177 - mean_absolute_error: 0.2177 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.24687\n",
      "Epoch 425/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2178 - mean_absolute_error: 0.2178 - val_loss: 0.2615 - val_mean_absolute_error: 0.2615\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.24687\n",
      "Epoch 426/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2174 - mean_absolute_error: 0.2174 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.24687\n",
      "Epoch 427/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2173 - mean_absolute_error: 0.2173 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.24687\n",
      "Epoch 428/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.24687\n",
      "Epoch 429/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.24687\n",
      "Epoch 430/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.24687\n",
      "Epoch 431/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2174 - mean_absolute_error: 0.2174 - val_loss: 0.2570 - val_mean_absolute_error: 0.2570\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.24687\n",
      "Epoch 432/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2173 - mean_absolute_error: 0.2173 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.24687\n",
      "Epoch 433/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.24687\n",
      "Epoch 434/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.24687\n",
      "Epoch 435/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.24687\n",
      "Epoch 436/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2176 - mean_absolute_error: 0.2176 - val_loss: 0.2543 - val_mean_absolute_error: 0.2543\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.24687\n",
      "Epoch 437/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2175 - mean_absolute_error: 0.2175 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.24687\n",
      "Epoch 438/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.24687\n",
      "Epoch 439/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.24687\n",
      "Epoch 440/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.24687\n",
      "Epoch 441/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2544 - val_mean_absolute_error: 0.2544\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.24687\n",
      "Epoch 442/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.24687\n",
      "Epoch 443/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.24687\n",
      "Epoch 444/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.24687\n",
      "Epoch 445/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2541 - val_mean_absolute_error: 0.2541\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.24687\n",
      "Epoch 446/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2545 - val_mean_absolute_error: 0.2545\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.24687\n",
      "Epoch 447/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.24687\n",
      "Epoch 448/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2589 - val_mean_absolute_error: 0.2589\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.24687\n",
      "Epoch 449/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2172 - mean_absolute_error: 0.2172 - val_loss: 0.2533 - val_mean_absolute_error: 0.2533\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.24687\n",
      "Epoch 450/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.24687\n",
      "Epoch 451/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.24687\n",
      "Epoch 452/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.24687\n",
      "Epoch 453/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.24687\n",
      "Epoch 454/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.24687\n",
      "Epoch 455/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.24687\n",
      "Epoch 456/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2518 - val_mean_absolute_error: 0.2518\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.24687\n",
      "Epoch 457/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2515 - val_mean_absolute_error: 0.2515\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.24687\n",
      "Epoch 458/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.24687\n",
      "Epoch 459/500\n",
      "128640/128640 [==============================] - 7s 54us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.24687\n",
      "Epoch 460/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.24687\n",
      "Epoch 461/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.24687\n",
      "Epoch 462/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2171 - mean_absolute_error: 0.2171 - val_loss: 0.2540 - val_mean_absolute_error: 0.2540\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.24687\n",
      "Epoch 463/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2538 - val_mean_absolute_error: 0.2538\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.24687\n",
      "Epoch 464/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.24687\n",
      "Epoch 465/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.24687\n",
      "Epoch 466/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.24687\n",
      "Epoch 467/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.24687\n",
      "Epoch 468/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2170 - mean_absolute_error: 0.2170 - val_loss: 0.2535 - val_mean_absolute_error: 0.2535\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.24687\n",
      "Epoch 469/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2547 - val_mean_absolute_error: 0.2547\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.24687\n",
      "Epoch 470/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.24687\n",
      "Epoch 471/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.24687\n",
      "Epoch 472/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.24687\n",
      "Epoch 473/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.24687\n",
      "Epoch 474/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.24687\n",
      "Epoch 475/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2169 - mean_absolute_error: 0.2169 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.24687\n",
      "Epoch 476/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2168 - mean_absolute_error: 0.2168 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.24687\n",
      "Epoch 477/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.24687\n",
      "Epoch 478/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.24687\n",
      "Epoch 479/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2165 - mean_absolute_error: 0.2165 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.24687\n",
      "Epoch 480/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2530 - val_mean_absolute_error: 0.2530\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.24687\n",
      "Epoch 481/500\n",
      "128640/128640 [==============================] - 7s 54us/step - loss: 0.2165 - mean_absolute_error: 0.2165 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.24687\n",
      "Epoch 482/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.24687\n",
      "Epoch 483/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2534 - val_mean_absolute_error: 0.2534\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.24687\n",
      "Epoch 484/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2522 - val_mean_absolute_error: 0.2522\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.24687\n",
      "Epoch 485/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2165 - mean_absolute_error: 0.2165 - val_loss: 0.2546 - val_mean_absolute_error: 0.2546\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.24687\n",
      "Epoch 486/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2167 - mean_absolute_error: 0.2167 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.24687\n",
      "Epoch 487/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2166 - mean_absolute_error: 0.2166 - val_loss: 0.2564 - val_mean_absolute_error: 0.2564\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.24687\n",
      "Epoch 488/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.24687\n",
      "Epoch 489/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2165 - mean_absolute_error: 0.2165 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.24687\n",
      "Epoch 490/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2546 - val_mean_absolute_error: 0.2546\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.24687\n",
      "Epoch 491/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2163 - mean_absolute_error: 0.2163 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.24687\n",
      "Epoch 492/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2540 - val_mean_absolute_error: 0.2540\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.24687\n",
      "Epoch 493/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2163 - mean_absolute_error: 0.2163 - val_loss: 0.2539 - val_mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.24687\n",
      "Epoch 494/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2163 - mean_absolute_error: 0.2163 - val_loss: 0.2536 - val_mean_absolute_error: 0.2536\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.24687\n",
      "Epoch 495/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2165 - mean_absolute_error: 0.2165 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.24687\n",
      "Epoch 496/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2165 - mean_absolute_error: 0.2165 - val_loss: 0.2524 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.24687\n",
      "Epoch 497/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2161 - mean_absolute_error: 0.2161 - val_loss: 0.2517 - val_mean_absolute_error: 0.2517\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.24687\n",
      "Epoch 498/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.24687\n",
      "Epoch 499/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2164 - mean_absolute_error: 0.2164 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.24687\n",
      "Epoch 500/500\n",
      "128640/128640 [==============================] - 7s 53us/step - loss: 0.2161 - mean_absolute_error: 0.2161 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.24687\n",
      "[Pipeline] ................ (step 2 of 2) Processing nn, total=55.5min\n",
      "CPU times: user 2h 51min 3s, sys: 8min 44s, total: 2h 59min 47s\n",
      "Wall time: 55min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                   'comercial',\n",
       "                                                   'tiene_servicio', 'edificio',\n",
       "                                                   'casa', 'parte_de_lote',\n",
       "                                                   'calle_cerrada',\n",
       "                                                   'indica_frente_y_fondo',\n",
       "                                                   'usa_easybroker',\n",
       "                                                   'tiene_seguridad',\n",
       "                                                   'tiene_antiguedad',\n",
       "                                                   'tiene_banos',\n",
       "                                                   'tiene_garages',\n",
       "                                                   'tiene_habitaciones',\n",
       "                                                   'tiene_metroscubiertos',\n",
       "                                                   'tiene_metrostotales'])],\n",
       "                                   verbose=False)),\n",
       "                ('nn',\n",
       "                 <keras.wrappers.scikit_learn.KerasRegressor object at 0x7fea0ba4dba8>)],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-ce06972978ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(estimator.history['acc'])\n",
    "plt.plot(regressor.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = 83, activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wights file of the best model :\n",
    "wights_file = 'Weights-106--0.24687.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25077164017586695\n"
     ]
    }
   ],
   "source": [
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617471.2638024779\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.6s\n",
      "Train on 192000 samples, validate on 48000 samples\n",
      "Epoch 1/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.4307 - mean_absolute_error: 0.4307 - val_loss: 0.3250 - val_mean_absolute_error: 0.3250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32503, saving model to Weights-001--0.32503.hdf5\n",
      "Epoch 2/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2942 - mean_absolute_error: 0.2942 - val_loss: 0.2917 - val_mean_absolute_error: 0.2917\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32503 to 0.29166, saving model to Weights-002--0.29166.hdf5\n",
      "Epoch 3/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2829 - mean_absolute_error: 0.2829 - val_loss: 0.3192 - val_mean_absolute_error: 0.3192\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29166\n",
      "Epoch 4/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2748 - mean_absolute_error: 0.2748 - val_loss: 0.2775 - val_mean_absolute_error: 0.2775\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.29166 to 0.27749, saving model to Weights-004--0.27749.hdf5\n",
      "Epoch 5/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2689 - mean_absolute_error: 0.2689 - val_loss: 0.2742 - val_mean_absolute_error: 0.2742\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27749 to 0.27422, saving model to Weights-005--0.27422.hdf5\n",
      "Epoch 6/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2642 - mean_absolute_error: 0.2642 - val_loss: 0.2550 - val_mean_absolute_error: 0.2550\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27422 to 0.25505, saving model to Weights-006--0.25505.hdf5\n",
      "Epoch 7/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2620 - mean_absolute_error: 0.2620 - val_loss: 0.2542 - val_mean_absolute_error: 0.2542\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25505 to 0.25420, saving model to Weights-007--0.25420.hdf5\n",
      "Epoch 8/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2618 - mean_absolute_error: 0.2618 - val_loss: 0.3104 - val_mean_absolute_error: 0.3104\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25420\n",
      "Epoch 9/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2591 - mean_absolute_error: 0.2591 - val_loss: 0.2521 - val_mean_absolute_error: 0.2521\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.25420 to 0.25215, saving model to Weights-009--0.25215.hdf5\n",
      "Epoch 10/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2582 - mean_absolute_error: 0.2582 - val_loss: 0.2907 - val_mean_absolute_error: 0.2907\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.25215\n",
      "Epoch 11/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2567 - mean_absolute_error: 0.2567 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.25215 to 0.25159, saving model to Weights-011--0.25159.hdf5\n",
      "Epoch 12/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2565 - mean_absolute_error: 0.2565 - val_loss: 0.2983 - val_mean_absolute_error: 0.2983\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.25159\n",
      "Epoch 13/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2563 - mean_absolute_error: 0.2563 - val_loss: 0.2696 - val_mean_absolute_error: 0.2696\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.25159\n",
      "Epoch 14/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2550 - mean_absolute_error: 0.2550 - val_loss: 0.2606 - val_mean_absolute_error: 0.2606\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.25159\n",
      "Epoch 15/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2550 - mean_absolute_error: 0.2550 - val_loss: 0.2763 - val_mean_absolute_error: 0.2763\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.25159\n",
      "Epoch 16/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2539 - mean_absolute_error: 0.2539 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.25159 to 0.24733, saving model to Weights-016--0.24733.hdf5\n",
      "Epoch 17/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2542 - mean_absolute_error: 0.2542 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.24733\n",
      "Epoch 18/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2542 - mean_absolute_error: 0.2542 - val_loss: 0.2631 - val_mean_absolute_error: 0.2631\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.24733\n",
      "Epoch 19/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2528 - mean_absolute_error: 0.2528 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.24733 to 0.24666, saving model to Weights-019--0.24666.hdf5\n",
      "Epoch 20/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2520 - mean_absolute_error: 0.2520 - val_loss: 0.2650 - val_mean_absolute_error: 0.2650\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.24666\n",
      "Epoch 21/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2513 - mean_absolute_error: 0.2513 - val_loss: 0.2580 - val_mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.24666\n",
      "Epoch 22/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2509 - mean_absolute_error: 0.2509 - val_loss: 0.2659 - val_mean_absolute_error: 0.2659\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.24666\n",
      "Epoch 23/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2502 - mean_absolute_error: 0.2502 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.24666\n",
      "Epoch 24/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2503 - mean_absolute_error: 0.2503 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.24666\n",
      "Epoch 25/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2500 - mean_absolute_error: 0.2500 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.24666\n",
      "Epoch 26/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2497 - mean_absolute_error: 0.2497 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.24666 to 0.24544, saving model to Weights-026--0.24544.hdf5\n",
      "Epoch 27/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2492 - mean_absolute_error: 0.2492 - val_loss: 0.2572 - val_mean_absolute_error: 0.2572\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.24544\n",
      "Epoch 28/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2484 - mean_absolute_error: 0.2484 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.24544\n",
      "Epoch 29/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2486 - mean_absolute_error: 0.2486 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.24544\n",
      "Epoch 30/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2485 - mean_absolute_error: 0.2485 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24544\n",
      "Epoch 31/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2479 - mean_absolute_error: 0.2479 - val_loss: 0.2554 - val_mean_absolute_error: 0.2554\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24544\n",
      "Epoch 32/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2479 - mean_absolute_error: 0.2479 - val_loss: 0.2514 - val_mean_absolute_error: 0.2514\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24544\n",
      "Epoch 33/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2469 - mean_absolute_error: 0.2469 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24544\n",
      "Epoch 34/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2467 - mean_absolute_error: 0.2467 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24544\n",
      "Epoch 35/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2466 - mean_absolute_error: 0.2466 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24544\n",
      "Epoch 36/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2469 - mean_absolute_error: 0.2469 - val_loss: 0.2549 - val_mean_absolute_error: 0.2549\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24544\n",
      "Epoch 37/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2461 - mean_absolute_error: 0.2461 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.24544\n",
      "Epoch 38/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2465 - mean_absolute_error: 0.2465 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.24544\n",
      "Epoch 39/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2467 - mean_absolute_error: 0.2467 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.24544\n",
      "Epoch 40/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2453 - mean_absolute_error: 0.2453 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.24544\n",
      "Epoch 41/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2461 - mean_absolute_error: 0.2461 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.24544\n",
      "Epoch 42/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2454 - mean_absolute_error: 0.2454 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.24544\n",
      "Epoch 43/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.24544\n",
      "Epoch 44/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.24544 to 0.24518, saving model to Weights-044--0.24518.hdf5\n",
      "Epoch 45/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2445 - mean_absolute_error: 0.2445 - val_loss: 0.2527 - val_mean_absolute_error: 0.2527\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.24518\n",
      "Epoch 46/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.24518\n",
      "Epoch 47/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2450 - mean_absolute_error: 0.2450 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.24518\n",
      "Epoch 48/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2697 - val_mean_absolute_error: 0.2697\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.24518\n",
      "Epoch 49/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.24518\n",
      "Epoch 50/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2443 - mean_absolute_error: 0.2443 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.24518\n",
      "Epoch 51/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2438 - mean_absolute_error: 0.2438 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.24518 to 0.24490, saving model to Weights-051--0.24490.hdf5\n",
      "Epoch 52/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.2556 - val_mean_absolute_error: 0.2556\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.24490\n",
      "Epoch 53/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2435 - mean_absolute_error: 0.2435 - val_loss: 0.2492 - val_mean_absolute_error: 0.2492\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.24490\n",
      "Epoch 54/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2435 - mean_absolute_error: 0.2435 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.24490\n",
      "Epoch 55/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2434 - mean_absolute_error: 0.2434 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.24490\n",
      "Epoch 56/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2435 - mean_absolute_error: 0.2435 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.24490 to 0.24473, saving model to Weights-056--0.24473.hdf5\n",
      "Epoch 57/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2435 - mean_absolute_error: 0.2435 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.24473\n",
      "Epoch 58/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2438 - mean_absolute_error: 0.2438 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.24473\n",
      "Epoch 59/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2431 - mean_absolute_error: 0.2431 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.24473\n",
      "Epoch 60/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2430 - mean_absolute_error: 0.2430 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.24473\n",
      "Epoch 61/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2434 - mean_absolute_error: 0.2434 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.24473\n",
      "Epoch 62/500\n",
      "192000/192000 [==============================] - 10s 51us/step - loss: 0.2429 - mean_absolute_error: 0.2429 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.24473\n",
      "Epoch 63/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2430 - mean_absolute_error: 0.2430 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.24473\n",
      "Epoch 64/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2422 - mean_absolute_error: 0.2422 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.24473\n",
      "Epoch 65/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2421 - mean_absolute_error: 0.2421 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.24473\n",
      "Epoch 66/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2421 - mean_absolute_error: 0.2421 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.24473\n",
      "Epoch 67/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2424 - mean_absolute_error: 0.2424 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.24473\n",
      "Epoch 68/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2423 - mean_absolute_error: 0.2423 - val_loss: 0.2538 - val_mean_absolute_error: 0.2538\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.24473\n",
      "Epoch 69/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2418 - mean_absolute_error: 0.2418 - val_loss: 0.2483 - val_mean_absolute_error: 0.2483\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.24473\n",
      "Epoch 70/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2418 - mean_absolute_error: 0.2418 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.24473 to 0.24473, saving model to Weights-070--0.24473.hdf5\n",
      "Epoch 71/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2420 - mean_absolute_error: 0.2420 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24473\n",
      "Epoch 72/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2416 - mean_absolute_error: 0.2416 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24473\n",
      "Epoch 73/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2418 - mean_absolute_error: 0.2418 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24473\n",
      "Epoch 74/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2413 - mean_absolute_error: 0.2413 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24473\n",
      "Epoch 75/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2416 - mean_absolute_error: 0.2416 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24473\n",
      "Epoch 76/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2409 - mean_absolute_error: 0.2409 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.24473\n",
      "Epoch 77/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2415 - mean_absolute_error: 0.2415 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.24473\n",
      "Epoch 78/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2411 - mean_absolute_error: 0.2411 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24473\n",
      "Epoch 79/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2410 - mean_absolute_error: 0.2410 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24473\n",
      "Epoch 80/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2407 - mean_absolute_error: 0.2407 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.24473\n",
      "Epoch 81/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2409 - mean_absolute_error: 0.2409 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.24473\n",
      "Epoch 82/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2408 - mean_absolute_error: 0.2408 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24473\n",
      "Epoch 83/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2409 - mean_absolute_error: 0.2409 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.24473\n",
      "Epoch 84/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2407 - mean_absolute_error: 0.2407 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24473\n",
      "Epoch 85/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2407 - mean_absolute_error: 0.2407 - val_loss: 0.2443 - val_mean_absolute_error: 0.2443\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.24473 to 0.24433, saving model to Weights-085--0.24433.hdf5\n",
      "Epoch 86/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2406 - mean_absolute_error: 0.2406 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24433\n",
      "Epoch 87/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2404 - mean_absolute_error: 0.2404 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24433\n",
      "Epoch 88/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2406 - mean_absolute_error: 0.2406 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24433\n",
      "Epoch 89/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2402 - mean_absolute_error: 0.2402 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24433\n",
      "Epoch 90/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2401 - mean_absolute_error: 0.2401 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24433\n",
      "Epoch 91/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2399 - mean_absolute_error: 0.2399 - val_loss: 0.2444 - val_mean_absolute_error: 0.2444\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24433\n",
      "Epoch 92/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2400 - mean_absolute_error: 0.2400 - val_loss: 0.2442 - val_mean_absolute_error: 0.2442\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.24433 to 0.24422, saving model to Weights-092--0.24422.hdf5\n",
      "Epoch 93/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2400 - mean_absolute_error: 0.2400 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.24422\n",
      "Epoch 94/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2399 - mean_absolute_error: 0.2399 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.24422\n",
      "Epoch 95/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2401 - mean_absolute_error: 0.2401 - val_loss: 0.2499 - val_mean_absolute_error: 0.2499\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.24422\n",
      "Epoch 96/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2397 - mean_absolute_error: 0.2397 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.24422\n",
      "Epoch 97/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2394 - mean_absolute_error: 0.2394 - val_loss: 0.2643 - val_mean_absolute_error: 0.2643\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.24422\n",
      "Epoch 98/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2395 - mean_absolute_error: 0.2395 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.24422\n",
      "Epoch 99/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2395 - mean_absolute_error: 0.2395 - val_loss: 0.2443 - val_mean_absolute_error: 0.2443\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.24422\n",
      "Epoch 100/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2396 - mean_absolute_error: 0.2396 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.24422\n",
      "Epoch 101/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2396 - mean_absolute_error: 0.2396 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.24422\n",
      "Epoch 102/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2394 - mean_absolute_error: 0.2394 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.24422\n",
      "Epoch 103/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2391 - mean_absolute_error: 0.2391 - val_loss: 0.2623 - val_mean_absolute_error: 0.2623\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.24422\n",
      "Epoch 104/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2392 - mean_absolute_error: 0.2392 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.24422\n",
      "Epoch 105/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2391 - mean_absolute_error: 0.2391 - val_loss: 0.2508 - val_mean_absolute_error: 0.2508\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.24422\n",
      "Epoch 106/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2395 - mean_absolute_error: 0.2395 - val_loss: 0.2519 - val_mean_absolute_error: 0.2519\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.24422\n",
      "Epoch 107/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2385 - mean_absolute_error: 0.2385 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.24422\n",
      "Epoch 108/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2391 - mean_absolute_error: 0.2391 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24422\n",
      "Epoch 109/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2387 - mean_absolute_error: 0.2387 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24422\n",
      "Epoch 110/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2383 - mean_absolute_error: 0.2383 - val_loss: 0.2440 - val_mean_absolute_error: 0.2440\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.24422 to 0.24404, saving model to Weights-110--0.24404.hdf5\n",
      "Epoch 111/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2381 - mean_absolute_error: 0.2381 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24404\n",
      "Epoch 112/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2377 - mean_absolute_error: 0.2377 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24404\n",
      "Epoch 113/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2381 - mean_absolute_error: 0.2381 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24404\n",
      "Epoch 114/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2384 - mean_absolute_error: 0.2384 - val_loss: 0.2444 - val_mean_absolute_error: 0.2444\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.24404\n",
      "Epoch 115/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2379 - mean_absolute_error: 0.2379 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.24404\n",
      "Epoch 116/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2379 - mean_absolute_error: 0.2379 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.24404\n",
      "Epoch 117/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2381 - mean_absolute_error: 0.2381 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.24404\n",
      "Epoch 118/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2376 - mean_absolute_error: 0.2376 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.24404\n",
      "Epoch 119/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2374 - mean_absolute_error: 0.2374 - val_loss: 0.2557 - val_mean_absolute_error: 0.2557\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.24404\n",
      "Epoch 120/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2374 - mean_absolute_error: 0.2374 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.24404\n",
      "Epoch 121/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2374 - mean_absolute_error: 0.2374 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.24404\n",
      "Epoch 122/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2375 - mean_absolute_error: 0.2375 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.24404\n",
      "Epoch 123/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2374 - mean_absolute_error: 0.2374 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.24404\n",
      "Epoch 124/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2371 - mean_absolute_error: 0.2371 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.24404\n",
      "Epoch 125/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2369 - mean_absolute_error: 0.2369 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.24404\n",
      "Epoch 126/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2373 - mean_absolute_error: 0.2373 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.24404\n",
      "Epoch 127/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2373 - mean_absolute_error: 0.2373 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.24404\n",
      "Epoch 128/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2372 - mean_absolute_error: 0.2372 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.24404\n",
      "Epoch 129/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2371 - mean_absolute_error: 0.2371 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.24404\n",
      "Epoch 130/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2369 - mean_absolute_error: 0.2369 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.24404\n",
      "Epoch 131/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2369 - mean_absolute_error: 0.2369 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.24404\n",
      "Epoch 132/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2371 - mean_absolute_error: 0.2371 - val_loss: 0.2446 - val_mean_absolute_error: 0.2446\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.24404\n",
      "Epoch 133/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2366 - mean_absolute_error: 0.2366 - val_loss: 0.2501 - val_mean_absolute_error: 0.2501\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.24404\n",
      "Epoch 134/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2368 - mean_absolute_error: 0.2368 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.24404\n",
      "Epoch 135/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.24404\n",
      "Epoch 136/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2368 - mean_absolute_error: 0.2368 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.24404\n",
      "Epoch 137/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.24404\n",
      "Epoch 138/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2363 - mean_absolute_error: 0.2363 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.24404\n",
      "Epoch 139/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.24404\n",
      "Epoch 140/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2445 - val_mean_absolute_error: 0.2445\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.24404\n",
      "Epoch 141/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.24404\n",
      "Epoch 142/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2362 - mean_absolute_error: 0.2362 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.24404\n",
      "Epoch 143/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2363 - mean_absolute_error: 0.2363 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.24404\n",
      "Epoch 144/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2365 - mean_absolute_error: 0.2365 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.24404\n",
      "Epoch 145/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2360 - mean_absolute_error: 0.2360 - val_loss: 0.2513 - val_mean_absolute_error: 0.2513\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.24404\n",
      "Epoch 146/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2361 - mean_absolute_error: 0.2361 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.24404\n",
      "Epoch 147/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2362 - mean_absolute_error: 0.2362 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.24404\n",
      "Epoch 148/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2362 - mean_absolute_error: 0.2362 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.24404\n",
      "Epoch 149/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2360 - mean_absolute_error: 0.2360 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.24404\n",
      "Epoch 150/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2360 - mean_absolute_error: 0.2360 - val_loss: 0.2444 - val_mean_absolute_error: 0.2444\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.24404\n",
      "Epoch 151/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.24404\n",
      "Epoch 152/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2359 - mean_absolute_error: 0.2359 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.24404\n",
      "Epoch 153/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.24404\n",
      "Epoch 154/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2357 - mean_absolute_error: 0.2357 - val_loss: 0.2501 - val_mean_absolute_error: 0.2501\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.24404\n",
      "Epoch 155/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2355 - mean_absolute_error: 0.2355 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.24404\n",
      "Epoch 156/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2357 - mean_absolute_error: 0.2357 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.24404\n",
      "Epoch 157/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.24404\n",
      "Epoch 158/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2359 - mean_absolute_error: 0.2359 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.24404\n",
      "Epoch 159/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2355 - mean_absolute_error: 0.2355 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.24404\n",
      "Epoch 160/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2357 - mean_absolute_error: 0.2357 - val_loss: 0.2504 - val_mean_absolute_error: 0.2504\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.24404\n",
      "Epoch 161/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.24404\n",
      "Epoch 162/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2354 - mean_absolute_error: 0.2354 - val_loss: 0.2528 - val_mean_absolute_error: 0.2528\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.24404\n",
      "Epoch 163/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2358 - mean_absolute_error: 0.2358 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.24404\n",
      "Epoch 164/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2354 - mean_absolute_error: 0.2354 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.24404\n",
      "Epoch 165/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2354 - mean_absolute_error: 0.2354 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.24404\n",
      "Epoch 166/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2354 - mean_absolute_error: 0.2354 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.24404\n",
      "Epoch 167/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2354 - mean_absolute_error: 0.2354 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.24404\n",
      "Epoch 168/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2352 - mean_absolute_error: 0.2352 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.24404\n",
      "Epoch 169/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.24404\n",
      "Epoch 170/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2353 - mean_absolute_error: 0.2353 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.24404\n",
      "Epoch 171/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.24404\n",
      "Epoch 172/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2349 - mean_absolute_error: 0.2349 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.24404\n",
      "Epoch 173/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.24404\n",
      "Epoch 174/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2352 - mean_absolute_error: 0.2352 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.24404\n",
      "Epoch 175/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.24404\n",
      "Epoch 176/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.24404\n",
      "Epoch 177/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.24404\n",
      "Epoch 178/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.24404\n",
      "Epoch 179/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2350 - mean_absolute_error: 0.2350 - val_loss: 0.2505 - val_mean_absolute_error: 0.2505\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.24404\n",
      "Epoch 180/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2351 - mean_absolute_error: 0.2351 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.24404\n",
      "Epoch 181/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2347 - mean_absolute_error: 0.2347 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.24404\n",
      "Epoch 182/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2345 - mean_absolute_error: 0.2345 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.24404\n",
      "Epoch 183/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2349 - mean_absolute_error: 0.2349 - val_loss: 0.2443 - val_mean_absolute_error: 0.2443\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.24404\n",
      "Epoch 184/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2483 - val_mean_absolute_error: 0.2483\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.24404\n",
      "Epoch 185/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2347 - mean_absolute_error: 0.2347 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.24404\n",
      "Epoch 186/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2346 - mean_absolute_error: 0.2346 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.24404\n",
      "Epoch 187/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2344 - mean_absolute_error: 0.2344 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.24404\n",
      "Epoch 188/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2348 - mean_absolute_error: 0.2348 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.24404\n",
      "Epoch 189/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2347 - mean_absolute_error: 0.2347 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.24404\n",
      "Epoch 190/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2344 - mean_absolute_error: 0.2344 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.24404\n",
      "Epoch 191/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2345 - mean_absolute_error: 0.2345 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.24404\n",
      "Epoch 192/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2345 - mean_absolute_error: 0.2345 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.24404\n",
      "Epoch 193/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2343 - mean_absolute_error: 0.2343 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.24404\n",
      "Epoch 194/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2345 - mean_absolute_error: 0.2345 - val_loss: 0.2445 - val_mean_absolute_error: 0.2445\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.24404\n",
      "Epoch 195/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.24404\n",
      "Epoch 196/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2343 - mean_absolute_error: 0.2343 - val_loss: 0.2445 - val_mean_absolute_error: 0.2445\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.24404\n",
      "Epoch 197/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2344 - mean_absolute_error: 0.2344 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.24404\n",
      "Epoch 198/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2346 - mean_absolute_error: 0.2346 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.24404\n",
      "Epoch 199/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.24404\n",
      "Epoch 200/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2343 - mean_absolute_error: 0.2343 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.24404\n",
      "Epoch 201/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2343 - mean_absolute_error: 0.2343 - val_loss: 0.2529 - val_mean_absolute_error: 0.2529\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.24404\n",
      "Epoch 202/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2345 - mean_absolute_error: 0.2345 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.24404\n",
      "Epoch 203/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.24404\n",
      "Epoch 204/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.24404\n",
      "Epoch 205/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2343 - mean_absolute_error: 0.2343 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.24404\n",
      "Epoch 206/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2488 - val_mean_absolute_error: 0.2488\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.24404\n",
      "Epoch 207/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2443 - val_mean_absolute_error: 0.2443\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.24404\n",
      "Epoch 208/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.24404\n",
      "Epoch 209/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.24404\n",
      "Epoch 210/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.24404\n",
      "Epoch 211/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2340 - mean_absolute_error: 0.2340 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.24404\n",
      "Epoch 212/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.24404\n",
      "Epoch 213/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.24404\n",
      "Epoch 214/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2340 - mean_absolute_error: 0.2340 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.24404\n",
      "Epoch 215/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2340 - mean_absolute_error: 0.2340 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.24404\n",
      "Epoch 216/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2341 - mean_absolute_error: 0.2341 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.24404\n",
      "Epoch 217/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.24404\n",
      "Epoch 218/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2343 - mean_absolute_error: 0.2343 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.24404\n",
      "Epoch 219/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2342 - mean_absolute_error: 0.2342 - val_loss: 0.2478 - val_mean_absolute_error: 0.2478\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.24404\n",
      "Epoch 220/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2340 - mean_absolute_error: 0.2340 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.24404\n",
      "Epoch 221/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2339 - mean_absolute_error: 0.2339 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.24404\n",
      "Epoch 222/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2339 - mean_absolute_error: 0.2339 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.24404\n",
      "Epoch 223/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.24404\n",
      "Epoch 224/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.24404\n",
      "Epoch 225/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.24404\n",
      "Epoch 226/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.24404\n",
      "Epoch 227/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2337 - mean_absolute_error: 0.2337 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.24404\n",
      "Epoch 228/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2336 - mean_absolute_error: 0.2336 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.24404\n",
      "Epoch 229/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2339 - mean_absolute_error: 0.2339 - val_loss: 0.2491 - val_mean_absolute_error: 0.2491\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.24404\n",
      "Epoch 230/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2337 - mean_absolute_error: 0.2337 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.24404\n",
      "Epoch 231/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2336 - mean_absolute_error: 0.2336 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.24404\n",
      "Epoch 232/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2336 - mean_absolute_error: 0.2336 - val_loss: 0.2446 - val_mean_absolute_error: 0.2446\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.24404\n",
      "Epoch 233/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2337 - mean_absolute_error: 0.2337 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.24404\n",
      "Epoch 234/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2335 - mean_absolute_error: 0.2335 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.24404\n",
      "Epoch 235/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.24404\n",
      "Epoch 236/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2336 - mean_absolute_error: 0.2336 - val_loss: 0.2478 - val_mean_absolute_error: 0.2478\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.24404\n",
      "Epoch 237/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2335 - mean_absolute_error: 0.2335 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.24404\n",
      "Epoch 238/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2338 - mean_absolute_error: 0.2338 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.24404\n",
      "Epoch 239/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.24404\n",
      "Epoch 240/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2335 - mean_absolute_error: 0.2335 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.24404\n",
      "Epoch 241/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2336 - mean_absolute_error: 0.2336 - val_loss: 0.2546 - val_mean_absolute_error: 0.2546\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.24404\n",
      "Epoch 242/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2497 - val_mean_absolute_error: 0.2497\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.24404\n",
      "Epoch 243/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.24404\n",
      "Epoch 244/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2557 - val_mean_absolute_error: 0.2557\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.24404\n",
      "Epoch 245/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.24404\n",
      "Epoch 246/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2520 - val_mean_absolute_error: 0.2520\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.24404\n",
      "Epoch 247/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2446 - val_mean_absolute_error: 0.2446\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.24404\n",
      "Epoch 248/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.24404\n",
      "Epoch 249/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.24404\n",
      "Epoch 250/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2490 - val_mean_absolute_error: 0.2490\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.24404\n",
      "Epoch 251/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.24404\n",
      "Epoch 252/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.24404\n",
      "Epoch 253/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.24404\n",
      "Epoch 254/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.24404\n",
      "Epoch 255/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2331 - mean_absolute_error: 0.2331 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.24404\n",
      "Epoch 256/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2334 - mean_absolute_error: 0.2334 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.24404\n",
      "Epoch 257/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.24404\n",
      "Epoch 258/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.24404\n",
      "Epoch 259/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.24404\n",
      "Epoch 260/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.24404\n",
      "Epoch 261/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2331 - mean_absolute_error: 0.2331 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.24404\n",
      "Epoch 262/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.24404\n",
      "Epoch 263/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2331 - mean_absolute_error: 0.2331 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.24404\n",
      "Epoch 264/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.24404\n",
      "Epoch 265/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2331 - mean_absolute_error: 0.2331 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.24404\n",
      "Epoch 266/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.24404\n",
      "Epoch 267/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.24404\n",
      "Epoch 268/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2333 - mean_absolute_error: 0.2333 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.24404\n",
      "Epoch 269/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.24404\n",
      "Epoch 270/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2478 - val_mean_absolute_error: 0.2478\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.24404\n",
      "Epoch 271/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2331 - mean_absolute_error: 0.2331 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.24404\n",
      "Epoch 272/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2332 - mean_absolute_error: 0.2332 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.24404\n",
      "Epoch 273/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2331 - mean_absolute_error: 0.2331 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.24404\n",
      "Epoch 274/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.24404\n",
      "Epoch 275/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.24404\n",
      "Epoch 276/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.24404\n",
      "Epoch 277/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.24404\n",
      "Epoch 278/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2329 - mean_absolute_error: 0.2329 - val_loss: 0.2446 - val_mean_absolute_error: 0.2446\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.24404\n",
      "Epoch 279/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.24404\n",
      "Epoch 280/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.24404\n",
      "Epoch 281/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.24404\n",
      "Epoch 282/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2328 - mean_absolute_error: 0.2328 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.24404\n",
      "Epoch 283/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2327 - mean_absolute_error: 0.2327 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.24404\n",
      "Epoch 284/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2330 - mean_absolute_error: 0.2330 - val_loss: 0.2530 - val_mean_absolute_error: 0.2530\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.24404\n",
      "Epoch 285/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2489 - val_mean_absolute_error: 0.2489\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.24404\n",
      "Epoch 286/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2327 - mean_absolute_error: 0.2327 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.24404\n",
      "Epoch 287/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2327 - mean_absolute_error: 0.2327 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.24404\n",
      "Epoch 288/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.24404\n",
      "Epoch 289/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2327 - mean_absolute_error: 0.2327 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.24404\n",
      "Epoch 290/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.24404\n",
      "Epoch 291/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.24404\n",
      "Epoch 292/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.24404\n",
      "Epoch 293/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.24404\n",
      "Epoch 294/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2526 - val_mean_absolute_error: 0.2526\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.24404\n",
      "Epoch 295/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.24404\n",
      "Epoch 296/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2446 - val_mean_absolute_error: 0.2446\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.24404\n",
      "Epoch 297/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2326 - mean_absolute_error: 0.2326 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.24404\n",
      "Epoch 298/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.24404\n",
      "Epoch 299/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.24404\n",
      "Epoch 300/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.24404\n",
      "Epoch 301/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.24404\n",
      "Epoch 302/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.24404\n",
      "Epoch 303/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2324 - mean_absolute_error: 0.2324 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.24404\n",
      "Epoch 304/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.24404\n",
      "Epoch 305/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.24404\n",
      "Epoch 306/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.24404\n",
      "Epoch 307/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.24404\n",
      "Epoch 308/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2325 - mean_absolute_error: 0.2325 - val_loss: 0.2541 - val_mean_absolute_error: 0.2541\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.24404\n",
      "Epoch 309/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.24404\n",
      "Epoch 310/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2592 - val_mean_absolute_error: 0.2592\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.24404\n",
      "Epoch 311/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2324 - mean_absolute_error: 0.2324 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.24404\n",
      "Epoch 312/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2512 - val_mean_absolute_error: 0.2512\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.24404\n",
      "Epoch 313/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2474 - val_mean_absolute_error: 0.2474\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.24404\n",
      "Epoch 314/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.24404\n",
      "Epoch 315/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.24404\n",
      "Epoch 316/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2551 - val_mean_absolute_error: 0.2551\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.24404\n",
      "Epoch 317/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.24404\n",
      "Epoch 318/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.24404\n",
      "Epoch 319/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2324 - mean_absolute_error: 0.2324 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.24404\n",
      "Epoch 320/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.24404\n",
      "Epoch 321/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.24404\n",
      "Epoch 322/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2324 - mean_absolute_error: 0.2324 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.24404\n",
      "Epoch 323/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.24404\n",
      "Epoch 324/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.24404\n",
      "Epoch 325/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2323 - mean_absolute_error: 0.2323 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.24404\n",
      "Epoch 326/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2445 - val_mean_absolute_error: 0.2445\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.24404\n",
      "Epoch 327/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.24404\n",
      "Epoch 328/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.24404\n",
      "Epoch 329/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.24404\n",
      "Epoch 330/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2492 - val_mean_absolute_error: 0.2492\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.24404\n",
      "Epoch 331/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.24404\n",
      "Epoch 332/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.24404\n",
      "Epoch 333/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2549 - val_mean_absolute_error: 0.2549\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.24404\n",
      "Epoch 334/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2473 - val_mean_absolute_error: 0.2473\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.24404\n",
      "Epoch 335/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2321 - mean_absolute_error: 0.2321 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.24404\n",
      "Epoch 336/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.24404\n",
      "Epoch 337/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.24404\n",
      "Epoch 338/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.24404\n",
      "Epoch 339/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.24404\n",
      "Epoch 340/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.24404\n",
      "Epoch 341/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2570 - val_mean_absolute_error: 0.2570\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.24404\n",
      "Epoch 342/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.24404\n",
      "Epoch 343/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.24404\n",
      "Epoch 344/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.24404\n",
      "Epoch 345/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.24404\n",
      "Epoch 346/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.24404\n",
      "Epoch 347/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.24404\n",
      "Epoch 348/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.24404\n",
      "Epoch 349/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2322 - mean_absolute_error: 0.2322 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.24404\n",
      "Epoch 350/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.24404\n",
      "Epoch 351/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.24404\n",
      "Epoch 352/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2320 - mean_absolute_error: 0.2320 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.24404\n",
      "Epoch 353/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.24404\n",
      "Epoch 354/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2480 - val_mean_absolute_error: 0.2480\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.24404\n",
      "Epoch 355/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2516 - val_mean_absolute_error: 0.2516\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.24404\n",
      "Epoch 356/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.24404\n",
      "Epoch 357/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2486 - val_mean_absolute_error: 0.2486\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.24404\n",
      "Epoch 358/500\n",
      "192000/192000 [==============================] - 10s 52us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2444 - val_mean_absolute_error: 0.2444\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.24404\n",
      "Epoch 359/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.24404\n",
      "Epoch 360/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.24404\n",
      "Epoch 361/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2319 - mean_absolute_error: 0.2319 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.24404\n",
      "Epoch 362/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2447 - val_mean_absolute_error: 0.2447\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.24404\n",
      "Epoch 363/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.24404\n",
      "Epoch 364/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.24404\n",
      "Epoch 365/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.24404\n",
      "Epoch 366/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.24404\n",
      "Epoch 367/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2450 - val_mean_absolute_error: 0.2450\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.24404\n",
      "Epoch 368/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.24404\n",
      "Epoch 369/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.24404\n",
      "Epoch 370/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2544 - val_mean_absolute_error: 0.2544\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.24404\n",
      "Epoch 371/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.24404\n",
      "Epoch 372/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2506 - val_mean_absolute_error: 0.2506\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.24404\n",
      "Epoch 373/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.24404\n",
      "Epoch 374/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2318 - mean_absolute_error: 0.2318 - val_loss: 0.2472 - val_mean_absolute_error: 0.2472\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.24404\n",
      "Epoch 375/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2483 - val_mean_absolute_error: 0.2483\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.24404\n",
      "Epoch 376/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.24404\n",
      "Epoch 377/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.24404\n",
      "Epoch 378/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.24404\n",
      "Epoch 379/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.24404\n",
      "Epoch 380/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2531 - val_mean_absolute_error: 0.2531\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.24404\n",
      "Epoch 381/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.24404\n",
      "Epoch 382/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.24404\n",
      "Epoch 383/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.24404\n",
      "Epoch 384/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.24404\n",
      "Epoch 385/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.24404\n",
      "Epoch 386/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.24404\n",
      "Epoch 387/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.24404\n",
      "Epoch 388/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.24404\n",
      "Epoch 389/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2317 - mean_absolute_error: 0.2317 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.24404\n",
      "Epoch 390/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.24404\n",
      "Epoch 391/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.24404\n",
      "Epoch 392/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.24404\n",
      "Epoch 393/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.24404\n",
      "Epoch 394/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.24404\n",
      "Epoch 395/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.24404\n",
      "Epoch 396/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.24404\n",
      "Epoch 397/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.24404\n",
      "Epoch 398/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.24404\n",
      "Epoch 399/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.24404\n",
      "Epoch 400/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2466 - val_mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.24404\n",
      "Epoch 401/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.24404\n",
      "Epoch 402/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.24404\n",
      "Epoch 403/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2478 - val_mean_absolute_error: 0.2478\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.24404\n",
      "Epoch 404/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.24404\n",
      "Epoch 405/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2316 - mean_absolute_error: 0.2316 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.24404\n",
      "Epoch 406/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.24404\n",
      "Epoch 407/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2510 - val_mean_absolute_error: 0.2510\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.24404\n",
      "Epoch 408/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.24404\n",
      "Epoch 409/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2477 - val_mean_absolute_error: 0.2477\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.24404\n",
      "Epoch 410/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2315 - mean_absolute_error: 0.2315 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.24404\n",
      "Epoch 411/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.24404\n",
      "Epoch 412/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.24404\n",
      "Epoch 413/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2314 - mean_absolute_error: 0.2314 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.24404\n",
      "Epoch 414/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.24404\n",
      "Epoch 415/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2500 - val_mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.24404\n",
      "Epoch 416/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.24404\n",
      "Epoch 417/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.24404\n",
      "Epoch 418/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.24404\n",
      "Epoch 419/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.24404\n",
      "Epoch 420/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2479 - val_mean_absolute_error: 0.2479\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.24404\n",
      "Epoch 421/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.24404\n",
      "Epoch 422/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2313 - mean_absolute_error: 0.2313 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.24404\n",
      "Epoch 423/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.24404\n",
      "Epoch 424/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.24404\n",
      "Epoch 425/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.24404\n",
      "Epoch 426/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2501 - val_mean_absolute_error: 0.2501\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.24404\n",
      "Epoch 427/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2487 - val_mean_absolute_error: 0.2487\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.24404\n",
      "Epoch 428/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.24404\n",
      "Epoch 429/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.24404\n",
      "Epoch 430/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2458 - val_mean_absolute_error: 0.2458\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.24404\n",
      "Epoch 431/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.24404\n",
      "Epoch 432/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.24404\n",
      "Epoch 433/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.24404\n",
      "Epoch 434/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.24404\n",
      "Epoch 435/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2476 - val_mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.24404\n",
      "Epoch 436/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2312 - mean_absolute_error: 0.2312 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.24404\n",
      "Epoch 437/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.24404\n",
      "Epoch 438/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2464 - val_mean_absolute_error: 0.2464\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.24404\n",
      "Epoch 439/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.24404\n",
      "Epoch 440/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.24404\n",
      "Epoch 441/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2452 - val_mean_absolute_error: 0.2452\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.24404\n",
      "Epoch 442/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.24404\n",
      "Epoch 443/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2311 - mean_absolute_error: 0.2311 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.24404\n",
      "Epoch 444/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.24404\n",
      "Epoch 445/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.24404\n",
      "Epoch 446/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.24404\n",
      "Epoch 447/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2483 - val_mean_absolute_error: 0.2483\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.24404\n",
      "Epoch 448/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.24404\n",
      "Epoch 449/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.24404\n",
      "Epoch 450/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2523 - val_mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.24404\n",
      "Epoch 451/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.24404\n",
      "Epoch 452/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.24404\n",
      "Epoch 453/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.24404\n",
      "Epoch 454/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.24404\n",
      "Epoch 455/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2471 - val_mean_absolute_error: 0.2471\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.24404\n",
      "Epoch 456/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2310 - mean_absolute_error: 0.2310 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.24404\n",
      "Epoch 457/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2448 - val_mean_absolute_error: 0.2448\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.24404\n",
      "Epoch 458/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.24404\n",
      "Epoch 459/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.24404\n",
      "Epoch 460/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.24404\n",
      "Epoch 461/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2457 - val_mean_absolute_error: 0.2457\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.24404\n",
      "Epoch 462/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.24404\n",
      "Epoch 463/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.24404\n",
      "Epoch 464/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.24404\n",
      "Epoch 465/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2308 - mean_absolute_error: 0.2308 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.24404\n",
      "Epoch 466/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2469 - val_mean_absolute_error: 0.2469\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.24404\n",
      "Epoch 467/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2449 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.24404\n",
      "Epoch 468/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2507 - val_mean_absolute_error: 0.2507\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.24404\n",
      "Epoch 469/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.24404\n",
      "Epoch 470/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.24404\n",
      "Epoch 471/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.24404\n",
      "Epoch 472/500\n",
      "192000/192000 [==============================] - 10s 54us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2465 - val_mean_absolute_error: 0.2465\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.24404\n",
      "Epoch 473/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2454 - val_mean_absolute_error: 0.2454\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.24404\n",
      "Epoch 474/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.24404\n",
      "Epoch 475/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2309 - mean_absolute_error: 0.2309 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.24404\n",
      "Epoch 476/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.24404\n",
      "Epoch 477/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2305 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.24404\n",
      "Epoch 478/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2468 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.24404\n",
      "Epoch 479/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2451 - val_mean_absolute_error: 0.2451\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.24404\n",
      "Epoch 480/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.24404\n",
      "Epoch 481/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2475 - val_mean_absolute_error: 0.2475\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.24404\n",
      "Epoch 482/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2485 - val_mean_absolute_error: 0.2485\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.24404\n",
      "Epoch 483/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2459 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.24404\n",
      "Epoch 484/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.24404\n",
      "Epoch 485/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.24404\n",
      "Epoch 486/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2461 - val_mean_absolute_error: 0.2461\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.24404\n",
      "Epoch 487/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2453 - val_mean_absolute_error: 0.2453\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.24404\n",
      "Epoch 488/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2511 - val_mean_absolute_error: 0.2511\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.24404\n",
      "Epoch 489/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2304 - mean_absolute_error: 0.2304 - val_loss: 0.2470 - val_mean_absolute_error: 0.2470\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.24404\n",
      "Epoch 490/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2494 - val_mean_absolute_error: 0.2494\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.24404\n",
      "Epoch 491/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2306 - mean_absolute_error: 0.2306 - val_loss: 0.2482 - val_mean_absolute_error: 0.2482\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.24404\n",
      "Epoch 492/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2455 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.24404\n",
      "Epoch 493/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2462 - val_mean_absolute_error: 0.2462\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.24404\n",
      "Epoch 494/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2307 - mean_absolute_error: 0.2307 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.24404\n",
      "Epoch 495/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2305 - mean_absolute_error: 0.2305 - val_loss: 0.2481 - val_mean_absolute_error: 0.2481\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.24404\n",
      "Epoch 496/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2304 - mean_absolute_error: 0.2304 - val_loss: 0.2456 - val_mean_absolute_error: 0.2456\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.24404\n",
      "Epoch 497/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2304 - mean_absolute_error: 0.2304 - val_loss: 0.2460 - val_mean_absolute_error: 0.2460\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.24404\n",
      "Epoch 498/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2304 - mean_absolute_error: 0.2304 - val_loss: 0.2484 - val_mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.24404\n",
      "Epoch 499/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2304 - mean_absolute_error: 0.2304 - val_loss: 0.2467 - val_mean_absolute_error: 0.2467\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.24404\n",
      "Epoch 500/500\n",
      "192000/192000 [==============================] - 10s 53us/step - loss: 0.2303 - mean_absolute_error: 0.2303 - val_loss: 0.2463 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.24404\n",
      "[Pipeline] ................ (step 2 of 2) Processing nn, total=83.9min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                   'comercial',\n",
       "                                                   'tiene_servicio', 'edificio',\n",
       "                                                   'casa', 'parte_de_lote',\n",
       "                                                   'calle_cerrada',\n",
       "                                                   'indica_frente_y_fondo',\n",
       "                                                   'usa_easybroker',\n",
       "                                                   'tiene_seguridad',\n",
       "                                                   'tiene_antiguedad',\n",
       "                                                   'tiene_banos',\n",
       "                                                   'tiene_garages',\n",
       "                                                   'tiene_habitaciones',\n",
       "                                                   'tiene_metroscubiertos',\n",
       "                                                   'tiene_metrostotales'])],\n",
       "                                   verbose=False)),\n",
       "                ('nn',\n",
       "                 <keras.wrappers.scikit_learn.KerasRegressor object at 0x7fea0ba4dba8>)],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pipe.fit(X.replace({True:1,False:0}), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/pandas/core/indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"4th approach. Red neuronal con Keras\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
