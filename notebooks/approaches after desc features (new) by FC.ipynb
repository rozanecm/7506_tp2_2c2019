{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow you to see all column names & rows when you are doing .head(). None of the column name will be truncated.\n",
    "# source: https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/38f2901c592bdffc40726cb0473318cf\n",
    "# Function which plays a beep of given duration and frequency.\n",
    "# Useful for when executing things that need a while to finish, to get notified.\n",
    "import os\n",
    "def beep(duration = 0.6, freq = 200):\n",
    "    \"\"\" play tone of duration in seconds and freq in Hz. \"\"\"\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_with_desc_full.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test_with_desc_full.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescued_coords = pd.read_csv('../data/rescueLatLongs.csv')\n",
    "rescued_antiguedad = pd.read_csv('../data/imputations/antiguedad.csv')\n",
    "rescued_banos = pd.read_csv('../data/imputations/banos.csv')\n",
    "rescued_garages = pd.read_csv('../data/imputations/garages.csv')\n",
    "rescued_habitaciones = pd.read_csv('../data/imputations/habitaciones.csv')\n",
    "rescued_metroscubiertos = pd.read_csv('../data/imputations/metroscubiertos.csv')\n",
    "rescued_metrostotales = pd.read_csv('../data/imputations/metrostotales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeamos con coords. extra obtenidas en tp1.\n",
    "train = train.merge(rescued_coords.drop('Unnamed: 0', axis=1), how='left', on='id')\n",
    "train['lat_x'] = train.apply(lambda x: x['lat_y'] if pd.isna(x['lat_x']) else x['lat_x'], axis=1)\n",
    "train['lng_x'] = train.apply(lambda x: x['lng_y'] if pd.isna(x['lng_x']) else x['lng_x'], axis=1)\n",
    "train.drop(['lat_y','lng_y'], axis=1, inplace=True)\n",
    "train.rename(columns={'lat_x':'lat','lng_x':'lon'}, inplace=True)\n",
    "\n",
    "# por consistencia, para que ambos datasets tengan mismos nombres\n",
    "test.rename(columns={'lng':'lon'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan para los datos fuera de rango es mejor que dropear todo el dato\n",
    "train.loc[(train['lat']>14) | (train['lat']<33),['lat','lon']] = np.nan\n",
    "train.loc[(train['lon']>86) | (train['lon']<118),['lat','lon']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf. values don't make sense. I think it's preferable to treat them as nans directly.\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[(train['lat']>14) | (train['lat']<33),['lat','lon']] = np.nan\n",
    "test.loc[(train['lon']>86) | (train['lon']<118),['lat','lon']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_models_predictions(df, predictions_df, col_name):\n",
    "    indicadora_name = \"tiene_\" + col_name\n",
    "    df[indicadora_name] = df[col_name].notna()\n",
    "    \n",
    "    df = df.merge(predictions_df, how='left', on='id')\n",
    "    original_col = col_name + \"_x\"\n",
    "    filler_col = col_name + \"_y\"\n",
    "    df[col_name] = df.apply(lambda x: x[filler_col] if pd.isna(x[original_col]) else x[original_col], axis=1)\n",
    "    df.drop([original_col,filler_col], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_values(df):\n",
    "    df = fillna_with_models_predictions(df, rescued_antiguedad, 'antiguedad')\n",
    "    df = fillna_with_models_predictions(df, rescued_banos, 'banos')\n",
    "    df = fillna_with_models_predictions(df, rescued_garages, 'garages')\n",
    "    df = fillna_with_models_predictions(df, rescued_habitaciones, 'habitaciones')\n",
    "    df = fillna_with_models_predictions(df, rescued_metroscubiertos, 'metroscubiertos')\n",
    "    df = fillna_with_models_predictions(df, rescued_metrostotales, 'metrostotales')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import  Pool\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parallelize_dataframe(train, fill_na_values, 8)\n",
    "test = parallelize_dataframe(test, fill_na_values, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregamos features que ya hemos creado para analisis de tp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_amenities(row):\n",
    "    return row['gimnasio'] + row['usosmultiples'] + row['piscina'] + row['escuelascercanas'] + row['centroscomercialescercanos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cant_amenities'] = train.apply(lambda x: contar_amenities(x), axis=1)\n",
    "test['cant_amenities'] = test.apply(lambda x: contar_amenities(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fechas(df):\n",
    "    # Para entender lo de los senos y cosenos: https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "    df['year'] = df['fecha'].dt.year\n",
    "    df['month'] = df['fecha'].dt.month\n",
    "    df['day'] = df['fecha'].dt.day\n",
    "    df['sin_month'] = np.sin(2*np.pi*df['month']/12)\n",
    "    df['cos_month'] = np.cos(2*np.pi*df['month']/12)\n",
    "    # tomo cant. de dias en mes: 31 en todos los casos. Para esto deberia servir bastante bien igual.\n",
    "    df['sin_day'] = np.sin(2*np.pi*df['day']/31)\n",
    "    df['cos_day'] = np.cos(2*np.pi*df['day']/31)\n",
    "    \n",
    "    # no necesito mas las cols. originales de month y day.\n",
    "    df.drop(['month','day'], axis=1, inplace=True)\n",
    "    \n",
    "feature_fechas(train)\n",
    "feature_fechas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_avenida = ['avenida', 'av']\n",
    "train['es_avenida'] = train['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))\n",
    "test['es_avenida'] = test['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Fin agregado de features de tp1 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sample submission no tiene header. **Ojo con eso al guardar la submission.** Hagamos la funcion para guardar submissions ahora, para evitar problemas a futuro y despreocuparnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save predictions.\n",
    "# There must be a directory ../predictions for this to work as expected.\n",
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "import time\n",
    "def _get_filename(my_name, timestamp):\n",
    "    return \"../predictions/\" + timestamp + \" by \" + my_name + \".csv\"\n",
    "\n",
    "def _save_description(authors_name, timestamp, submission_description):\n",
    "    f = open(\"../predictions/\" + authors_name + \".txt\",\"a\")\n",
    "    f.write(timestamp + \": \" + submission_description + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def save_submission(submission_df, authors_name=\"fcozza\", description = \"no description.\", index=False, header=True):\n",
    "    timestamp = time.strftime(\"%Y.%m.%d - %H:%M:%S\")\n",
    "    submission_df.to_csv(_get_filename(authors_name, timestamp), index=index, header=header)\n",
    "    _save_description(authors_name, timestamp, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del X, y, X_train, X_test, y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a seed, so all algorithms that accept a seed, take the same, for consistency reasons,\n",
    "# so everything can be replicated without problems random state\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.1 - XGBoost con CV basico solo features de matriz de correlacion(Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/feature-selection-for-machine-learning-1-2-1597d9ccb54a \n",
    "- https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['gimnasio','usosmultiples','piscina','cant_palabras_positivas','cant_areas_verdes','tiene_bodega',\\\n",
    "                'tiene_servicio','tiene_seguridad','banos','garages','habitaciones','metroscubiertos','metrostotales','year',\\\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:  2.1min remaining:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:  2.1min remaining:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  2.2min remaining:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:  2.2min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:  2.3min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:  2.4min remaining:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  3.3min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=3,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=-1,\n",
       "                                    objective='reg:squarederror',\n",
       "                                    random_stat...\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [1], 'gamma': [30],\n",
       "                         'learning_rate': [0.06], 'max_depth': [20, 22, 25],\n",
       "                         'min_child_weight': [11], 'n_estimators': [200],\n",
       "                         'reg_alpha': [10], 'subsample': [0.946934]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [20,22,25],\n",
    "    \"learning_rate\": [0.06], \n",
    "    \"colsample_bytree\": [1],\n",
    "    \"subsample\": [0.946934], \n",
    "    \"gamma\":[30],\n",
    "    'reg_alpha': [10],\n",
    "    \"min_child_weight\": [11]\n",
    "}\n",
    "\n",
    "regXGB = xgb.XGBRegressor(objective ='reg:squarederror',nthread=-1) \n",
    "\n",
    "regXGBwithCV = GridSearchCV(regXGB, params, n_jobs=-1,verbose=10,cv=3) # n_iters es la cant de veces que busca, 10 es lo default\n",
    "\n",
    "regXGBwithCV.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.568280850314891\n",
      "{'colsample_bytree': 1, 'gamma': 30, 'learning_rate': 0.06, 'max_depth': 20, 'min_child_weight': 11, 'n_estimators': 200, 'reg_alpha': 10, 'subsample': 0.946934}\n"
     ]
    }
   ],
   "source": [
    "print(regXGBwithCV.best_score_)\n",
    "print(regXGBwithCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No sirve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.2 - XGBoost con CV basico solo features de BoostARoota (Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/chasedehan/BoostARoota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['gimnasio','usosmultiples','centroscomercialescercanos','cant_palabras_positivas','cant_areas_dedicadas',\\\n",
    "                   'cant_areas_verdes','cant_areas_entretenimiento_cerca','planta_alta','planta_baja','tiene_bodega',\\\n",
    "                   'comercial','tiene_servicio','edificio','casa','usa_easybroker','tiene_seguridad','tiene_antiguedad',\\\n",
    "                   'antiguedad','tiene_banos','banos','tiene_garages','garages','tiene_habitaciones','habitaciones',\\\n",
    "                   'tiene_metroscubiertos','metroscubiertos','tiene_metrostotales','metrostotales','cant_amenities','year',\\\n",
    "                   'sin_month','sin_day'\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:  3.8min remaining: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:  3.8min remaining:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  4.1min remaining:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:  4.2min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:  4.2min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:  4.5min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  6.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  6.6min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                    colsample_bylevel=1, colsample_bynode=1,\n",
       "                                    colsample_bytree=1, gamma=0,\n",
       "                                    importance_type='gain', learning_rate=0.1,\n",
       "                                    max_delta_step=0, max_depth=3,\n",
       "                                    min_child_weight=1, missing=None,\n",
       "                                    n_estimators=100, n_jobs=1, nthread=-1,\n",
       "                                    objective='reg:squarederror',\n",
       "                                    random_stat...\n",
       "                                    scale_pos_weight=1, seed=None, silent=None,\n",
       "                                    subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [1], 'gamma': [30],\n",
       "                         'learning_rate': [0.06], 'max_depth': [20, 22, 25],\n",
       "                         'min_child_weight': [11], 'n_estimators': [200],\n",
       "                         'reg_alpha': [10], 'subsample': [0.946934]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": [200],\n",
    "    \"max_depth\": [20,22,25],\n",
    "    \"learning_rate\": [0.06], \n",
    "    \"colsample_bytree\": [1],\n",
    "    \"subsample\": [0.946934], \n",
    "    \"gamma\":[30],\n",
    "    'reg_alpha': [10],\n",
    "    \"min_child_weight\": [11]\n",
    "}\n",
    "\n",
    "regXGB = xgb.XGBRegressor(objective ='reg:squarederror',nthread=-1) \n",
    "\n",
    "regXGBwithCV = GridSearchCV(regXGB, params, n_jobs=-1,verbose=10,cv=3) # n_iters es la cant de veces que busca, 10 es lo default\n",
    "\n",
    "regXGBwithCV.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6387103697028009\n",
      "{'colsample_bytree': 1, 'gamma': 30, 'learning_rate': 0.06, 'max_depth': 20, 'min_child_weight': 11, 'n_estimators': 200, 'reg_alpha': 10, 'subsample': 0.946934}\n"
     ]
    }
   ],
   "source": [
    "print(regXGBwithCV.best_score_)\n",
    "print(regXGBwithCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No sirve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.3 - XGBoost con CV y todos los features (categorical encoding 1 - label encoding) (Random Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@songxia.sophia/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_columns = ['ciudad', 'tipodepropiedad', 'provincia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df = X[encode_columns]\n",
    "encode_df = encode_df.astype('str')\n",
    "encode_df = encode_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encode_drop = X.drop(encode_columns, axis = 1)\n",
    "score_encode = pd.concat([score_encode_drop, encode_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(score_encode, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"colsample_bytree\": uniform(0.7, 0.3),\n",
    "          \"gamma\": uniform(0, 0.5),\n",
    "          \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n",
    "          \"max_depth\": randint(2, 6), # default 3\n",
    "          \"n_estimators\": randint(100, 250), # default 100\n",
    "          \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=seed)\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits = 8)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=seed,\\\n",
    "                                n_iter=4, cv=time_split, verbose=1, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  2.1min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.9 s, sys: 324 ms, total: 37.2 s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=8),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1...\n",
       "                                        'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27bfc9f828>,\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27c9150e48>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27c13e7438>,\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f27c3a85588>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_search.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770412558172045\n",
      "{'colsample_bytree': 0.7467983561008608, 'gamma': 0.02904180608409973, 'learning_rate': 0.2628528437324805, 'max_depth': 5, 'n_estimators': 203, 'subsample': 0.8832290311184181}\n"
     ]
    }
   ],
   "source": [
    "print(xgb_search.best_score_)\n",
    "print(xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 977271.2515415936\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_search.predict(X_test)\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print ('RMSE:', rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio'], axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_columns = ['ciudad', 'tipodepropiedad', 'provincia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df = X[encode_columns]\n",
    "encode_df = encode_df.astype('str')\n",
    "encode_df = encode_df.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encode_drop = X.drop(encode_columns, axis = 1)\n",
    "score_encode = pd.concat([score_encode_drop, encode_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 s, sys: 240 ms, total: 56.2 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Entrenar modelo\n",
    "model = xgb.XGBRegressor(colsample_bytree=0.7467983561008608,gamma=0.02904180608409973,learning_rate=0.2628528437324805,max_depth=5,n_estimators=203,\\\n",
    "                 subsample=0.8832290311184181,nthread=-1,objective ='reg:squarederror')\n",
    "\n",
    "model.fit(score_encode, y,eval_metric=\"rmse\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_encode = test.drop(['id','fecha','titulo', 'descripcion', 'direccion'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df_test = test_to_encode[encode_columns]\n",
    "encode_df_test = encode_df_test.astype('str')\n",
    "encode_df_test = encode_df_test.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_encode_drop_test = test_to_encode.drop(encode_columns, axis = 1)\n",
    "score_encode_test = pd.concat([score_encode_drop_test, encode_df_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(score_encode_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"1st approach_full_features. XGBoost\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.4 - XGBoost con CV y todos los features (categorical encoding 2) (Random Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@songxia.sophia/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns = ['ciudad', 'tipodepropiedad', 'provincia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_df = pd.get_dummies(X, columns = onehot_columns)\n",
    "score_onehot_drop = X.drop(onehot_columns, axis = 1)\n",
    "score_onehot = pd.concat([score_onehot_drop, onehot_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(score_onehot, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"colsample_bytree\": uniform(0.7, 0.3),\n",
    "          \"gamma\": uniform(0, 0.5),\n",
    "          \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n",
    "          \"max_depth\": randint(2, 6), # default 3\n",
    "          \"n_estimators\": randint(100, 250), # default 100\n",
    "          \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=seed)\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits = 8)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=seed,\\\n",
    "                                n_iter=4, cv=time_split, verbose=1, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:,~X_train.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    685\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1467\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    664\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 666\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tester/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_search.fit(X_train, y_train, eval_metric=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot cuelga la pc con las dimensiones, habra que achicarlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en el approach 2 de Matias en first approaches by rozanecm uso lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "params = {\"colsample_bytree\": uniform(0.7, 0.3),\n",
    "          \"gamma\": uniform(0, 0.5),\n",
    "          \"learning_rate\": uniform(0.003, 0.3), # default 0.1 \n",
    "          \"max_depth\": randint(2, 6), # default 3\n",
    "          \"n_estimators\": randint(100, 250), # default 100\n",
    "          \"subsample\": uniform(0.6, 0.4)}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=seed)\n",
    "\n",
    "time_split = TimeSeriesSplit(n_splits = 8)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=seed,\\\n",
    "                                n_iter=4, cv=time_split, verbose=1, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_random_search\", xgb_search))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.8s\n",
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  3.4min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_random_search, total= 4.5min\n",
      "610214.3501041636\n"
     ]
    }
   ],
   "source": [
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.0s\n",
      "Fitting 8 folds for each of 4 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:  4.4min finished\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_random_search, total= 5.3min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"1st approach_full_features. XGBoost con one hot\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengamos predicciones para todas las propiedades en nuestro train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   5.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.9min\n",
      "613617.4667506837\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   7.9s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 3.1min\n",
      "619893.2199777354\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   6.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.9min\n",
      "611674.0503883368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"rozanecm_approach_2\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/on_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.5 - XGBoost parameter tunning (Grid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning max depth y min child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "                             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27)\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = xgb_model,param_grid = param_test1,n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search1\", gsearch1))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search1, total=17.1min\n",
      "573755.0267781329\n",
      "CPU times: user 1min 44s, sys: 1.07 s, total: 1min 45s\n",
      "Wall time: 17min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 9, 'min_child_weight': 5}, 0.8073939828312875)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning mas preciso sobre este tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[8,9,10],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = xgb_model,param_grid = param_test2, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search1\", gsearch2))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search1, total=19.6min\n",
      "CPU times: user 1min 56s, sys: 889 ms, total: 1min 57s\n",
      "Wall time: 19min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=140, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'max_depth': [8, 9, 10],\n",
       "                                          'min_child_weight': [4, 5, 6]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10, 'min_child_weight': 6}, 0.8090391201801971)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valores mayores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2b = {\n",
    " 'max_depth':[9,10,11,12],\n",
    " 'min_child_weight':[6,8,10,12]\n",
    "}\n",
    "\n",
    "gsearch2b = GridSearchCV(estimator = xgb_model,param_grid = param_test2b, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch2b))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   7.2s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=39.5min\n",
      "CPU times: user 1min 57s, sys: 1.32 s, total: 1min 58s\n",
      "Wall time: 39min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=140, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'max_depth': [9, 10, 11, 12],\n",
       "                                          'min_child_weight': [6, 8, 10, 12]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10, 'min_child_weight': 6}, 0.8090391201801971)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2b.best_params_, gsearch2b.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces 10 y 6 son optimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=140, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = xgb_model,param_grid = param_test3, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch3))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=14.0min\n",
      "CPU times: user 1min 57s, sys: 3.08 s, total: 2min\n",
      "Wall time: 15min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     min_child_weight=6,\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=140, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gamma': 0.0}, 0.8090391201801971)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = xgb_model,param_grid = param_test4, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch4))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.8s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=50.1min\n",
      "CPU times: user 2min 21s, sys: 1.54 s, total: 2min 22s\n",
      "Wall time: 50min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'colsample_bytree': [0.6, 0.7, 0.8,\n",
       "                                                               0.9],\n",
       "                                          'subsample': [0.6, 0.7, 0.8, 0.9]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'subsample': 0.9}, 0.8117147682581691)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(65,80,5)]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor( learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = xgb_model,param_grid = param_test5, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch5))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  11.2s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=35.2min\n",
      "CPU times: user 2min 17s, sys: 1.44 s, total: 2min 19s\n",
      "Wall time: 35min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.8,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'colsample_bytree': [0.65, 0.7, 0.75],\n",
       "                                          'subsample': [0.8, 0.85, 0.9, 0.95]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'colsample_bytree': 0.7, 'subsample': 0.95}, 0.8121743836444513)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch6 = GridSearchCV(estimator = xgb_model,param_grid = param_test6, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch6))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.2s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=18.6min\n",
      "CPU times: user 2min 17s, sys: 772 ms, total: 2min 18s\n",
      "Wall time: 18min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'reg_alpha': [1e-05, 0.01, 0.1, 1,\n",
       "                                                        100]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 1}, 0.8123286654703807)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch7 = GridSearchCV(estimator = xgb_model,param_grid = param_test7, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch7))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  10.0s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=18.2min\n",
      "CPU times: user 2min 17s, sys: 1.67 s, total: 2min 19s\n",
      "Wall time: 18min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=0, reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'reg_alpha': [0.5, 0.75, 1, 1.25, 1.5,\n",
       "                                                        2]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg_alpha': 1.5}, 0.8123413160129532)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test8 = {\n",
    " 'n_estimators':[100, 150, 200, 250, 300]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch8 = GridSearchCV(estimator = xgb_model,param_grid = param_test8, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch8))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  12.5s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=17.4min\n",
      "CPU times: user 3min 18s, sys: 1.61 s, total: 3min 20s\n",
      "Wall time: 17min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'n_estimators': [100, 150, 200, 250,\n",
       "                                                           300]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 300}, 0.8136330892288279)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch8.best_params_, gsearch8.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test9 = {\n",
    " 'n_estimators':[300, 500, 700, 900, 1200]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch9 = GridSearchCV(estimator = xgb_model,param_grid = param_test9, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch9))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.0s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=57.2min\n",
      "CPU times: user 9min 29s, sys: 1.59 s, total: 9min 31s\n",
      "Wall time: 57min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'n_estimators': [300, 500, 700, 900,\n",
       "                                                           1200]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 900}, 0.8151624288480696)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch9.best_params_, gsearch9.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test10 = {\n",
    " 'learning_rate':[0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch10 = GridSearchCV(estimator = xgb_model,param_grid = param_test10, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch10))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=13.2min\n",
      "CPU times: user 2min 16s, sys: 1.44 s, total: 2min 17s\n",
      "Wall time: 13min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     missing=None,\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'learning_rate': [0.001, 0.01, 0.1,\n",
       "                                                            1]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1}, 0.8123413160129532)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch10.best_params_, gsearch10.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test11 = {\n",
    " 'learning_rate':[0.075, 0.085, 0.095, 0.1, 0.15]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)\n",
    "\n",
    "gsearch11 = GridSearchCV(estimator = xgb_model,param_grid = param_test11, n_jobs=-1,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_grid_search\", gsearch11))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   4.1s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_grid_search, total=16.9min\n",
      "CPU times: user 2min 16s, sys: 1.17 s, total: 2min 18s\n",
      "Wall time: 17min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_trans',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('small_cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('category_imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value='',\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('one_hot',\n",
       "                                                                   OneHotEncoder(categoric...\n",
       "                                                     n_estimators=200, n_jobs=1,\n",
       "                                                     nthread=-1,\n",
       "                                                     objective='reg:squarederror',\n",
       "                                                     random_state=0,\n",
       "                                                     reg_alpha=1.5,\n",
       "                                                     reg_lambda=1,\n",
       "                                                     scale_pos_weight=1,\n",
       "                                                     seed=27, silent=None,\n",
       "                                                     subsample=0.95,\n",
       "                                                     verbosity=1),\n",
       "                              iid=False, n_jobs=-1,\n",
       "                              param_grid={'learning_rate': [0.075, 0.085, 0.095,\n",
       "                                                            0.1, 0.15]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=0))],\n",
       "         verbose=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1}, 0.8123413160129532)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch11.best_params_, gsearch11.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_best_params\", xgb_model))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   9.0s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total= 2.3min\n",
      "561393.1192913017\n",
      "CPU times: user 2min 19s, sys: 1.07 s, total: 2min 20s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total= 3.4min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(X, axis=1).replace({True:1,False:0}), y)\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"1st approach_full_features. XGBoost with tunning parameters\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengamos predicciones para todas las propiedades en nuestro train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   5.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.9min\n",
      "613617.4667506837\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   7.9s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 3.1min\n",
      "619893.2199777354\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   6.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.9min\n",
      "611674.0503883368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"rozanecm_approach_2\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/on_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1.6 - XGBoost optimum parameters + log precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_best_params\", xgb_model))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total=  32.4s\n",
      "0.22766361886806274\n",
      "CPU times: user 4min 16s, sys: 1.62 s, total: 4min 18s\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562383.1390865688\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2: LightGBM optimizado with log price + features desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "                                  importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "                                  min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "                                  n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "                                  random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "                                  subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optimized.fit(X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train, sample_weight=None, init_score=None, eval_set=[(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1),y_test)], eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=10,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "y_scores = gbm_optimized.predict(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optimized.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y, sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"2nd approach. LightGBM previous grid search. Log(precio) y features descripcion\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 2.1: LightGBM with log price + features desc (grid search con nuevos features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 64.5min\n",
      "[Parallel(n_jobs=-1)]: Done 648 out of 648 | elapsed: 67.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.05, max_depth=25,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMRegressor(silent=False)\n",
    "\n",
    "param_dist = {\"boosting_type\":['gbdt','dart'],\n",
    "              \"max_depth\": [25,50,75],\n",
    "              \"learning_rate\" : [0.001,0.01,0.05,0.1],\n",
    "              \"num_leaves\": [300,900,1200],\n",
    "              \"n_estimators\": [50,100,200],\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(gbm, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"neg_mean_absolute_error\", verbose=5)\n",
    "grid_search.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21345376777287184\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(X_train.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train, sample_weight=None, init_score=None, eval_set=[(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1),y_test)], eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=10,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "y_scores = gbm_optimized.predict(X_test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525219.9818594557\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(np.exp(y_test), np.exp(y_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y, sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"2.1 approach. LightGBM with new grid search for new features of desc. Log(precio)\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3: Test promedio LightGBM + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_xgboost = pd.read_csv('../predictions/2019.11.27 - 01:34:19 by fcozza.csv')\n",
    "predictions_lightgbm = pd.read_csv('../predictions/2019.11.27 - 02:14:16 by fcozza.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promedio las predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.merge(predictions_xgboost,predictions_lightgbm,on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['target'] = (pred['target_x'] + pred['target_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred [['id','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"3rd approach. Promedio entre xgboost y lightgbm\"\n",
    "save_submission(pred,description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No mejor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
