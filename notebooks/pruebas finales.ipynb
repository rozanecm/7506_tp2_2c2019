{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow you to see all column names & rows when you are doing .head(). None of the column name will be truncated.\n",
    "# source: https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/38f2901c592bdffc40726cb0473318cf\n",
    "# Function which plays a beep of given duration and frequency.\n",
    "# Useful for when executing things that need a while to finish, to get notified.\n",
    "import os\n",
    "def beep(duration = 0.6, freq = 200):\n",
    "    \"\"\" play tone of duration in seconds and freq in Hz. \"\"\"\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_master.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])\n",
    "test = pd.read_csv('../data/test_master.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save predictions.\n",
    "# There must be a directory ../predictions for this to work as expected.\n",
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "import time\n",
    "def _get_filename(my_name, timestamp):\n",
    "    return \"../predictions/last_pred/\" + timestamp + \" by \" + my_name + \".csv\"\n",
    "\n",
    "def _save_description(authors_name, timestamp, submission_description):\n",
    "    f = open(\"../predictions/last_pred/\" + authors_name + \".txt\",\"a\")\n",
    "    f.write(timestamp + \": \" + submission_description + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def save_submission(submission_df, authors_name=\"fcozza\", description = \"no description.\", index=False, header=True):\n",
    "    timestamp = time.strftime(\"%Y.%m.%d - %H:%M:%S\")\n",
    "    submission_df.to_csv(_get_filename(authors_name, timestamp), index=index, header=header)\n",
    "    _save_description(authors_name, timestamp, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 1 - rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "cat_columns = ['tipodepropiedad','ciudad','provincia']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas','stopwords_count',\n",
    "    'punctuations_count','vocab_size','characters_count','top10_trigram_occ','top10_bigram_occ',\n",
    "    'numerics_count','avg_word','diversity_score','top_used_words_count','least_used_words_count',\n",
    "    'words_start_count','words_end_count','sentiment','ratio_cubiertos_totales','titulo_cant_html_tags',\n",
    "    'titulo_cant_palabras','titulo_cant_palabras_unicas','titulo_diversity_score','titulo_cant_caracteres',\n",
    "    'titulo_cant_signos_puntuacion','titulo_entropy','titulo_mean_word_length','descripcion_cant_html_tags',\n",
    "    'descripcion_cant_palabras_unicas','descripcion_entropy']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "steps.append((\"rfr\", RandomForestRegressor(n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=seed)))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   1.6s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.1min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 1\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   1.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 1.2min\n",
      "743609.9634729946\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   1.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 1.2min\n",
      "745952.5537785584\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   1.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 1.2min\n",
      "739375.8240193245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "approach_numer = \"fcozza_target_1\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], 'approach_1':y_scores}))\n",
    "\n",
    "df.to_csv('../predictions/last_train_data/' + approach_numer,index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 2 - rf + one hot + svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas','stopwords_count',\n",
    "    'punctuations_count','vocab_size','characters_count','top10_trigram_occ','top10_bigram_occ',\n",
    "    'numerics_count','avg_word','diversity_score','top_used_words_count','least_used_words_count',\n",
    "    'words_start_count','words_end_count','sentiment','ratio_cubiertos_totales','titulo_cant_html_tags',\n",
    "    'titulo_cant_palabras','titulo_cant_palabras_unicas','titulo_diversity_score','titulo_cant_caracteres',\n",
    "    'titulo_cant_signos_puntuacion','titulo_entropy','titulo_mean_word_length','descripcion_cant_html_tags',\n",
    "    'descripcion_cant_palabras_unicas','descripcion_entropy']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "steps.append((\"rfr\", RandomForestRegressor(n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=seed)))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   4.1s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 2.7min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 2\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.8s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 1.7min\n",
      "602745.5106908402\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.7s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 1.7min\n",
      "607534.2621182624\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.7s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 1.7min\n",
      "600904.0241585325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "approach_numer = \"fcozza_target_2\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], 'approach_1':y_scores}))\n",
    "\n",
    "df.to_csv('../predictions/last_train_data/' + approach_numer,index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 3 - rf + one hashing vectorizer + svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas','stopwords_count',\n",
    "    'punctuations_count','vocab_size','characters_count','top10_trigram_occ','top10_bigram_occ',\n",
    "    'numerics_count','avg_word','diversity_score','top_used_words_count','least_used_words_count',\n",
    "    'words_start_count','words_end_count','sentiment','ratio_cubiertos_totales','titulo_cant_html_tags',\n",
    "    'titulo_cant_palabras','titulo_cant_palabras_unicas','titulo_diversity_score','titulo_cant_caracteres',\n",
    "    'titulo_cant_signos_puntuacion','titulo_entropy','titulo_mean_word_length','descripcion_cant_html_tags',\n",
    "    'descripcion_cant_palabras_unicas','descripcion_entropy']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "steps.append((\"rfr\", RandomForestRegressor(n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=seed)))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  28.3s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 6.9min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 3\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  23.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 4.3min\n",
      "611115.8825614405\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  23.6s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 4.2min\n",
      "617234.5271664321\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  23.6s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 4.2min\n",
      "610302.0091033245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_3\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], 'approach_1':y_scores}))\n",
    "\n",
    "df.to_csv('../predictions/last_train_data/' + approach_numer,index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 4 - rf + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas','stopwords_count',\n",
    "    'punctuations_count','vocab_size','characters_count','top10_trigram_occ','top10_bigram_occ',\n",
    "    'numerics_count','avg_word','diversity_score','top_used_words_count','least_used_words_count',\n",
    "    'words_start_count','words_end_count','sentiment','ratio_cubiertos_totales','titulo_cant_html_tags',\n",
    "    'titulo_cant_palabras','titulo_cant_palabras_unicas','titulo_diversity_score','titulo_cant_caracteres',\n",
    "    'titulo_cant_signos_puntuacion','titulo_entropy','titulo_mean_word_length','descripcion_cant_html_tags',\n",
    "    'descripcion_cant_palabras_unicas','descripcion_entropy']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', stop_words=get_stop_words('spanish'),\n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "steps.append((\"rfr\", RandomForestRegressor(n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=seed)))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  27.5s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 6.8min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 4\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  23.2s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 4.2min\n",
      "603622.4021449108\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  23.4s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 4.2min\n",
      "610608.4406444031\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  23.8s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfr, total= 4.2min\n",
      "602515.9190960078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_4\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], 'approach_1':y_scores}))\n",
    "\n",
    "df.to_csv('../predictions/last_train_data/' + approach_numer,index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 5 -lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "gbm = lgb.LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1,\n",
    "                        n_estimators=100, subsample_for_bin=200000, objective=None,\n",
    "                        class_weight=None, min_split_gain=0.0, min_child_weight=0.001,\n",
    "                        min_child_samples=20, subsample=1.0, subsample_freq=0,\n",
    "                        colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0,\n",
    "                        random_state=seed, n_jobs=-1, silent=True, importance_type='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm.fit(train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio'],axis=1), train['precio'], sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 5\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588652.1770681143\n",
      "593555.9972213265\n",
      "591447.8585886128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_5\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    gbm.fit(X_train2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train2)\n",
    "    y_scores = gbm.predict(X_test2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 6 - lightgbm with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "              importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "              n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio'],axis=1), train['precio'], sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 6\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588652.1770681143\n",
      "593555.9972213265\n",
      "591447.8585886128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_6\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    gbm.fit(X_train2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train2)\n",
    "    y_scores = gbm.predict(X_test2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 8 - lightgbm grid search + feat eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "              importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "              n_estimators=300, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "              random_state=seed, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio'],axis=1), train['precio'], sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 7\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588652.1770681143\n",
      "593555.9972213265\n",
      "591447.8585886128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_7\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    gbm.fit(X_train2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train2)\n",
    "    y_scores = gbm.predict(X_test2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 12 - lightgbm log precio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(['precio', 'precio_log'], axis=1), train['precio_log'], test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "              importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "              n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio','precio_log'],axis=1), train['precio_log'], sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 8\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21309995483007793\n",
      "525440.1788438802\n",
      "0.21405842690778873\n",
      "531720.4468441766\n",
      "0.21340811161957143\n",
      "525368.0651654683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_8\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio','precio_log'],axis=1).iloc[train_index], train.drop(['precio','precio_log'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio_log'][train_index], train['precio_log'][test_index]\n",
    "    \n",
    "    gbm_optimized.fit(X_train2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train2)\n",
    "    y_scores = gbm_optimized.predict(X_test2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    print(mean_absolute_error(np.exp(y_test2), np.exp(y_scores)))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:np.exp(y_scores)}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 13 - lightgbm log precio y skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, skewtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_to_log(df, attribute_name, treshold=0.5):\n",
    "    original_skew = skew(df[attribute_name])\n",
    "    print(\"skew as is:\", skew(df[attribute_name]))\n",
    "    if abs(original_skew) < treshold:\n",
    "        print(\"No action needed.\")\n",
    "    else:\n",
    "        df[attribute_name] = df[attribute_name].astype('float64')\n",
    "        df[attribute_name] = df[attribute_name].fillna(df[attribute_name].mean())\n",
    "        df.loc[:,attribute_name] = np.log1p(df[attribute_name])\n",
    "        print(\"skew logged variable:\", skew(df[attribute_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing id\n",
      "skew as is: 0.0011793797535118043\n",
      "No action needed.\n",
      "skew as is: -0.004728242943954671\n",
      "No action needed.\n",
      "\n",
      "processing idzona\n",
      "skew as is: nan\n",
      "skew logged variable: -4.363605150634373\n",
      "skew as is: nan\n",
      "skew logged variable: -4.361812715085782\n",
      "\n",
      "processing lat\n",
      "skew as is: nan\n",
      "skew logged variable: nan\n",
      "skew as is: nan\n",
      "skew logged variable: nan\n",
      "\n",
      "processing lon\n",
      "skew as is: nan\n",
      "skew logged variable: nan\n",
      "skew as is: nan\n",
      "skew logged variable: nan\n",
      "\n",
      "processing cant_comodidades_en_desc\n",
      "skew as is: 0.3203184338289262\n",
      "No action needed.\n",
      "skew as is: 0.3437700969660058\n",
      "No action needed.\n",
      "\n",
      "processing cant_palabras_positivas\n",
      "skew as is: 0.8450344935580002\n",
      "skew logged variable: -0.1109111394348578\n",
      "skew as is: 0.8377301106860434\n",
      "skew logged variable: -0.11771546615615593\n",
      "\n",
      "processing cant_areas_dedicadas\n",
      "skew as is: 0.1495034193923016\n",
      "No action needed.\n",
      "skew as is: 0.14971863295484558\n",
      "No action needed.\n",
      "\n",
      "processing cant_areas_verdes\n",
      "skew as is: 1.1204940975046116\n",
      "skew logged variable: 0.36039703584814203\n",
      "skew as is: 1.1301671121205348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/pandas/core/series.py:856: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skew logged variable: 0.36171784299692944\n",
      "\n",
      "processing cant_areas_entretenimiento_cerca\n",
      "skew as is: 3.012321645202613\n",
      "skew logged variable: 2.6317511461851337\n",
      "skew as is: 3.027727681462814\n",
      "skew logged variable: 2.626777024366695\n",
      "\n",
      "processing cant_lugares_cerca\n",
      "skew as is: 2.3572148515091556\n",
      "skew logged variable: 1.4532223772986501\n",
      "skew as is: 2.3686111788255064\n",
      "skew logged variable: 1.4498184957330698\n",
      "\n",
      "processing stopwords_count\n",
      "skew as is: 4.078255760033841\n",
      "skew logged variable: -0.46169959032628877\n",
      "skew as is: 3.9831152764002224\n",
      "skew logged variable: -0.4528969774267983\n",
      "\n",
      "processing punctuations_count\n",
      "skew as is: 34.22697275045236\n",
      "skew logged variable: -0.0838840608742315\n",
      "skew as is: 204.23250811658554\n",
      "skew logged variable: -0.06391437768999186\n",
      "\n",
      "processing vocab_size\n",
      "skew as is: 8.71289796022834\n",
      "skew logged variable: -0.7329696705887189\n",
      "skew as is: 32.524792328567756\n",
      "skew logged variable: -0.7266017472333228\n",
      "\n",
      "processing characters_count\n",
      "skew as is: 17.484879580158903\n",
      "skew logged variable: -1.248281782639822\n",
      "skew as is: 47.21581612914117\n",
      "skew logged variable: -1.2475630780406841\n",
      "\n",
      "processing top10_trigram_occ\n",
      "skew as is: 75.24874550868888\n",
      "skew logged variable: 1.6572110362043673\n",
      "skew as is: 69.23374377408814\n",
      "skew logged variable: 1.7754111298638278\n",
      "\n",
      "processing top10_bigram_occ\n",
      "skew as is: 55.93014984992328\n",
      "skew logged variable: 0.3132002629357764\n",
      "skew as is: 57.64858212309172\n",
      "skew logged variable: 0.3703607448753499\n",
      "\n",
      "processing numerics_count\n",
      "skew as is: 73.43943550629021\n",
      "skew logged variable: -0.12245399108707639\n",
      "skew as is: 97.61805867009474\n",
      "skew logged variable: -0.12145320048550261\n",
      "\n",
      "processing avg_word\n",
      "skew as is: 22.501835310713822\n",
      "skew logged variable: -5.458248367491069\n",
      "skew as is: 0.03514021624183642\n",
      "No action needed.\n",
      "\n",
      "processing diversity_score\n",
      "skew as is: nan\n",
      "skew logged variable: 1.1394237113258108\n",
      "skew as is: nan\n",
      "skew logged variable: 1.5500693390962712\n",
      "\n",
      "processing top_used_words_count\n",
      "skew as is: 13.463885480523334\n",
      "skew logged variable: -0.6732325109478232\n",
      "skew as is: 212.02835086954738\n",
      "skew logged variable: -0.655708340579623\n",
      "\n",
      "processing least_used_words_count\n",
      "skew as is: 30.276914442457247\n",
      "skew logged variable: 7.939630078084943\n",
      "skew as is: 19.350718733477674\n",
      "skew logged variable: 10.788198262557032\n",
      "\n",
      "processing words_start_count\n",
      "skew as is: 9.307403767169834\n",
      "skew logged variable: -0.850874172677555\n",
      "skew as is: 5.416433411697173\n",
      "skew logged variable: -0.8618225798142787\n",
      "\n",
      "processing words_end_count\n",
      "skew as is: 82.82250317532241\n",
      "skew logged variable: 0.0019551769261513478\n",
      "skew as is: 22.05776977790026\n",
      "skew logged variable: 0.00017039158486774634\n",
      "\n",
      "processing sentiment\n",
      "skew as is: 3.1240102765492495\n",
      "skew logged variable: nan\n",
      "skew as is: 3.1242499828210493\n",
      "skew logged variable: -0.24261175529632603\n",
      "\n",
      "processing antiguedad\n",
      "skew as is: inf\n",
      "skew logged variable: nan\n",
      "skew as is: inf\n",
      "skew logged variable: nan\n",
      "\n",
      "processing banos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/pandas/core/series.py:856: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/scipy/stats/stats.py:984: RuntimeWarning: invalid value encountered in subtract\n",
      "  a_zero_mean = a - np.expand_dims(np.mean(a, axis), axis)\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/scipy/stats/stats.py:994: RuntimeWarning: overflow encountered in multiply\n",
      "  s *= a_zero_mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skew as is: 0.3984375\n",
      "No action needed.\n",
      "skew as is: 0.3984375\n",
      "No action needed.\n",
      "\n",
      "processing garages\n",
      "skew as is: -0.34033203125\n",
      "No action needed.\n",
      "skew as is: -0.334716796875\n",
      "No action needed.\n",
      "\n",
      "processing habitaciones\n",
      "skew as is: 1.91796875\n",
      "skew logged variable: 0.23057799946024607\n",
      "skew as is: 1.9658203125\n",
      "skew logged variable: 0.25954977968052134\n",
      "\n",
      "processing metroscubiertos\n",
      "skew as is: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/scipy/stats/stats.py:988: RuntimeWarning: overflow encountered in square\n",
      "  s = a_zero_mean**2\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/scipy/stats/stats.py:992: RuntimeWarning: overflow encountered in square\n",
      "  s = s**2\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/numpy/core/_methods.py:151: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skew logged variable: -0.23724252162629508\n",
      "skew as is: nan\n",
      "skew logged variable: -0.2306564648088782\n",
      "\n",
      "processing metrostotales\n",
      "skew as is: nan\n",
      "skew logged variable: -0.1995989553526971\n",
      "skew as is: nan\n",
      "skew logged variable: -0.19429339826260794\n",
      "\n",
      "processing cant_amenities\n",
      "skew as is: 0.8957617677434134\n",
      "skew logged variable: 0.3085083320508332\n",
      "skew as is: 0.8860637249233118\n",
      "skew logged variable: 0.3016688029773044\n",
      "\n",
      "processing ratio_cubiertos_totales\n",
      "skew as is: 10.720785073600561\n",
      "skew logged variable: 1.0752118201951253\n",
      "skew as is: 11.967061907854456\n",
      "skew logged variable: 1.1077890959412882\n",
      "\n",
      "processing titulo_cant_html_tags\n",
      "skew as is: 148.89299674995277\n",
      "skew logged variable: 126.96764643012544\n",
      "skew as is: 244.05764834500965\n",
      "skew logged variable: 229.7406926077282\n",
      "\n",
      "processing titulo_cant_palabras\n",
      "skew as is: 0.4322613811550008\n",
      "No action needed.\n",
      "skew as is: 0.3890690915954755\n",
      "No action needed.\n",
      "\n",
      "processing titulo_cant_palabras_unicas\n",
      "skew as is: 0.38944633730612577\n",
      "No action needed.\n",
      "skew as is: 0.3747204797536026\n",
      "No action needed.\n",
      "\n",
      "processing titulo_diversity_score\n",
      "skew as is: nan\n",
      "skew logged variable: -1.3615095878506922\n",
      "skew as is: nan\n",
      "skew logged variable: -1.3383764310886084\n",
      "\n",
      "processing titulo_cant_caracteres\n",
      "skew as is: 0.300807282252804\n",
      "No action needed.\n",
      "skew as is: 0.28540163859461654\n",
      "No action needed.\n",
      "\n",
      "processing titulo_cant_signos_puntuacion\n",
      "skew as is: 0.9621768080762486\n",
      "skew logged variable: -0.6808242604777482\n",
      "skew as is: 0.9730902484934241\n",
      "skew logged variable: -0.6849184063846531\n",
      "\n",
      "processing titulo_entropy\n",
      "skew as is: -4.364309525971062\n",
      "skew logged variable: -5.673686942697464\n",
      "skew as is: -4.345474731486439\n",
      "skew logged variable: -5.620683041167865\n",
      "\n",
      "processing titulo_mean_word_length\n",
      "skew as is: nan\n",
      "skew logged variable: 0.6810160436414963\n",
      "skew as is: nan\n",
      "skew logged variable: 0.6993491474033943\n",
      "\n",
      "processing descripcion_cant_html_tags\n",
      "skew as is: 11.106326918796745\n",
      "skew logged variable: 1.9735564536209103\n",
      "skew as is: 227.89107421499423\n",
      "skew logged variable: 1.9801396244860716\n",
      "\n",
      "processing descripcion_cant_palabras_unicas\n",
      "skew as is: 2.190719492053053\n",
      "skew logged variable: -1.3146471097337955\n",
      "skew as is: 2.19780291973\n",
      "skew logged variable: -1.3148677983089376\n",
      "\n",
      "processing descripcion_entropy\n",
      "skew as is: -8.142041388906259\n",
      "skew logged variable: -10.564013993564432\n",
      "skew as is: -8.114266216886664\n",
      "skew logged variable: -10.522092331532997\n",
      "\n",
      "processing year\n",
      "skew as is: -0.6416179039326001\n",
      "skew logged variable: -0.6423498100306668\n",
      "skew as is: -0.645671272298429\n",
      "skew logged variable: -0.6464028484418369\n",
      "\n",
      "processing sin_month\n",
      "skew as is: 0.18064810550233013\n",
      "No action needed.\n",
      "skew as is: 0.18801475233017229\n",
      "No action needed.\n",
      "\n",
      "processing cos_month\n",
      "skew as is: -0.19083237711325823\n",
      "No action needed.\n",
      "skew as is: -0.19072397076222994\n",
      "No action needed.\n",
      "\n",
      "processing sin_day\n",
      "skew as is: -0.0508343537109758\n",
      "No action needed.\n",
      "skew as is: -0.03362615573877046\n",
      "No action needed.\n",
      "\n",
      "processing cos_day\n",
      "skew as is: -0.005063172406648998\n",
      "No action needed.\n",
      "skew as is: -0.00807423087888374\n",
      "No action needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in train.select_dtypes(include=[np.number]).drop(['precio','precio_log'],axis=1).columns:\n",
    "    print(\"processing\", column)\n",
    "    var_to_log(train, column)\n",
    "    var_to_log(test, column)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop(['precio', 'precio_log'], axis=1), train['precio_log'], test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "              importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "              n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "gbm_optimized.fit(train.drop(['id','fecha','titulo', 'descripcion', 'direccion','precio','precio_log'],axis=1), train['precio_log'], sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 9\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21378962889639574\n",
      "526812.0978341543\n",
      "0.2142419894711451\n",
      "532482.6769932512\n",
      "0.21400335999655526\n",
      "527176.0745130337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_9\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio','precio_log'],axis=1).iloc[train_index], train.drop(['precio','precio_log'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio_log'][train_index], train['precio_log'][test_index]\n",
    "    \n",
    "    gbm_optimized.fit(X_train2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train2)\n",
    "    y_scores = gbm_optimized.predict(X_test2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    print(mean_absolute_error(np.exp(y_test2), np.exp(y_scores)))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:np.exp(y_scores)}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 15 - xgboost tunned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('precio', axis=1) #set de datos\n",
    "y = train['precio'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(learning_rate =0.1, n_estimators=200, max_depth=10,\n",
    "                             min_child_weight=6, gamma=0, subsample=0.95, colsample_bytree=0.7,\n",
    "                             reg_alpha=1.5,\n",
    "                             objective= 'reg:squarederror', nthread=-1, scale_pos_weight=1,seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day','cant_comodidades_en_desc',\n",
    "    'cant_amenities','cant_lugares_cerca','cant_areas_entretenimiento_cerca',\n",
    "    'cant_areas_verdes','cant_areas_dedicadas','cant_palabras_positivas','stopwords_count',\n",
    "    'punctuations_count','vocab_size','characters_count','top10_trigram_occ','top10_bigram_occ',\n",
    "    'numerics_count','avg_word','diversity_score','top_used_words_count','least_used_words_count',\n",
    "    'words_start_count','words_end_count','sentiment','ratio_cubiertos_totales','titulo_cant_html_tags',\n",
    "    'titulo_cant_palabras','titulo_cant_palabras_unicas','titulo_diversity_score','titulo_cant_caracteres',\n",
    "    'titulo_cant_signos_puntuacion','titulo_entropy','titulo_mean_word_length','descripcion_cant_html_tags',\n",
    "    'descripcion_cant_palabras_unicas','descripcion_entropy']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida',\n",
    "               'planta_alta','planta_baja','tiene_bodega','oficina','cerca_o_en_esquina','cerca_o_en_avenida',\n",
    "               'comercial','tiene_servicio','edificio','casa','parte_de_lote','calle_cerrada',\n",
    "               'indica_frente_y_fondo','usa_easybroker','tiene_seguridad','tiene_antiguedad','tiene_banos',\n",
    "               'tiene_garages','tiene_habitaciones','tiene_metroscubiertos','tiene_metrostotales']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "#                 ,'descripcion'\n",
    "#                 ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "steps.append((\"xgboost_best_params\", xgb_model))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total= 1.2min\n"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(X.replace({True:1,False:0}), y)\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 10\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.8s\n",
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total=  39.9s\n",
      "562524.5181035156\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total=  39.8s\n",
      "567984.5908488312\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline]  (step 2 of 2) Processing xgboost_best_params, total=  39.3s\n",
      "562207.1580576171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_10\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train2, y_test2 = y[train_index], y[test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target 18 - lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['precio_log'] = np.log(train['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['precio', 'precio_log'], axis=1) #set de datos\n",
    "y = train['precio_log'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "gbm_optimized = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "                                  importance_type='split', learning_rate=0.05, max_depth=75,\n",
    "                                  min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "                                  n_estimators=200, n_jobs=-1, num_leaves=1200, objective=None,\n",
    "                                  random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
    "                                  subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabrizio/anaconda3/envs/tester/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['ciudad', 'provincia', 'tipodepropiedad']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 1.33 s, total: 4min 35s\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbm_optimized.fit(X.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y, sample_weight=None, init_score=None, eval_set=None, eval_names=None,\n",
    "            eval_sample_weight=None, eval_init_score=None, eval_metric='mae', early_stopping_rounds=None,\n",
    "            verbose=False, feature_name='auto', categorical_feature=['tipodepropiedad', 'ciudad','provincia'], callbacks=None)\n",
    "\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = gbm_optimized.predict(test.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), num_iteration=gbm_optimized.best_iteration_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_predictions = np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':exp_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"target 11\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21309995483007793\n",
      "525440.1788438802\n",
      "0.21405842690778873\n",
      "531720.4468441766\n",
      "0.21340811161957143\n",
      "525368.0651654683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"fcozza_target_11\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train2, y_test2 = y[train_index], y[test_index]\n",
    "    \n",
    "    gbm_optimized.fit(X_train2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1), y_train2)\n",
    "    y_scores = gbm_optimized.predict(X_test2.drop(['id','fecha','titulo', 'descripcion', 'direccion'],axis=1))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    print(mean_absolute_error(np.exp(y_test2), np.exp(y_scores)))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:np.exp(y_scores)}))\n",
    "\n",
    "df.to_csv(\"../predictions/last_train_data/\" + approach_numer, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
