{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow you to see all column names & rows when you are doing .head(). None of the column name will be truncated.\n",
    "# source: https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/38f2901c592bdffc40726cb0473318cf\n",
    "# Function which plays a beep of given duration and frequency.\n",
    "# Useful for when executing things that need a while to finish, to get notified.\n",
    "import os\n",
    "def beep(duration = 1, freq = 1500):\n",
    "    \"\"\" play tone of duration in seconds and freq in Hz. \"\"\"\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])\n",
    "test = pd.read_csv('../data/test.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])\n",
    "sample_submission = pd.read_csv('../data/ejemploRespuesta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescued_coords = pd.read_csv('../data/rescueLatLongs.csv')\n",
    "rescued_antiguedad = pd.read_csv('../data/imputations/antiguedad.csv')\n",
    "rescued_banos = pd.read_csv('../data/imputations/banos.csv')\n",
    "rescued_garages = pd.read_csv('../data/imputations/garages.csv')\n",
    "rescued_habitaciones = pd.read_csv('../data/imputations/habitaciones.csv')\n",
    "rescued_metroscubiertos = pd.read_csv('../data/imputations/metroscubiertos.csv')\n",
    "rescued_metrostotales = pd.read_csv('../data/imputations/metrostotales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeamos con coords. extra obtenidas en tp1.\n",
    "train['tiene_coordenadas'] = train['lat'].notna()\n",
    "train = train.merge(rescued_coords.drop('Unnamed: 0', axis=1), how='left', on='id')\n",
    "train['lat_x'] = train.apply(lambda x: x['lat_y'] if pd.isna(x['lat_x']) else x['lat_x'], axis=1)\n",
    "train['lng_x'] = train.apply(lambda x: x['lng_y'] if pd.isna(x['lng_x']) else x['lng_x'], axis=1)\n",
    "train.drop(['lat_y','lng_y'], axis=1, inplace=True)\n",
    "train.rename(columns={'lat_x':'lat','lng_x':'lon'}, inplace=True)\n",
    "\n",
    "# por consistencia, para que ambos datasets tengan mismos nombres\n",
    "test.rename(columns={'lng':'lon'}, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_models_predictions(df, predictions_df, col_name):\n",
    "    indicadora_name = \"tiene_\" + col_name\n",
    "    df[indicadora_name] = df[col_name].notna()\n",
    "    \n",
    "    df = df.merge(predictions_df, how='left', on='id')\n",
    "    original_col = col_name + \"_x\"\n",
    "    filler_col = col_name + \"_y\"\n",
    "    df[col_name] = df.apply(lambda x: x[filler_col] if pd.isna(x[original_col]) else x[original_col], axis=1)\n",
    "    df.drop([original_col,filler_col], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fillna_with_models_predictions(train, rescued_antiguedad, 'antiguedad')\n",
    "train = fillna_with_models_predictions(train, rescued_banos, 'banos')\n",
    "train = fillna_with_models_predictions(train, rescued_garages, 'garages')\n",
    "train = fillna_with_models_predictions(train, rescued_habitaciones, 'habitaciones')\n",
    "train = fillna_with_models_predictions(train, rescued_metroscubiertos, 'metroscubiertos')\n",
    "train = fillna_with_models_predictions(train, rescued_metrostotales, 'metrostotales')\n",
    "\n",
    "test = fillna_with_models_predictions(test, rescued_antiguedad, 'antiguedad')\n",
    "test = fillna_with_models_predictions(test, rescued_banos, 'banos')\n",
    "test = fillna_with_models_predictions(test, rescued_garages, 'garages')\n",
    "test = fillna_with_models_predictions(test, rescued_habitaciones, 'habitaciones')\n",
    "test = fillna_with_models_predictions(test, rescued_metroscubiertos, 'metroscubiertos')\n",
    "test = fillna_with_models_predictions(test, rescued_metrostotales, 'metrostotales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf. values don't make sense. I think it's preferable to treat them as nans directly.\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregamos features que ya hemos creado para analisis de tp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_amenities(row):\n",
    "    return row['gimnasio'] + row['usosmultiples'] + row['piscina'] + row['escuelascercanas'] + row['centroscomercialescercanos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cant_amenities'] = train.apply(lambda x: contar_amenities(x), axis=1)\n",
    "test['cant_amenities'] = test.apply(lambda x: contar_amenities(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fechas(df):\n",
    "    # Para entender lo de los senos y cosenos: https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "    df['year'] = df['fecha'].dt.year\n",
    "    df['month'] = df['fecha'].dt.month\n",
    "    df['day'] = df['fecha'].dt.day\n",
    "    df['sin_month'] = np.sin(2*np.pi*df['month']/12)\n",
    "    df['cos_month'] = np.cos(2*np.pi*df['month']/12)\n",
    "    # tomo cant. de dias en mes: 31 en todos los casos. Para esto deberia servir bastante bien igual.\n",
    "    df['sin_day'] = np.sin(2*np.pi*df['day']/31)\n",
    "    df['cos_day'] = np.cos(2*np.pi*df['day']/31)\n",
    "    \n",
    "    # no necesito mas las cols. originales de month y day.\n",
    "    df.drop(['month','day'], axis=1, inplace=True)\n",
    "    \n",
    "feature_fechas(train)\n",
    "feature_fechas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_avenida = ['avenida', 'av']\n",
    "train['es_avenida'] = train['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))\n",
    "test['es_avenida'] = test['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Fin agregado de features de tp1 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sample submission no tiene header. **Ojo con eso al guardar la submission.** Hagamos la funcion para guardar submissions ahora, para evitar problemas a futuro y despreocuparnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save predictions.\n",
    "# There must be a directory ../predictions for this to work as expected.\n",
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "import time\n",
    "def _get_filename(my_name, timestamp):\n",
    "    return \"../predictions/\" + timestamp + \" by \" + my_name + \".csv\"\n",
    "\n",
    "def _save_description(authors_name, timestamp, submission_description):\n",
    "    f = open(\"../predictions/\" + authors_name + \".txt\",\"a\")\n",
    "    f.write(timestamp + \": \" + submission_description + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def save_submission(submission_df, authors_name=\"rozanecm\", description = \"no description.\", index=False, header=True):\n",
    "    timestamp = time.strftime(\"%Y.%m.%d - %H:%M:%S\")\n",
    "    submission_df.to_csv(_get_filename(authors_name, timestamp), index=index, header=header)\n",
    "    _save_description(authors_name, timestamp, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a seed, so all algorithms that accept a seed, take the same, for consistency reasons,\n",
    "# so everything can be replicated without problems random state\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('precio', axis=1), train['precio'], test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
