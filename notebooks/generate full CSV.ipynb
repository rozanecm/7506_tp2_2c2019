{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow you to see all column names & rows when you are doing .head(). None of the column name will be truncated.\n",
    "# source: https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/38f2901c592bdffc40726cb0473318cf\n",
    "# Function which plays a beep of given duration and frequency.\n",
    "# Useful for when executing things that need a while to finish, to get notified.\n",
    "import os\n",
    "def beep(duration = 0.6, freq = 200):\n",
    "    \"\"\" play tone of duration in seconds and freq in Hz. \"\"\"\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregamos features creados a partir de descripcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = pd.read_csv('../data/train_desc_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc = pd.read_csv('../data/test_desc_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_feat = pd.read_csv('../data/train_simple_text_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_feat = pd.read_csv('../data/test_simple_text_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_kmeans_tfidf = pd.read_csv('../data/train_kmeans_tfidf_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_kmeans_tfidf = pd.read_csv('../data/test_kmeans_tfidf_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_kmeans_glove = pd.read_csv('../data/train_kmeans_glove_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_kmeans_glove = pd.read_csv('../data/test_kmeans_glove_cluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, train_desc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, test_desc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, train_new_feat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, test_new_feat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.concat([train, train_kmeans_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.concat([test, test_kmeans_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.concat([train, train_kmeans_glove], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.concat([test, test_kmeans_glove], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correccion de los datos faltantes y/o NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescued_coords = pd.read_csv('../data/rescueLatLongs.csv')\n",
    "rescued_antiguedad = pd.read_csv('../data/imputations/antiguedad.csv')\n",
    "rescued_banos = pd.read_csv('../data/imputations/banos.csv')\n",
    "rescued_garages = pd.read_csv('../data/imputations/garages.csv')\n",
    "rescued_habitaciones = pd.read_csv('../data/imputations/habitaciones.csv')\n",
    "rescued_metroscubiertos = pd.read_csv('../data/imputations/metroscubiertos.csv')\n",
    "rescued_metrostotales = pd.read_csv('../data/imputations/metrostotales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeamos con coords. extra obtenidas en tp1.\n",
    "train = train.merge(rescued_coords.drop('Unnamed: 0', axis=1), how='left', on='id')\n",
    "train['lat_x'] = train.apply(lambda x: x['lat_y'] if pd.isna(x['lat_x']) else x['lat_x'], axis=1)\n",
    "train['lng_x'] = train.apply(lambda x: x['lng_y'] if pd.isna(x['lng_x']) else x['lng_x'], axis=1)\n",
    "train.drop(['lat_y','lng_y'], axis=1, inplace=True)\n",
    "train.rename(columns={'lat_x':'lat','lng_x':'lon'}, inplace=True)\n",
    "\n",
    "# por consistencia, para que ambos datasets tengan mismos nombres\n",
    "test.rename(columns={'lng':'lon'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nan para los datos fuera de rango es mejor que dropear todo el dato\n",
    "train.loc[(train['lat']>14) | (train['lat']<33),['lat','lon']] = np.nan\n",
    "train.loc[(train['lon']>86) | (train['lon']<118),['lat','lon']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf. values don't make sense. I think it's preferable to treat them as nans directly.\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[(train['lat']>14) | (train['lat']<33),['lat','lon']] = np.nan\n",
    "test.loc[(train['lon']>86) | (train['lon']<118),['lat','lon']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_models_predictions(df, predictions_df, col_name):\n",
    "    indicadora_name = \"tiene_\" + col_name\n",
    "    df[indicadora_name] = df[col_name].notna()\n",
    "    \n",
    "    df = df.merge(predictions_df, how='left', on='id')\n",
    "    original_col = col_name + \"_x\"\n",
    "    filler_col = col_name + \"_y\"\n",
    "    df[col_name] = df.apply(lambda x: x[filler_col] if pd.isna(x[original_col]) else x[original_col], axis=1)\n",
    "    df.drop([original_col,filler_col], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_values(df):\n",
    "    df = fillna_with_models_predictions(df, rescued_antiguedad, 'antiguedad')\n",
    "    df = fillna_with_models_predictions(df, rescued_banos, 'banos')\n",
    "    df = fillna_with_models_predictions(df, rescued_garages, 'garages')\n",
    "    df = fillna_with_models_predictions(df, rescued_habitaciones, 'habitaciones')\n",
    "    df = fillna_with_models_predictions(df, rescued_metroscubiertos, 'metroscubiertos')\n",
    "    df = fillna_with_models_predictions(df, rescued_metrostotales, 'metrostotales')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import  Pool\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parallelize_dataframe(train, fill_na_values, 8)\n",
    "test = parallelize_dataframe(test, fill_na_values, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregado de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_amenities(row):\n",
    "    return row['gimnasio'] + row['usosmultiples'] + row['piscina'] + row['escuelascercanas'] + row['centroscomercialescercanos']\n",
    "\n",
    "train['cant_amenities'] = train.apply(lambda x: contar_amenities(x), axis=1)\n",
    "test['cant_amenities'] = test.apply(lambda x: contar_amenities(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools\n",
    "#def two_set_bools(df, bool_features):\n",
    "#    for combination in itertools.combinations(bool_features,2):\n",
    "#        if combination[0] != combination[1]:\n",
    "#            new_feature_name = combination[0] + \"_AND_\" + combination[1]\n",
    "#            df[new_feature_name] = df[combination[0]] & df[combination[1]]\n",
    "            \n",
    "#            new_feature_name = combination[0] + \"_OR_\" + combination[1]\n",
    "#            df[new_feature_name] = df[combination[0]] | df[combination[1]]\n",
    "            \n",
    "#            new_feature_name = combination[0] + \"_XOR_\" + combination[1]\n",
    "#            df[new_feature_name] = df[combination[0]] ^ df[combination[1]]\n",
    "\n",
    "#two_set_bools(train, ['gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos'])\n",
    "#two_set_bools(test, ['gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas', 'centroscomercialescercanos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ratio_cubiertos_totales'] = train['metroscubiertos']/train['metrostotales']\n",
    "test['ratio_cubiertos_totales'] = test['metroscubiertos']/test['metrostotales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def feature_fechas(df):\n",
    "    # Para entender lo de los senos y cosenos: https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "#    df['year'] = df['fecha'].dt.year\n",
    "#    df['month'] = df['fecha'].dt.month\n",
    "#    df['day'] = df['fecha'].dt.day\n",
    "#    df['sin_month'] = np.sin(2*np.pi*df['month']/12)\n",
    "#    df['cos_month'] = np.cos(2*np.pi*df['month']/12)\n",
    "    # tomo cant. de dias en mes: 31 en todos los casos. Para esto deberia servir bastante bien igual.\n",
    "#    df['sin_day'] = np.sin(2*np.pi*df['day']/31)\n",
    "#    df['cos_day'] = np.cos(2*np.pi*df['day']/31)\n",
    "    \n",
    "#    df['dayofweek'] = df['fecha'].dt.dayofweek\n",
    "#    df['sin_dayofweek'] = np.sin(2*np.pi*df['dayofweek']/7)\n",
    "#    df['cos_dayofweek'] = np.cos(2*np.pi*df['dayofweek']/7)\n",
    "    \n",
    "#    df['dayofyear'] = df['fecha'].dt.dayofyear\n",
    "#    df['sin_dayofyear'] = np.sin(2*np.pi*df['dayofyear']/365)\n",
    "#    df['cos_dayofyear'] = np.cos(2*np.pi*df['dayofyear']/365)\n",
    "    \n",
    "    \n",
    "    #     df['days_in_month'] = df['fecha'].dt.days_in_month\n",
    "#    df['daysinmonth'] = df['fecha'].dt.daysinmonth\n",
    "#    df['is_leap_year'] = df['fecha'].dt.is_leap_year\n",
    "#    df['is_month_end'] = df['fecha'].dt.is_month_end\n",
    "#    df['is_month_start'] = df['fecha'].dt.is_month_start\n",
    "#    df['is_quarter_end'] = df['fecha'].dt.is_quarter_end\n",
    "#    df['is_quarter_start'] = df['fecha'].dt.is_quarter_start\n",
    "#    df['is_year_end'] = df['fecha'].dt.is_year_end\n",
    "#    df['is_year_start'] = df['fecha'].dt.is_year_start\n",
    "    \n",
    "#     df['week'] = df['fecha'].dt.week\n",
    "#    df['weekofyear'] = df['fecha'].dt.weekofyear\n",
    "#    df['sin_weekofyear'] = np.sin(2*np.pi*df['weekofyear']/53)\n",
    "#    df['cos_weekofyear'] = np.cos(2*np.pi*df['weekofyear']/53)\n",
    "    \n",
    "    # no necesito mas las cols. originales de month y day.\n",
    "#    df.drop(['month','day','dayofweek','dayofyear','weekofyear'], axis=1, inplace=True)\n",
    "\n",
    "#feature_fechas(train)\n",
    "#feature_fechas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/a/2979208\n",
    "import math\n",
    "\n",
    "def entropy(string):\n",
    "        \"Calculates the Shannon entropy of a string\"\n",
    "\n",
    "        # get probability of chars in string\n",
    "        prob = [ float(string.count(c)) / len(string) for c in dict.fromkeys(list(string)) ]\n",
    "\n",
    "        # calculate the entropy\n",
    "        entropy = - sum([ p * math.log(p) / math.log(2.0) for p in prob ])\n",
    "\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['titulo'].fillna(\"\", inplace=True)\n",
    "test['titulo'].fillna(\"\", inplace=True)\n",
    "\n",
    "train['titulo_cant_html_tags'] = train['titulo'].apply(lambda x: len(x.split('<'))-1)\n",
    "test['titulo_cant_html_tags'] = test['titulo'].apply(lambda x: len(x.split('<'))-1)\n",
    "\n",
    "train['titulo_cant_palabras'] = train['titulo'].apply(lambda x: len(x.split()))\n",
    "test['titulo_cant_palabras'] = test['titulo'].apply(lambda x: len(x.split()))\n",
    "\n",
    "train['titulo_cant_palabras_unicas'] = train['titulo'].apply(lambda x: len(set(x.split())))\n",
    "test['titulo_cant_palabras_unicas'] = test['titulo'].apply(lambda x: len(set(x.split())))\n",
    "\n",
    "# Concepto traido de https://docs.featuretools.com/generated/nlp_primitives.DiversityScore.html#nlp_primitives.DiversityScore\n",
    "# Cito de la doc oficial:\n",
    "#         Given a list of strings, calculates the total number of unique words divided by the total number of words\n",
    "#         in order to give the text a score from 0-1 that indicates how unique the words used in it are.\n",
    "\n",
    "train['titulo_diversity_score'] = train['titulo_cant_palabras_unicas']/train['titulo_cant_palabras']\n",
    "test['titulo_diversity_score'] = test['titulo_cant_palabras_unicas']/test['titulo_cant_palabras']\n",
    "\n",
    "train['titulo_cant_caracteres'] = train['titulo'].apply(lambda x: len(x))\n",
    "test['titulo_cant_caracteres'] = test['titulo'].apply(lambda x: len(x))\n",
    "\n",
    "import re\n",
    "train['titulo_cant_signos_puntuacion'] = train['titulo'].apply(lambda x: len(re.split(\"['.', ',', '!', '?', '¿', '¡', '-']\",x)))\n",
    "test['titulo_cant_signos_puntuacion'] = test['titulo'].apply(lambda x: len(re.split(\"['.', ',', '!', '?', '¿', '¡', '-']\",x)))\n",
    "\n",
    "train['titulo_entropy'] = train['titulo'].apply(lambda x: entropy(x))\n",
    "test['titulo_entropy'] = test['titulo'].apply(lambda x: entropy(x))\n",
    "\n",
    "train['titulo_mean_word_length'] = train['titulo'].apply(lambda x: np.mean([len(word) for word in x.split()] if x.split() else np.nan))\n",
    "test['titulo_mean_word_length'] = test['titulo'].apply(lambda x: np.mean([len(word) for word in x.split()] if x.split() else np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['descripcion'].fillna(\"\", inplace=True)\n",
    "test['descripcion'].fillna(\"\", inplace=True)\n",
    "\n",
    "train['descripcion_cant_html_tags'] = train['descripcion'].apply(lambda x: len(x.split('<'))-1)\n",
    "test['descripcion_cant_html_tags'] = test['descripcion'].apply(lambda x: len(x.split('<'))-1)\n",
    "\n",
    "train['descripcion_cant_palabras_unicas'] = train['descripcion'].apply(lambda x: len(set(x.split())))\n",
    "test['descripcion_cant_palabras_unicas'] = test['descripcion'].apply(lambda x: len(set(x.split())))\n",
    "\n",
    "train['descripcion_entropy'] = train['descripcion'].apply(lambda x: entropy(x))\n",
    "test['descripcion_entropy'] = test['descripcion'].apply(lambda x: entropy(x))\n",
    "\n",
    "#train['descripcion_mean_word_length'] = train['descripcion'].apply(lambda x: np.mean([len(word) for word in x.split()] if x.split() else np.nan))\n",
    "#test['descripcion_mean_word_length'] = test['descripcion'].apply(lambda x: np.mean([len(word) for word in x.split()] if x.split() else np.nan)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_avenida = ['avenida', 'av']\n",
    "train['es_avenida'] = train['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))\n",
    "test['es_avenida'] = test['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fechas(df):\n",
    "    # Para entender lo de los senos y cosenos: https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "    df['year'] = df['fecha'].dt.year\n",
    "    df['month'] = df['fecha'].dt.month\n",
    "    df['day'] = df['fecha'].dt.day\n",
    "    df['sin_month'] = np.sin(2*np.pi*df['month']/12)\n",
    "    df['cos_month'] = np.cos(2*np.pi*df['month']/12)\n",
    "    # tomo cant. de dias en mes: 31 en todos los casos. Para esto deberia servir bastante bien igual.\n",
    "    df['sin_day'] = np.sin(2*np.pi*df['day']/31)\n",
    "    df['cos_day'] = np.cos(2*np.pi*df['day']/31)\n",
    "    \n",
    "    # no necesito mas las cols. originales de month y day.\n",
    "    df.drop(['month','day'], axis=1, inplace=True)\n",
    "    \n",
    "feature_fechas(train)\n",
    "feature_fechas(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Fin agregado de features *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/train_master.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('../data/test_master.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
