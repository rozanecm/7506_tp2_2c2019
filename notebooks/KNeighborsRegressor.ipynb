{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will allow you to see all column names & rows when you are doing .head(). None of the column name will be truncated.\n",
    "# source: https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/rozanecm/38f2901c592bdffc40726cb0473318cf\n",
    "# Function which plays a beep of given duration and frequency.\n",
    "# Useful for when executing things that need a while to finish, to get notified.\n",
    "import os\n",
    "def beep(duration = 1, freq = 1500):\n",
    "    \"\"\" play tone of duration in seconds and freq in Hz. \"\"\"\n",
    "    os.system('play --no-show-progress --null --channels 1 synth %s sine %f' % (duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])\n",
    "test = pd.read_csv('../data/test.csv', dtype={'tipodepropiedad':'category', 'ciudad':'category', 'provincia':'category', 'id':'int32', 'antiguedad':'float16', 'habitaciones':'float16', 'garages':'float16', 'banos':'float16', 'metroscubiertos':'float16', 'metrostotales':'float16', 'idzona':'float16', 'lat':'float16', 'lng':'float16', 'gimnasio':'bool', 'usosmultiples':'bool', 'piscina':'bool', 'escuelascercanas':'bool', 'centroscomercialescercanos':'bool'}, parse_dates=['fecha'])\n",
    "sample_submission = pd.read_csv('../data/ejemploRespuesta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescued_coords = pd.read_csv('../data/rescueLatLongs.csv')\n",
    "precios_en_dolares = pd.read_csv('../data/precios_en_dolares.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titulo</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>tipodepropiedad</th>\n",
       "      <th>direccion</th>\n",
       "      <th>ciudad</th>\n",
       "      <th>provincia</th>\n",
       "      <th>antiguedad</th>\n",
       "      <th>habitaciones</th>\n",
       "      <th>garages</th>\n",
       "      <th>banos</th>\n",
       "      <th>metroscubiertos</th>\n",
       "      <th>metrostotales</th>\n",
       "      <th>idzona</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>fecha</th>\n",
       "      <th>gimnasio</th>\n",
       "      <th>usosmultiples</th>\n",
       "      <th>piscina</th>\n",
       "      <th>escuelascercanas</th>\n",
       "      <th>centroscomercialescercanos</th>\n",
       "      <th>precio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254099</td>\n",
       "      <td>depto. tipo a-402</td>\n",
       "      <td>depto. interior de 80.15m2, consta de sala com...</td>\n",
       "      <td>Apartamento</td>\n",
       "      <td>Avenida Division del Norte 2005</td>\n",
       "      <td>Benito Juárez</td>\n",
       "      <td>Distrito Federal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>23536.0</td>\n",
       "      <td>31.733330</td>\n",
       "      <td>-106.48333</td>\n",
       "      <td>2015-08-23</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2273000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53461</td>\n",
       "      <td>condominio horizontal en venta</td>\n",
       "      <td>&lt;p&gt;entre sonora y guerrero, atr&amp;aacute;s del h...</td>\n",
       "      <td>Casa en condominio</td>\n",
       "      <td>AV. MEXICO</td>\n",
       "      <td>La Magdalena Contreras</td>\n",
       "      <td>Distrito Federal</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>24512.0</td>\n",
       "      <td>19.312500</td>\n",
       "      <td>-99.25000</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247984</td>\n",
       "      <td>casa en venta urbi 3 recamaras tonala</td>\n",
       "      <td>descripcion \\nla mejor ubicacion residencial e...</td>\n",
       "      <td>Casa</td>\n",
       "      <td>Urbi Tonala</td>\n",
       "      <td>Tonalá</td>\n",
       "      <td>Jalisco</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>48544.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-17</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1200000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209067</td>\n",
       "      <td>casa sola en toluca zinacantepec con credito i...</td>\n",
       "      <td>casa en privada con caseta de vigilancia casas...</td>\n",
       "      <td>Casa</td>\n",
       "      <td>IGNACIO MANUEL ALTAMIRANO 128</td>\n",
       "      <td>Zinacantepec</td>\n",
       "      <td>Edo. de México</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>53664.0</td>\n",
       "      <td>19.296875</td>\n",
       "      <td>-99.68750</td>\n",
       "      <td>2012-03-09</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>185997</td>\n",
       "      <td>paseos del sol</td>\n",
       "      <td>bonito departamento en excelentes condiciones ...</td>\n",
       "      <td>Apartamento</td>\n",
       "      <td>PASEOS DEL SOL</td>\n",
       "      <td>Zapopan</td>\n",
       "      <td>Jalisco</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>47840.0</td>\n",
       "      <td>20.723560</td>\n",
       "      <td>-103.38479</td>\n",
       "      <td>2016-06-07</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1150000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                             titulo  \\\n",
       "0  254099                                  depto. tipo a-402   \n",
       "1   53461                     condominio horizontal en venta   \n",
       "2  247984              casa en venta urbi 3 recamaras tonala   \n",
       "3  209067  casa sola en toluca zinacantepec con credito i...   \n",
       "4  185997                                     paseos del sol   \n",
       "\n",
       "                                         descripcion     tipodepropiedad  \\\n",
       "0  depto. interior de 80.15m2, consta de sala com...         Apartamento   \n",
       "1  <p>entre sonora y guerrero, atr&aacute;s del h...  Casa en condominio   \n",
       "2  descripcion \\nla mejor ubicacion residencial e...                Casa   \n",
       "3  casa en privada con caseta de vigilancia casas...                Casa   \n",
       "4  bonito departamento en excelentes condiciones ...         Apartamento   \n",
       "\n",
       "                         direccion                  ciudad         provincia  \\\n",
       "0  Avenida Division del Norte 2005           Benito Juárez  Distrito Federal   \n",
       "1                       AV. MEXICO  La Magdalena Contreras  Distrito Federal   \n",
       "2                      Urbi Tonala                  Tonalá           Jalisco   \n",
       "3    IGNACIO MANUEL ALTAMIRANO 128            Zinacantepec    Edo. de México   \n",
       "4                  PASEOS DEL SOL                  Zapopan           Jalisco   \n",
       "\n",
       "   antiguedad  habitaciones  garages  banos  metroscubiertos  metrostotales  \\\n",
       "0         NaN           2.0      1.0    2.0             80.0           80.0   \n",
       "1        10.0           3.0      2.0    2.0            268.0          180.0   \n",
       "2         5.0           3.0      2.0    2.0            144.0          166.0   \n",
       "3         1.0           2.0      1.0    1.0             63.0           67.0   \n",
       "4        10.0           2.0      1.0    1.0             95.0           95.0   \n",
       "\n",
       "    idzona        lat        lon      fecha  gimnasio  usosmultiples  piscina  \\\n",
       "0  23536.0  31.733330 -106.48333 2015-08-23     False          False    False   \n",
       "1  24512.0  19.312500  -99.25000 2013-06-28     False          False    False   \n",
       "2  48544.0        NaN        NaN 2015-10-17     False          False    False   \n",
       "3  53664.0  19.296875  -99.68750 2012-03-09     False          False    False   \n",
       "4  47840.0  20.723560 -103.38479 2016-06-07     False          False    False   \n",
       "\n",
       "   escuelascercanas  centroscomercialescercanos     precio  \n",
       "0             False                       False  2273000.0  \n",
       "1              True                        True  3600000.0  \n",
       "2             False                       False  1200000.0  \n",
       "3              True                        True   650000.0  \n",
       "4             False                       False  1150000.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mergeamos con coords. extra obtenidas en tp1.\n",
    "train = train.merge(rescued_coords.drop('Unnamed: 0', axis=1), how='left', on='id')\n",
    "train['lat_x'] = train.apply(lambda x: x['lat_y'] if pd.isna(x['lat_x']) else x['lat_x'], axis=1)\n",
    "train['lng_x'] = train.apply(lambda x: x['lng_y'] if pd.isna(x['lng_x']) else x['lng_x'], axis=1)\n",
    "train.drop(['lat_y','lng_y'], axis=1, inplace=True)\n",
    "train.rename(columns={'lat_x':'lat','lng_x':'lon'}, inplace=True)\n",
    "\n",
    "# por consistencia, para que ambos datasets tengan mismos nombres\n",
    "test.rename(columns={'lng':'lon'}, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inf. values don't make sense. I think it's preferable to treat them as nans directly.\n",
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 240000 entries, 0 to 239999\n",
      "Data columns (total 23 columns):\n",
      "id                            240000 non-null int64\n",
      "titulo                        234613 non-null object\n",
      "descripcion                   238381 non-null object\n",
      "tipodepropiedad               239954 non-null category\n",
      "direccion                     186928 non-null object\n",
      "ciudad                        239628 non-null category\n",
      "provincia                     239845 non-null category\n",
      "antiguedad                    196445 non-null float16\n",
      "habitaciones                  217529 non-null float16\n",
      "garages                       202235 non-null float16\n",
      "banos                         213779 non-null float16\n",
      "metroscubiertos               222600 non-null float16\n",
      "metrostotales                 188533 non-null float16\n",
      "idzona                        118808 non-null float16\n",
      "lat                           138398 non-null float64\n",
      "lon                           138398 non-null float64\n",
      "fecha                         240000 non-null datetime64[ns]\n",
      "gimnasio                      240000 non-null bool\n",
      "usosmultiples                 240000 non-null bool\n",
      "piscina                       240000 non-null bool\n",
      "escuelascercanas              240000 non-null bool\n",
      "centroscomercialescercanos    240000 non-null bool\n",
      "precio                        240000 non-null float64\n",
      "dtypes: bool(5), category(3), datetime64[ns](1), float16(7), float64(3), int64(1), object(3)\n",
      "memory usage: 21.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agregamos features que ya hemos creado para analisis de tp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_amenities(row):\n",
    "    return row['gimnasio'] + row['usosmultiples'] + row['piscina'] + row['escuelascercanas'] + row['centroscomercialescercanos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['cant_amenities'] = train.apply(lambda x: contar_amenities(x), axis=1)\n",
    "test['cant_amenities'] = test.apply(lambda x: contar_amenities(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_fechas(df):\n",
    "    # Para entender lo de los senos y cosenos: https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/\n",
    "    df['year'] = df['fecha'].dt.year\n",
    "    df['month'] = df['fecha'].dt.month\n",
    "    df['day'] = df['fecha'].dt.day\n",
    "    df['sin_month'] = np.sin(2*np.pi*df['month']/12)\n",
    "    df['cos_month'] = np.cos(2*np.pi*df['month']/12)\n",
    "    # tomo cant. de dias en mes: 31 en todos los casos. Para esto deberia servir bastante bien igual.\n",
    "    df['sin_day'] = np.sin(2*np.pi*df['day']/31)\n",
    "    df['cos_day'] = np.cos(2*np.pi*df['day']/31)\n",
    "    \n",
    "    # no necesito mas las cols. originales de month y day.\n",
    "    df.drop(['month','day'], axis=1, inplace=True)\n",
    "    \n",
    "feature_fechas(train)\n",
    "feature_fechas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_avenida = ['avenida', 'av']\n",
    "train['es_avenida'] = train['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))\n",
    "test['es_avenida'] = test['direccion'].fillna('no info').apply(lambda x: any(avenida_indicator in x.lower() for avenida_indicator in palabras_avenida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperar_cant_amb(x):\n",
    "    \"Recupera el dato amb en x (puede ser la descripcion o el titulo)\"\n",
    "    try:\n",
    "        x = x.lower()\n",
    "        if x.rfind(\"ambientes\") != -1 or x.rfind(\"amb\") != -1:\n",
    "            pos = x.rfind(\"amb\")\n",
    "            try:\n",
    "                return (np.float64(x[pos -2]))\n",
    "            except ValueError:\n",
    "                if \"dos amb\" in x or \"2 amb\" in x:\n",
    "                    return 2.0\n",
    "                elif \"tres amb\" in x or \"3 amb\" in x:\n",
    "                    return 3.0\n",
    "                elif \"cuatro amb\" in x or \"4 amb\" in x:\n",
    "                    return 4.0\n",
    "                elif \"cinco amb\" in x or \"5 amb\" in x:\n",
    "                    return 5.0\n",
    "                elif \"seis amb\" in x or \"6 amb\" in x:\n",
    "                    return 6.0\n",
    "                return 0\n",
    "    except AttributeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recuperamos el dato de habitaciones en el titulo y la descripcion\n",
    "train.loc[train[\"habitaciones\"].isnull(),'habitaciones'] = train[\"descripcion\"].apply(recuperar_cant_amb)\n",
    "train.loc[train[\"habitaciones\"].isnull(),'habitaciones'] = train[\"titulo\"].apply(recuperar_cant_amb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Recategorizacion\n",
    "\n",
    "tipo_vivienda = [\"apartamento\", \"casa en condominio\", \"casa\",\n",
    "                 \"villa\", \"duplex\", \"departamento compartido\", \"hospedaje\", \"quinta vacacional\"]\n",
    "tipo_comercial = [\"terreno comercial\", \"local comercial\", \"oficina comercial\", \n",
    "                  \"local en centro comercial\", \"bodega comercial\", \"inmuebles productivos urbanos\"]\n",
    "\n",
    "tipo_inversion = [\"casa uso de suelo\", \"terreno\", \"edificio\", \"huerta\", \"lote\", \"garage\", \"otros\", \"nave industrial\",  \"rancho\"]\n",
    "undefined = [\"terreno\", \"edificio\", \"casa uso de suelo\", \"otros\", \"nave industrial\", \"rancho\", \"nan\", \n",
    "             \"huerta\", \"lote\", \"garage\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ancester_category(tipo_de_propiedad):\n",
    "    if (tipo_de_propiedad in tipo_inversion): return \"inversion\"\n",
    "\n",
    "    if (tipo_de_propiedad in tipo_comercial): return \"comercial\"\n",
    "\n",
    "    if (tipo_de_propiedad in tipo_vivienda): return \"vivienda\"\n",
    "\n",
    "    return \"undefined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"categoria_padre\"] = train[\"tipodepropiedad\"].apply(lambda x: get_ancester_category(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates([\"tipodepropiedad\",\"direccion\",\"provincia\",\"ciudad\"], keep='last', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Fin agregado de features de tp1 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sample submission no tiene header. **Ojo con eso al guardar la submission.** Hagamos la funcion para guardar submissions ahora, para evitar problemas a futuro y despreocuparnos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save predictions.\n",
    "# There must be a directory ../predictions for this to work as expected.\n",
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "import time\n",
    "def _get_filename(my_name, timestamp):\n",
    "    return \"../predictions/\" + timestamp + \" by \" + my_name + \".csv\"\n",
    "\n",
    "def _save_description(authors_name, timestamp, submission_description):\n",
    "    f = open(\"../predictions/\" + authors_name + \".txt\",\"a\")\n",
    "    f.write(timestamp + \": \" + submission_description + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def save_submission(submission_df, authors_name=\"lucioll\", description = \"no description.\", index=False, header=True):\n",
    "    timestamp = time.strftime(\"%Y.%m.%d - %H:%M:%S\")\n",
    "    submission_df.to_csv(_get_filename(authors_name, timestamp), index=index, header=header)\n",
    "    _save_description(authors_name, timestamp, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a seed, so all algorithms that accept a seed, take the same, for consistency reasons,\n",
    "# so everything can be replicated without problems random state\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('precio', axis=1), train['precio'], test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach n...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total= 1.2min\n",
      "[Pipeline] ....... (step 2 of 2) Processing knn-kd-tree, total=  48.9s\n",
      "737661.7190366162\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn-kd-tree\", KNeighborsRegressor(n_neighbors=20,algorithm=\"kd_tree\", p = 1, n_jobs=-1)))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  33.2s\n",
      "[Pipeline] ....... (step 2 of 2) Processing knn-kd-tree, total=   7.9s\n",
      "748873.5102935112\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn-kd-tree\", KNeighborsRegressor(n_neighbors=20,algorithm=\"kd_tree\", p = 1, n_jobs=-1)))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con todos los datos para obtener predicciones a subir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  56.3s\n",
      "[Pipeline] ... (step 2 of 2) Processing knn-brute-force, total= 1.9min\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [79200, 60000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-94a06d315c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# prediciendo valores posta...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \"\"\"\n\u001b[1;32m    171\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 172\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [79200, 60000]"
     ]
    }
   ],
   "source": [
    "my_pipe.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "\n",
    "# prediciendo valores posta...\n",
    "predictions = my_pipe.predict(test.replace({True:1,False:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'id':test['id'], 'target':predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"KNN ball_tree with n=4 and p=1\"\n",
    "save_submission(df, description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengamos predicciones para todas las propiedades en nuestro train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  36.9s\n",
      "[Pipeline] ... (step 2 of 2) Processing knn-brute-force, total=   0.0s\n",
      "828032.356075\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  39.9s\n",
      "[Pipeline] ... (step 2 of 2) Processing knn-brute-force, total=   0.0s\n",
      "835511.888059375\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  59.9s\n",
      "[Pipeline] ... (step 2 of 2) Processing knn-brute-force, total=   0.0s\n",
      "825146.995178125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "# UPDATE THIS VALUE\n",
    "approach_numer = \"lucioll_approach_1\"\n",
    "\n",
    "for train_index, test_index in kf.split(train):\n",
    "    # for loop copied from docs: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n",
    "    X_train2, X_test2 = train.drop(['precio'],axis=1).iloc[train_index], train.drop(['precio'],axis=1).iloc[test_index]\n",
    "    y_train2, y_test2 = train['precio'][train_index], train['precio'][test_index]\n",
    "    \n",
    "    my_pipe.fit(X_train2.replace({True:1,False:0}), y_train2)\n",
    "    y_scores = my_pipe.predict(X_test2.replace({True:1,False:0}))\n",
    "    \n",
    "    print(mean_absolute_error(y_test2, y_scores))\n",
    "    \n",
    "    df = df.append(pd.DataFrame(data={'id':X_test2['id'], approach_numer:y_scores}))\n",
    "\n",
    "df.to_csv(\"../predictions/on_train_data/\" + approach_numer + \".csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'titulo', 'descripcion', 'tipodepropiedad', 'direccion', 'ciudad',\n",
       "       'provincia', 'antiguedad', 'habitaciones', 'garages', 'banos',\n",
       "       'metroscubiertos', 'metrostotales', 'idzona', 'lat', 'lon', 'fecha',\n",
       "       'gimnasio', 'usosmultiples', 'piscina', 'escuelascercanas',\n",
       "       'centroscomercialescercanos', 'cant_amenities', 'year', 'sin_month',\n",
       "       'cos_month', 'sin_day', 'cos_day', 'es_avenida'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplico Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  42.0s\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 77.7min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 1098.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed: 2869.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 2 of 2) Processing grid-search, total=1188.7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=16, p=2,\n",
       "                    weights='distance')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "k_range = list(range(1, 21))\n",
    "print(k_range)\n",
    "\n",
    "param_dist = {\"n_neighbors\": k_range,\n",
    "              \"weights\" : ['uniform', 'distance'],\n",
    "              \"metric\": ['minkowski']\n",
    "             }\n",
    "\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "grid = GridSearchCV(knn, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"neg_mean_absolute_error\", verbose=5)\n",
    "steps.append((\"grid-search\", grid))\n",
    "\n",
    "my_pipeline2 = Pipeline(steps, verbose=True)\n",
    "\n",
    "my_pipeline2.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  45.4s\n",
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 31.7min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-34f18dd59aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mmy_pipeline2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mmy_pipeline2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor()\n",
    "k_range = [1, 2, 4, 6, 8, 10, 12, 14 ,16, 18, 20]\n",
    "print(k_range)\n",
    "\n",
    "param_dist = {\"n_neighbors\": k_range,\n",
    "              \"weights\" : ['uniform', 'distance'],\n",
    "              \"metric\": ['minkowski']\n",
    "             }\n",
    "\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", MinMaxScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "grid = GridSearchCV(knn, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"neg_mean_absolute_error\", verbose=5)\n",
    "steps.append((\"grid-search\", grid))\n",
    "\n",
    "my_pipeline2 = Pipeline(steps, verbose=True)\n",
    "\n",
    "my_pipeline2.fit(train.drop(['precio'], axis=1).replace({True:1,False:0}), train['precio'])\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  43.9s\n",
      "[Pipeline] ......... (step 2 of 2) Processing Best knn , total=   9.7s\n",
      "784687.2314277309\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"Best knn \", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                    metric_params=None, n_jobs=None, n_neighbors=16, p=1.7,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_dist(x,y):\n",
    "    return np.sum(((x-y)**0.7)**(1/0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"braycurtis\", \"canberra\", \"chebyshev\", \"correlation\", \"dice\", \"hamming\", \n",
    "           \"jaccard\", \"kulsinski\", \"mahalanobis\", \"minkowski\", \"rogerstanimoto\", \n",
    "           \"russellrao\", \"seuclidean\", \"sokalmichener\", \"sokalsneath\", \"sqeuclidean\", \"yule\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 2 of 2) Processing Best knn , total=  25.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900408.6760940655\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"Best knn \", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='canberra',\n",
    "                    metric_params=None, n_jobs=-1, n_neighbors=6,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  48.6s\n",
      "[Pipeline] . (step 2 of 2) Processing knn with canberra, total=   9.3s\n",
      "743110.1724742119\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn with braycurtis\", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='braycurtis',\n",
    "                    metric_params=None, n_jobs=-1, n_neighbors=12,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  49.1s\n",
      "[Pipeline]  (step 2 of 2) Processing knn with braycurtis, total=   8.2s\n",
      "740497.2572507106\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn with braycurtis\", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='braycurtis',\n",
    "                    metric_params=None, n_jobs=-1, n_neighbors=16,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  45.1s\n",
      "[Pipeline]  (step 2 of 2) Processing knn with braycurtis, total=   8.9s\n",
      "739920.9036943609\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn with braycurtis\", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='braycurtis',\n",
    "                    metric_params=None, n_jobs=-1, n_neighbors=20,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  45.2s\n",
      "[Pipeline]  (step 2 of 2) Processing knn with braycurtis, total=   9.2s\n",
      "741718.1871170142\n"
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn with braycurtis\", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='braycurtis',\n",
    "                    metric_params=None, n_jobs=-1, n_neighbors=24,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def _validate_vector(u, dtype=None):\n",
    "    # XXX Is order='c' really necessary?\n",
    "    u = np.asarray(u, dtype=dtype, order='c').squeeze()\n",
    "    # Ensure values such as u=1 and u=[1] still return 1-D arrays.\n",
    "    u = np.atleast_1d(u)\n",
    "    if u.ndim > 1:\n",
    "        raise ValueError(\"Input vector should be 1-D.\")\n",
    "    return u\n",
    "\n",
    "def fractional(u, v, p=0.7, w=None):\n",
    "    \"\"\"\n",
    "    Compute the Minkowski distance between two 1-D arrays.\n",
    "    The Minkowski distance between 1-D arrays `u` and `v`,\n",
    "    is defined as\n",
    "    .. math::\n",
    "       {||u-v||}_p = (\\\\sum{|u_i - v_i|^p})^{1/p}.\n",
    "       \\\\left(\\\\sum{w_i(|(u_i - v_i)|^p)}\\\\right)^{1/p}.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : (N,) array_like\n",
    "        Input array.\n",
    "    v : (N,) array_like\n",
    "        Input array.\n",
    "    p : int\n",
    "        The order of the norm of the difference :math:`{||u-v||}_p`.\n",
    "    w : (N,) array_like, optional\n",
    "        The weights for each value in `u` and `v`. Default is None,\n",
    "        which gives each value a weight of 1.0\n",
    "    Returns\n",
    "    -------\n",
    "    minkowski : double\n",
    "        The Minkowski distance between vectors `u` and `v`.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from scipy.spatial import distance\n",
    "    >>> distance.minkowski([1, 0, 0], [0, 1, 0], 1)\n",
    "    2.0\n",
    "    >>> distance.minkowski([1, 0, 0], [0, 1, 0], 2)\n",
    "    1.4142135623730951\n",
    "    >>> distance.minkowski([1, 0, 0], [0, 1, 0], 3)\n",
    "    1.2599210498948732\n",
    "    >>> distance.minkowski([1, 1, 0], [0, 1, 0], 1)\n",
    "    1.0\n",
    "    >>> distance.minkowski([1, 1, 0], [0, 1, 0], 2)\n",
    "    1.0\n",
    "    >>> distance.minkowski([1, 1, 0], [0, 1, 0], 3)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    u = _validate_vector(u)\n",
    "    v = _validate_vector(v)\n",
    "\n",
    "    u_v = u - v\n",
    "    if w is not None:\n",
    "        w = _validate_weights(w)\n",
    "        if p == 1:\n",
    "            root_w = w\n",
    "        if p == 2:\n",
    "            # better precision and speed\n",
    "            root_w = np.sqrt(w)\n",
    "        else:\n",
    "            root_w = np.power(w, 1/p)\n",
    "        u_v = root_w * u_v\n",
    "    dist = LA.norm(u_v, ord=p)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 2) Processing col_trans, total=  28.4s\n",
      "[Pipeline]  (step 2 of 2) Processing knn with braycurtis, total=  29.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b1b99ff9bbe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mmy_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0my_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/neighbors/regression.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 delayed_query(\n\u001b[1;32m    453\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m             )\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# source: https://gist.github.com/rozanecm/ee8333741db42b10158b3e0aff3f22aa\n",
    "small_size_cat_columns = ['tipodepropiedad','provincia', 'categoria_padre']\n",
    "large_size_cat_columns = ['ciudad']\n",
    "\n",
    "num_columns = [\n",
    "#     'id',\n",
    "    \"antiguedad\",\"habitaciones\",'garages',\n",
    "    'banos','metroscubiertos', 'metrostotales','idzona',\n",
    "    'lat', 'lon', 'cant_amenities',\n",
    "    'year','sin_month','cos_month', 'sin_day', 'cos_day']\n",
    "\n",
    "bool_columns = ['gimnasio','usosmultiples','piscina','escuelascercanas','centroscomercialescercanos','es_avenida']\n",
    "\n",
    "text_columns = ['titulo'\n",
    "                ,'descripcion'\n",
    "                ,'direccion'\n",
    "               ]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "transformers = []\n",
    "\n",
    "transformers.append((\"small_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=11, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     small_size_cat_columns))\n",
    "\n",
    "transformers.append((\"large_cat\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"category_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                         (\"one_hot\", OneHotEncoder(handle_unknown='ignore')),\n",
    "                         (\"svd\", TruncatedSVD(n_components=25, n_iter=7, random_state=seed))\n",
    "                     ]),\n",
    "                     large_size_cat_columns))\n",
    "\n",
    "transformers.append((\"num\",\n",
    "                     Pipeline(steps=[\n",
    "                         (\"num_imputer\", SimpleImputer(strategy='most_frequent',verbose=1)),\n",
    "                         (\"num_transformer\", StandardScaler())\n",
    "                     ]),\n",
    "                   num_columns))\n",
    "\n",
    "transformers.append((\"bool\",\n",
    "                    Pipeline(steps=[\n",
    "                        (\"bool_imputer\", SimpleImputer(strategy='most_frequent')),\n",
    "                    ]),\n",
    "                     bool_columns))\n",
    "\n",
    "# The reason this for is necessary is because text transformers take an array-like parameter.\n",
    "# If we pass a list of columns, then the transformer will receive a dataframe, and that will result in error.\n",
    "# If you don't want to process all the text columns with the same pipeline, you'll have to define\n",
    "# a different pipelines for each, and pass a different list for each of the pipelines.\n",
    "# for col in text_columns_titulo:\n",
    "for col in text_columns:\n",
    "    # First, fill empty texts with an empty string.\n",
    "    X_train[col] = X_train[col].fillna(\"\")\n",
    "    X_test[col] = X_test[col].fillna(\"\")\n",
    "    train[col] = train[col].fillna(\"\")\n",
    "    test[col] = test[col].fillna(\"\")\n",
    "    transformer_name = \"text_\" + col\n",
    "    transformers.append((transformer_name,\n",
    "                        Pipeline(steps=[\n",
    "#                             (\"text_imputer\", SimpleImputer(strategy='constant', fill_value=\"\")),\n",
    "                            (\"hashing_vectorizer\", HashingVectorizer(decode_error='replace', strip_accents='ascii', \n",
    "#                                                                      ngram_range=(2,5)\n",
    "                                                                    )),\n",
    "                            (\"svd\", TruncatedSVD(n_components=20, n_iter=7, random_state=seed))\n",
    "    #                         se podria agregar una svd.... o alguna proyeccion... \n",
    "                        ]),\n",
    "                         col))\n",
    "\n",
    "my_col_transformer = ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, \n",
    "                                       n_jobs=-1, \n",
    "                                       transformer_weights=None)\n",
    "\n",
    "steps = []\n",
    "\n",
    "steps.append((\"col_trans\", my_col_transformer))\n",
    "\n",
    "\n",
    "steps.append((\"knn with braycurtis\", KNeighborsRegressor(algorithm='auto', leaf_size=30, metric=fractional,\n",
    "                    metric_params=None, n_jobs=-1, n_neighbors=8,\n",
    "                    weights='distance')))\n",
    "\n",
    "my_pipe = Pipeline(steps, verbose=True)\n",
    "\n",
    "# .replace is introduced because algorithms need numbers; booleans don't make it.\n",
    "my_pipe.fit(X_train.replace({True:1,False:0}), y_train)\n",
    "\n",
    "y_scores = my_pipe.predict(X_test.replace({True:1,False:0}))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(mean_absolute_error(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
